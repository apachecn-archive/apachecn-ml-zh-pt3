# 3.构建图像分类器

在这一章中，我们将通过构建几个检测图像中对象的 web 应用程序来深入探究 TensorFlow.js 的特性。

将会有更完整的代码示例和解释，因此您可以更好地理解如何在您的项目中实现机器学习。

## 3.1 使用预先训练的模型

我们要建立的第一个项目是一个快速游戏，在这个游戏中，系统会提示你找到你周围的特定物体，使用你设备的摄像头为它们拍照，并检查机器学习模型是否识别它们。

输出将如下所示:

![../images/496132_1_En_3_Chapter/496132_1_En_3_Fig1_HTML.jpg](../images/496132_1_En_3_Chapter/496132_1_En_3_Fig1_HTML.jpg)

图 3-1

图像分类项目快照

这个项目的核心是我们之前谈论过的相同的对象检测模型，称为 **mobilenet** 。

该模型使用开源 ImageNet 数据库进行预训练，该数据库由组织在 1000 个不同类中的图像组成。

这意味着该模型能够根据训练数据识别 1000 种不同的对象。

要启动这个项目，我们需要导入 TensorFlow.js 和 mobilenet 模型。

有两种方法可以做到这一点。您可以使用 HTML 文件中的脚本标签导入它们。

```py
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs/dist/tf.min.js"></script>

<script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/mobilenet@1.0.0"></script>

Listing 3-1Importing TensorFlow.js and mobilenet

```

或者，如果您使用的是前端框架，例如 React.js，您可以在依赖项中安装 TensorFlow.js，然后将其导入到 JavaScript 文件中。

在您的终端中:

```py
npm install @tensorflow/tfjs @tensorflow-models/mobilenet

yarn add @tensorflow/tfjs @tensorflow-models/mobilenet

Listing 3-2Installing the TensorFlow.js and mobilenet modules

```

在您的 JavaScript 文件中:

```py
import "@tensorflow/tfjs";

Import "@tensorflow-models/mobilenet";

Listing 3-3Importing the modules

```

导入这两个文件可以让我们访问`tf`和`mobilenet`对象。

我们需要采取的第一步是**在应用程序中加载模型**。

```py
async function app(){
       const model = await model.load();
}

Listing 3-4Loading

the model

```

![../images/496132_1_En_3_Chapter/496132_1_En_3_Fig2_HTML.jpg](../images/496132_1_En_3_Chapter/496132_1_En_3_Fig2_HTML.jpg)

图 3-2

模型的控制台输出

模型是相当大的文件，加载它们需要几秒钟，因此应该使用`async/await`来加载。

如果您想知道这个对象包含什么，您可以记录它并查看它的属性。

请记住，您不必理解对象中的每个属性才能使用它。

然而，有趣的属性之一是*模型*中的*输入*属性。

![../images/496132_1_En_3_Chapter/496132_1_En_3_Fig3_HTML.jpg](../images/496132_1_En_3_Chapter/496132_1_En_3_Fig3_HTML.jpg)

图 3-3

模型的控制台输出

此属性向我们显示了用于训练模型的输入类型。在这种情况下，我们可以看到使用了图像，考虑到这是一个对象检测模型，这是有意义的。更重要的是，我们可以看到训练过程中使用的数据的形状。

shape 属性显示了值`[-1, 224, 224, 3]`，这意味着提供给模型的图像是大小为 224*224 像素的 RGB 图像(数组末尾的值 3 表示通道数)。

这个值对于本章的下一部分特别有意义，在这一部分我们将研究如何使用 mobilenet 模型进行迁移学习。

请随意进一步探索该模型。

构建该应用程序的下一步是允许 TensorFlow.js 访问来自网络摄像头的输入，以便能够运行预测和检测对象。

因为我们的项目使用设备的网络摄像头，所以我们的 HTML 中有一个`<video>`元素。

在 JavaScript 中，我们需要访问这个元素，并使用 TensorFlow 的方法之一从数据 API 创建一个对象，该对象可以将图像捕获为张量。

```py
const webcamElement = document.getElementsByTagName("video")[0];

const webcam = await tf.data.webcam(webcamElement);

Listing 3-5Instantiating a webcam object

```

这两行仍然是我们应用程序设置过程的一部分。目前，我们只加载了模型并创建了这个`webcam`变量，它将把快照从相机转换成张量。

现在，为了实现这个逻辑，我们需要从在 HTML 中添加一个简单的按钮开始。它将用于在点击时触发图像捕捉。

```py
<button class="capture-image">SNAP</button>

Listing 3-6Button to capture an image

```

在我们的 JavaScript 文件中，我们需要访问这个元素，使用 onclick 事件监听器，并使用 TensorFlow.js 来捕获图像，并对其进行分类。

```py
const captureButton = document.getElementsByClassName("capture-image")[0];
captureButton.onclick = async () => {
    const img = await webcam.capture();
    const predictions = await model.classify(img);
    return predictions;
};

Listing 3-7Classifying

an image

```

为了从视频提要中捕获图像，TensorFlow.js 有一个内置的`capture()`方法，需要在之前使用`tf.data.webcam`创建的对象上调用该方法。

它允许将单个图像直接转换为张量，因此可以轻松地用于其他 TensorFlow.js 操作。

捕获图像后，我们通过将它传入`mobilenet.classify`来生成预测。

这将返回一个预测数组。

例如，这张拍摄塑料瓶的图片将返回以下预测数组。

![../images/496132_1_En_3_Chapter/496132_1_En_3_Fig5_HTML.jpg](../images/496132_1_En_3_Chapter/496132_1_En_3_Fig5_HTML.jpg)

图 3-5

控制台中打印的预测结果

![../images/496132_1_En_3_Chapter/496132_1_En_3_Fig4_HTML.jpg](../images/496132_1_En_3_Chapter/496132_1_En_3_Fig4_HTML.jpg)

图 3-4

活体分类

正如你所看到的，第一个预测，也是模型最有信心的预测，有一个“汽水瓶”的标签。它成功地检测到图像中存在瓶子；但是，概率真的很差，尽管是正确的结果。

预测的置信度只有 30%的事实可能是由于物体背后的背景。背景越复杂，模型就越难找到图像中的物体并对其进行分类。

这个问题更多的是与计算机视觉领域本身有关，而不是一个框架问题。

如下图所示，如果您尝试在更清晰的背景上拍摄同一张照片，预测的质量似乎会好得多。

![../images/496132_1_En_3_Chapter/496132_1_En_3_Fig6_HTML.png](../images/496132_1_En_3_Chapter/496132_1_En_3_Fig6_HTML.png)

图 3-6

更清晰背景下的预测结果

不仅概率高得多，几乎达到 89%，而且下面的预测也更加准确。

在第一个例子中，第二个预测是“吸尘器”，这远远不准确，但在这里，它回来了“水瓶”，这是一个更接近事实的结果。

如果您计划将对象检测集成到您的应用程序中，这个限制肯定是您应该考虑的。考虑您的项目将被使用的环境对于避免糟糕的用户体验是很重要的。

最后，这个过程还有最后一步。我们需要清理我们不再需要的内存。一旦图像被捕获并输入 TensorFlow.js 进行分类，我们就不再需要它了，因此应该释放它占用的内存。

为此，TensorFlow.js 提供了像这样使用的`dispose`方法。

```py
img.dispose();

Listing 3-8Free some memory with the dispose method

```

我们已经讨论了对象检测的主要逻辑部分。然而，游戏的第一部分是被提示寻找特定的物体来拍照。

这段代码不是特定于 TensorFlow.js 的，它可以是一个简单的 UI，每当您成功找到前一个对象时，它都会要求您找到一个新对象。

但是，如果您的 UI 要求您查找手机，您将需要确保模型已经用手机的图片进行了训练，以便它可以检测到正确的对象。

幸运的是，mobilenet 模型可以识别的对象类别列表可以在 [`https://github.com/tensorflow/tfjs-models/blob/master/mobilenet/src/imagenet_classes.ts`](https://github.com/tensorflow/tfjs-models/blob/master/mobilenet/src/imagenet_classes.ts) 的存储库中找到。

如果您在应用程序中导入这个列表，那么您的代码就可以遍历这个包含 1000 个条目的对象，并在 UI 中随机显示一个条目，要求用户在它们周围找到这个对象。

由于这段代码不涉及 TensorFlow.js 库的使用，我们不打算在本书中涉及它。

但是，如果您想看看前面显示的所有代码示例是如何组合在一起的，下面是它应该看起来的样子。

```py
async function app() {
  const webcamElement = document.getElementsByTagName("video")[0];
  const model = await mobilenet.load();

  const webcam = await tf.data.webcam(webcamElement);
  const captureButton = document.getElementsByTagName("button")[0];

  captureButton.onclick = async () => {
    const img = await webcam.capture();

    const predictions = await model.classify(img);
    img.dispose();
    return predictions;
  };
}
app();

Listing 3-10Complete JavaScript code

```

```py
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0"
    />
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs/dist/tf.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/mobilenet@1.0.0"></script>
    <title>Snap it</title>
  </head>
  <body>
    <main>
      <section class="content">
        <h1>Snap it</h1>
        <video></video>
        <button>SNAP</button>
      </section>
    </main>
  </body>

  <script src="index.js"></script>
</html>

Listing 3-9Complete HTML file

```

在这一小节中，我们已经使用对象检测构建了一个小游戏，但是它可以用于非常不同的应用。

## 3.2 迁移学习

使用预先训练好的模型真的很有用，可以让你非常快速地构建项目，但是如果你发现自己需要更定制的东西，你会很快达到它的极限。

在这一小节中，我们将利用我们在最后几页中编写的代码的一些部分，并对它们进行调整以使用自定义输入数据。

我们将从我们的网络摄像头收集定制数据样本，以建立一个可以识别我们头部运动的模型。这可以用作界面的潜在控件，所以你可以想象使用这个模型通过上下倾斜头部来滚动网页，或者使用相同的动作来导航地图。

这个项目将重点放在训练模型识别新样本和测试其预测。

您将在接下来的几页中阅读的代码将生成一个界面，其中包含收集新数据的按钮和运行预测的附加按钮。结果将显示在页面上，供您验证您的模型的准确性。

![../images/496132_1_En_3_Chapter/496132_1_En_3_Fig8_HTML.jpg](../images/496132_1_En_3_Chapter/496132_1_En_3_Fig8_HTML.jpg)

图 3-8

从网络摄像头输入中分类头部运动

![../images/496132_1_En_3_Chapter/496132_1_En_3_Fig7_HTML.jpg](../images/496132_1_En_3_Chapter/496132_1_En_3_Fig7_HTML.jpg)

图 3-7

从网络摄像头输入中分类头部运动

正如你在前面的截图中看到的，头部在向下和向左之间的运动被准确地预测到了。

首先，我们需要导入 TensorFlow.js、mobilenet 模块和 K 近邻分类器。

```py
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs/dist/tf.min.js"></script>

<script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/mobilenet@1.0.0"></script>

<script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/knn-classifier"></script>

Listing 3-11Importing TensorFlow.js, mobilenet, and a KNN classifier

```

如前所述，我们还需要一个视频元素来显示网络摄像头馈送、一些按钮和一个段落来显示我们的预测结果。

```py
<video class="webcam"></video>

<section class="buttons">
      <button>Up</button>
      <button>Down</button>
      <button>Left</button>
      <button>Right</button>
</section>

<section class="buttons">
      <button class="predict">Predict</button>
</section>

<p class="prediction"></p>

Listing 3-12HTML elements needed for this project

```

在一个 JavaScript 文件中，我们需要编写一个逻辑，当我们点击按钮时，它将从网络摄像头中收集样本，并将其提供给 KNN 分类器。

在深入研究逻辑之前，我们需要首先实例化分类器、模型和网络摄像头的一些变量。

```py
const classifier = knnClassifier.create();
const net = await mobilenet.load();
const webcam = await tf.data.webcam(webcamElement);

Listing 3-13Instantiating the classifier, loading the model, and preparing the webcam object

```

在最后一行中，`webcamElement`变量指的是 HTML 视频元素，您可以通过使用标准的文档接口方法(如`getElementsByClassName`)获得。

为了实现这个逻辑，我们可以创建一个名为`addExample`的新函数。该函数将从网络摄像头捕捉图像，将其转换为张量，用图像张量及其标签重新训练 mobilenet 模型，将该示例添加到 KNN 分类器，并处理张量。

这听起来可能很多，但是需要的代码实际上不超过几行。

```py
const addExample = async classId => {
      const img = await webcam.capture();

      const activation = net.infer(img, "conv_preds");

      classifier.addExample(activation, classId);

      img.dispose();
};

Listing 3-14addExample function to retrain the model with custom inputs

```

第二行允许我们从网络摄像头捕捉单个图像，并将其直接转换为张量，因此可以立即与其他 TensorFlow.js 方法一起使用。

`activation`变量保存 mobilenet 模型的值，该模型使用来自网络摄像头的新图像张量重新训练，使用其激活函数之一“conv _ 预测”。

激活函数是帮助神经网络学习数据中复杂模式的函数。

下一步是使用重新训练模型的结果，并将其作为一个示例添加到我们的分类器中，带有一个类 ID，以便它可以将新样本映射到其标签。

在机器学习中，即使我们通常认为标签是字符串，例如，在我们的情况下，“右”、“左”等等，在训练过程中，这些标签实际上是与它们在标签数组中的索引交换的。

如果我们的类是`["up", "down", "left", "right"]`，当我们训练模型识别我们的头部向下移动时，类 ID 将是 1，因为“down”是数组中的第二个元素。

最后，一旦图像张量被使用，我们就把它处理掉，以释放一些内存。

这个`addExample`方法需要在我们点击四个按钮中的一个时被触发。

```py
for (var i = 0; i < buttons.length; i++) {
    if (buttons[i] !== predictButton) {
      let index = i;
      buttons[i].onclick = () => addExample(index);
    }
}

Listing 3-15Looping through the buttons elements to attach an onclick event listener that will trigger the addExample function

```

考虑到`buttons`变量包含 DOM 中的按钮元素，我们希望在除了用于运行预测的按钮之外的所有按钮上触发我们的`addExample`函数。

我们将按钮索引传递给函数，所以当我们单击“向上”按钮时，例如，类 ID 将是 0。

这样，每当我们单击四个按钮中的一个，就会有一个示例添加到分类器中，并带有相应的类 ID。

一旦我们对模型进行了几次重新训练，我们就可以单击“预测”按钮来运行实时预测。

```py
predictButton.onclick = () => runPredictions();

Listing 3-16Calling the runPredictions function when clicking the predict button

```

此`runPredictions`功能将重复之前解释的类似步骤；然而，它不是将示例添加到 KNN 分类器，而是触发`predictClass`方法，根据我们刚刚经历的训练过程，对来自网络摄像头的实时输入进行分类。

```py
async function runPredictions() {
    while (true) {
      if (classifier.getNumClasses() > 0) {
        const img = await webcam.capture();
        const activation = net.infer(img, "conv_preds");
        const result = await classifier.predictClass(activation);

        predictionParagraph.innerText = `
           prediction: ${classes[result.label]},
           probability: ${result.confidences[result.label]}`;

        img.dispose();
      }

      await tf.nextFrame();
    }
}

Listing 3-17The runPredictions function

```

在前面的示例中，我们将逻辑包装在一个`while`循环中，因为我们希望持续预测来自网络摄像头的输入；然而，如果您想在单击一个元素后才获得预测，您也可以用一个`onclick`事件来代替它。

如果已经用新样本训练了分类器，我们重复从网络摄像头捕获图像并将其用于 mobilenet 模型的两个步骤。

```py
const img = await webcam.capture();
const activation = net.infer(img, "conv_preds");

Listing 3-18Steps repeated between training the classifier and running the predictions

```

然后我们在 KNN 分类器上调用的`predictClass`方法中传递这个数据来预测它的标签。

调用这个方法的结果是一个包含一个`classIndex`、一个`label,`和一个名为`confidences`的对象的对象。

![../images/496132_1_En_3_Chapter/496132_1_En_3_Fig9_HTML.jpg](../images/496132_1_En_3_Chapter/496132_1_En_3_Fig9_HTML.jpg)

图 3-9

控制台中分类的输出

在这种情况下，我向右倾斜我的头，所以`classIndex`和`label`返回值 3，因为训练模型识别这个手势的按钮是 4 个按钮中的最后一个。

`confidences`对象向我们展示了预测标签的概率。值 1 意味着模型非常确信所识别的手势是正确的。

概率值可以在 0 和 1 之间变化。

从预测中获得结果后，我们处理图像以释放一些内存。

最后，在再次运行这段代码并预测下一帧的类之前，我们调用`tf.nextFrame()`等待`requestAnimationFrame`完成。

下面是代码的整体工作方式。

```py
const webcamElement = document.getElementsByClassName("webcam")[0];
const buttons = document.getElementsByTagName("button");
const predictButton = document.getElementsByClassName("predict")[0];
const classes = ["up", "down", "left", "right"];
const predictionParagraph = document.getElementsByClassName("prediction")[0];

async function app() {
  const classifier = knnClassifier.create();
  const net = await mobilenet.load();
  const webcam = await tf.data.webcam(webcamElement);

  const addExample = async classId => {
    const img = await webcam.capture();
    const activation = net.infer(img, "conv_preds");
    classifier.addExample(activation, classId);
    img.dispose();
  };

  for (var i = 0; i < buttons.length; i++) {
    if (buttons[i] !== predictButton) {
      let index = i;
      buttons[i].onclick = () => addExample(index);
    }
  }

  predictButton.onclick = () => runPredictions();

  async function runPredictions() {
    while (true) {
      if (classifier.getNumClasses() > 0) {
        const img = await webcam.capture();
        const activation = net.infer(img, "conv_preds");
        const result = await classifier.predictClass(activation);

        predictionParagraph.innerText = `
            prediction: ${classes[result.label]},
            probability: ${result.confidences[result.label]}`;

        img.dispose();
      }
      await tf.nextFrame();
    }
  }
}
app();

Listing 3-20Complete JavaScript code

```

```py
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0"
     />
    <title>Transfer learning</title>
    <script 

src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs/dist/tf.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/mobilenet@1.0.0"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/knn-classifier"></script>
  </head>
  <body>
    <main>
      <section class="content">
        <video class="webcam"></video>
        <section class="buttons">
          <button>Up</button>
          <button>Down</button>
          <button>Left</button>
          <button>Right</button>
        </section>

        <section class="buttons">
          <button class="predict">Predict</button>
        </section>
        <p class="prediction"></p>
      </section>
    </main>

    <script src="index.js"></script>
  </body>
</html>

Listing 3-19Complete HTML code

```

使用迁移学习可以让我们快速地重新训练一个模型，以适应定制的输入。只用几行代码，我们就能够创建一个定制的图像分类模型。

根据你输入的新输入数据，你可能需要添加或多或少的新例子来获得准确的预测，但这总比收集一个全新的带标签的数据集并从头创建自己的机器学习模型要快。