# 5.输入实验

在前面的章节中，我们研究了如何使用图像和文本数据的机器学习来进行对象检测和分类，以及情感分析、毒性分类和问题回答。

这些可能是机器学习可以做什么的最常见的例子。然而，该列表并不详尽，可以使用更多的输入。

在这一章中，我们将探索不同类型的输入数据，并建立几个实验项目，以了解如何使用音频和硬件数据的机器学习，以及使用专注于身体和动作识别的模型。

## 5.1 音频数据

当你第一次读到“音频数据”这几个字时，你可能会认为这本书的这一部分将集中在音乐上；然而，我打算更广泛地使用声音。

我们并不经常思考这个问题，但是我们周围的很多东西都会产生声音，这给了我们周围环境的背景信息。

例如，打雷的声音可以帮助你了解天气可能很糟糕，而不必向窗外看，或者你甚至可以在看到飞机飞过之前就认出它的声音，或者甚至听到海浪的声音表明你可能接近海洋，等等。

如果没有意识到、认识到和理解这些声音的含义，就会影响我们的日常生活和行为。听到敲门声表明有人可能在后面等你开门，或者在你做饭的时候听到开水的声音表明你可以往里面倒东西了。

使用声音数据和机器学习可以帮助我们利用声音的丰富属性来识别某些人类活动，并增强当前的智能系统，如 Siri、Alexa 等。

这就是所谓的**听觉活动识别**。

考虑到我们周围的许多设备都拥有麦克风，这项技术有很多机会。

![../images/496132_1_En_5_Chapter/496132_1_En_5_Fig1_HTML.jpg](../images/496132_1_En_5_Chapter/496132_1_En_5_Fig1_HTML.jpg)

图 5-1

拥有麦克风的个人设备的图示

到目前为止，我们中的一些智能系统可能会使用识别单词来触发命令，但它们并不了解周围发生的事情；你的手机不知道你在浴室，你的 Alexa 设备不知道你可能在厨房，等等。然而，他们可以，这可以用来创造更适合和有用的数字体验。

在我们深入本章的实践部分并了解如何使用 TensorFlow.js 在 JavaScript 中构建这样的系统之前，先了解什么是声音的基础知识，以及如何将声音转换成我们可以在代码中使用的数据是有帮助的。

### 5.1.1 什么是声音？

声音是空气分子的振动。

如果你曾经把扬声器的音量调得很大，你可能会注意到它们最终会随着音乐来回移动。这种运动推动空气粒子，改变气压并产生声波。

同样的现象也发生在言语上。当你说话时，你的声带振动，扰乱周围的空气分子并改变气压，产生声波。

下图可以说明这种现象。

![../images/496132_1_En_5_Chapter/496132_1_En_5_Fig2_HTML.jpg](../images/496132_1_En_5_Chapter/496132_1_En_5_Fig2_HTML.jpg)

图 5-2

声波如何工作的插图。来源: [`www.researchgate.net/figure/Sound-as-a-pressure-wave-The-peaks-represent-times-when-air-molecules-are-clustered_fig2_215646583`](http://www.researchgate.net/figure/Sound-as-a-pressure-wave-The-peaks-represent-times-when-air-molecules-are-clustered_fig2_215646583)

当你敲击音叉时，它会开始振动。这种来回运动会改变周围的气压。向前运动将产生较高的压力，向后运动将产生较低的压力。这种运动的重复会产生波浪。

在接收器方面，我们的耳膜随着压力的变化而振动，然后这种振动被转换成电信号发送到大脑。

那么，如果声音是气压的变化，我们如何将声波转换成我们可以在设备上使用的数据呢？

为了能够解释声音数据，我们的设备使用麦克风。

存在不同类型的麦克风，但是一般来说，这些设备具有当暴露于由声波引起的气压变化时振动的隔膜或薄膜。

这些振动会移动麦克风内部线圈附近的磁铁，从而产生微弱的电流。然后你的计算机将这个信号转换成代表音量和频率的数字。

### 访问音频数据

在 JavaScript 中，允许开发人员访问来自计算机麦克风的数据的 Web API 是 Web Audio API。

如果你以前没用过这个 API，完全没问题；我们将介绍您需要设置的主要内容。

首先，我们需要访问全局`window`对象上的`AudioContext`接口，并确保我们可以通过`getUserMedia`获得访问音频和视频输入设备的权限。

```py
window.AudioContext = window.AudioContext || window.webkitAudioContext;
navigator.getUserMedia = navigator.getUserMedia || navigator.webkitGetUserMedia;

Listing 5-1Setup to use the Web Audio API in JavaScript

```

此代码示例考虑了跨浏览器兼容性。

然后，为了开始监听来自麦克风的输入，我们需要等待页面上的用户操作，例如，单击。

一旦用户与网页进行了交互，我们就可以实例化一个音频上下文，允许访问计算机的音频输入设备，并使用一些 web 音频 API 内置方法来创建一个源和一个分析器，并将这两者连接在一起以开始获取一些数据。

```py
document.body.onclick = async () => {
  const audioctx = new window.AudioContext();
  const stream = await navigator.mediaDevices.getUserMedia({ audio: true });

  const source = audioctx.createMediaStreamSource(stream);
  analyser = audioctx.createAnalyser();
  analyser.smoothingTimeConstant = 0;

  source.connect(analyser);
  analyser.fftSize = 1024;
  getAudioData();
};

Listing 5-2JavaScript code sample to set up the audio context on click

```

在前面的代码示例中，我们使用`navigator.mediaDevices.getUserMedia`来访问麦克风。如果你以前曾经构建过使用音频或视频输入设备的应用程序，你可能对编写`navigator.getUserMedia()`很熟悉；然而，这是不赞成的，你现在应该使用`navigator.mediaDevices.getUserMedia()`。

用老方法写仍然可以，但是不推荐，因为在接下来的几年里可能不支持它。

完成基本设置后，`getAudioData`功能会过滤来自设备的原始数据，仅获取频率数据。

```py
const getAudioData = () => {
  const freqdata = new Uint8Array(analyser.frequencyBinCount);
  analyser.getByteFrequencyData(freqdata);

  console.log(freqdata);

  requestAnimationFrame(getAudioData);
};

Listing 5-3Function to filter through the raw data to get the frequency data we will use

```

我们还调用`requestAnimationFrame`来连续调用这个函数，并用实时数据更新我们正在记录的数据。

总之，您可以用不到 25 行 JavaScript 从麦克风访问实时数据！

```py
window.AudioContext = window.AudioContext || window.webkitAudioContext;
navigator.getUserMedia = navigator.getUserMedia || navigator.webkitGetUserMedia;
let analyser;

document.body.onclick = async () => {
  const audioctx = new window.AudioContext();
  const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
  const source = audioctx.createMediaStreamSource(stream);
  analyser = audioctx.createAnalyser();
  analyser.smoothingTimeConstant = 0;

  source.connect(analyser);
  analyser.fftSize = 1024;
  getAudioData();
};

const getAudioData = () => {
  const freqdata = new Uint8Array(analyser.frequencyBinCount);
  analyser.getByteFrequencyData(freqdata);

  console.log(freqdata);

  requestAnimationFrame(getAudioData);
};

Listing 5-4Complete code sample to get input data from the microphone in JavaScript

```

这段代码的输出是我们在浏览器控制台中记录的一组原始数据。

![../images/496132_1_En_5_Chapter/496132_1_En_5_Fig3_HTML.jpg](../images/496132_1_En_5_Chapter/496132_1_En_5_Fig3_HTML.jpg)

图 5-3

前面的代码示例返回的数据的屏幕截图

这些数组代表组成计算机麦克风记录的声音的频率。默认的采样率是 44，100Hz，这意味着我们每秒钟可以获得大约 44，000 个数据样本。

在前面显示的格式(整数数组)中，寻找模式来识别某种类型的活动似乎相当困难。我们真的无法辨别说话、大笑、音乐播放等等之间的区别。

为了帮助理解这些原始的频率数据，我们可以将它们可视化。

### 5.1.3 可视化音频数据

有不同的方法来形象化声音。你可能熟悉的几种方法是波形图或频率表。

波形可视化器表示声波随时间的位移。

![../images/496132_1_En_5_Chapter/496132_1_En_5_Fig4_HTML.jpg](../images/496132_1_En_5_Chapter/496132_1_En_5_Fig4_HTML.jpg)

图 5-4

波形可视化的图示。来源: [`https://css-tricks.com/making-an-audio-waveform-visualizer-with-vanilla-javascript/`](https://css-tricks.com/making-an-audio-waveform-visualizer-with-vanilla-javascript/)

x 轴(水平轴)是时间单位，y 轴(垂直轴)是频率。声音发生在一段时间内，由多种频率组成。

这种可视化声音的方式有点过于简单，无法识别模式。正如您在前面的插图中所看到的，构成声音的所有频率都减少到了一条线上。

频率表是一种可视化工具，用来衡量波形在给定时间内重复的次数。

![../images/496132_1_En_5_Chapter/496132_1_En_5_Fig5_HTML.jpg](../images/496132_1_En_5_Chapter/496132_1_En_5_Fig5_HTML.jpg)

图 5-5

频率表可视化的图示

你可能熟悉这种类型的音频可视化，因为它们可能是最常见的一种。

这种可视化的方式可能会给你一些关于一个节拍的见解，因为它代表重复，或者可能是关于声音有多响，因为 y 轴显示音量，但仅此而已。

这种可视化没有给我们足够的信息来识别和分类我们正在可视化的声音。

另一种更有帮助的可视化叫做**光谱图**。

声谱图就像声音的图片。它显示了构成声音的频率从低到高，以及它们如何随时间变化。它是信号频谱的可视化表示，有点像声音的热图。

![../images/496132_1_En_5_Chapter/496132_1_En_5_Fig6_HTML.jpg](../images/496132_1_En_5_Chapter/496132_1_En_5_Fig6_HTML.jpg)

图 5-6

声谱图插图

y 轴是频谱，x 轴是时间量。这些轴看起来类似于我们之前提到的另外两种类型的可视化，但是我们不是用一条线代表所有的频率，而是代表整个频谱。

在光谱图中，第三个轴也很有帮助，即振幅。声音的振幅可以用音量来描述。颜色越亮，声音越大。

将声音可视化为声谱图更有助于找到帮助我们识别和分类声音的模式。

例如，接下来是我讲话时运行的频谱图的输出截图。

![../images/496132_1_En_5_Chapter/496132_1_En_5_Fig7_HTML.jpg](../images/496132_1_En_5_Chapter/496132_1_En_5_Fig7_HTML.jpg)

图 5-7

讲话时拍摄的声谱图插图

就其本身而言，这可能无法帮助你理解为什么光谱图是更有帮助的可视化。下面是我拍手三次时拍的另一张声谱图截图。

![../images/496132_1_En_5_Chapter/496132_1_En_5_Fig8_HTML.jpg](../images/496132_1_En_5_Chapter/496132_1_En_5_Fig8_HTML.jpg)

图 5-8

我拍手三次时拍摄的声谱图插图

希望它开始变得更有意义！如果你比较两种光谱图，你可以清楚地区分两种活动:说话和拍手。

如果你愿意，你可以试着想象更多的声音，比如咳嗽声、电话铃声、冲厕所声等等。

总的来说，主要的收获是，声谱图帮助我们更清楚地看到各种声音的特征，并区分不同的活动。

如果我们可以通过查看声谱图的截图来进行区分，我们可以希望使用机器学习算法来使用这些数据也可以找到模式，并对这些声音进行分类，以建立一个活动分类器。

使用光谱图进行活动分类的一个更广泛的例子来自美国卡内基梅隆大学发表的一篇研究论文。在他们题为“Ubicoustics:即插即用声学活动识别”的论文中，他们创建了各种活动的频谱图，从使用链锯到附近行驶的车辆。

![../images/496132_1_En_5_Chapter/496132_1_En_5_Fig9_HTML.jpg](../images/496132_1_En_5_Chapter/496132_1_En_5_Fig9_HTML.jpg)

图 5-9

卡内基梅隆大学收集的光谱图。来源: [`http://www.gierad.com/projects/ubicoustics/`](http://www.gierad.com/projects/ubicoustics/)

因此，在我们开始使用声音和机器学习之前，让我们先来看看如何使用 Web Audio API 将来自麦克风的实时数据转换为声谱图。

#### 创建光谱图

在我们之前编写的代码示例中，我们创建了一个`getAudioData`函数，它从原始数据中获取频率数据，并将其记录到浏览器的控制台中。

```py
const getAudioData = () => {
  const freqdata = new Uint8Array(analyser.frequencyBinCount);
  analyser.getByteFrequencyData(freqdata);

  console.log(freqdata);

  requestAnimationFrame(getAudioData);
};

Listing 5-5getAudioData function to get frequency data from raw data

```

在我们编写`console.log`语句的地方，我们将添加代码来创建可视化。

要做到这一点，我们将使用 Canvas API，所以我们需要从像这样向 HTML 文件添加一个 Canvas 元素开始。

```py
<canvas id="canvas"></canvas>

Listing 5-6Adding a canvas element to the HTML file

```

在我们的 JavaScript 中，我们将能够访问这个元素，并使用 Canvas API 中的一些方法来绘制我们的可视化。

```py
var canvas = document.getElementById("canvas");
var ctx = canvas.getContext("2d");

Listing 5-7Getting the canvas element and context in JavaScript

```

这种可视化的主要概念是绘制随时间变化的频谱，因此我们需要获得当前画布，并在每次获得新的实时数据时重新绘制。

```py
imagedata = ctx.getImageData(1, 0, canvas.width - 1, canvas.height);
ctx.putImageData(imagedata, 0, 0);

Listing 5-8Getting the image data from the canvas element and redrawing over it

```

然后，我们需要遍历从 Web Audio API 获得的频率数据，并将它们绘制到画布上。

```py
for (var i = 0; i < freqdata.length; i++) {
  let value = (2 * freqdata[i]) / 255;

  ctx.beginPath();
  ctx.strokeStyle = `rgba(${Math.max(0, 255 * value)}, ${Math.max(
      0,
      255 * (value - 1)
    )}, 54, 255)`;
  ctx.moveTo(canvas.width - 1, canvas.height - i * (canvas.height / freqdata.length));

    ctx.lineTo(
      canvas.width - 1,
      canvas.height -
        (i * (canvas.height / freqdata.length) +
          canvas.height / freqdata.length)
    );
    ctx.stroke();
  }

Listing 5-9Looping through frequency data and drawing it onto the canvas

```

在这个 for 循环中，我们使用`beginPath`方法来表示我们将开始在画布上绘制一些东西。

然后，我们调用`strokeStyle`并向它传递一个动态值，该值将表示用于显示声音振幅的颜色。

之后，我们调用`moveTo`将可视化向左移动 1 个像素，并留出空间让新输入绘制到屏幕的最右边，用`lineTo`绘制。

最后我们调用`stroke`方法来画线。

总之，我们的`getAudioData`函数应该看起来像这样。

```py
const getAudioData = () => {
  freqdata = new Uint8Array(analyser.frequencyBinCount);
  analyser.getByteFrequencyData(freqdata);
  console.log(freqdata);
  imagedata = ctx.getImageData(1, 0, canvas.width - 1, canvas.height);
  ctx.putImageData(imagedata, 0, 0);

  for (var i = 0; i < freqdata.length; i++) {
    let value = (2 * freqdata[i]) / 255;

    ctx.beginPath();
    ctx.strokeStyle = `rgba(${Math.max(0, 255 * value)}, ${Math.max(
      0,
      255 * (value - 1)
    )}, 54, 255)`;
    ctx.moveTo(
      canvas.width - 1,
      canvas.height - i * (canvas.height / freqdata.length)
    );
    ctx.lineTo(
      canvas.width - 1,
      canvas.height -
        (i * (canvas.height / freqdata.length) +
          canvas.height / freqdata.length)
    );
    ctx.stroke();
  }

  requestAnimationFrame(getAudioData);
};

Listing 5-10Full getAudioData function

```

你可能想知道为什么理解如何创建光谱图很重要。主要原因是它被用作机器学习算法的训练数据。

我们没有像在浏览器控制台中那样使用原始数据，而是使用生成的频谱图图片将声音问题转化为图像问题。

在过去的几年里，图像识别和分类方面的进步真的很好，并且用于图像数据的算法已经被证明是非常高效的。

此外，将声音数据转换为图像意味着我们可以处理更少量的数据来训练模型，这将缩短所需的时间。

事实上，Web Audio API 的默认采样率是 44KHz，这意味着它每秒收集 44，000 个数据样本。

如果我们录制 2 秒钟的音频，那么单个样本就有 88，000 个数据点。

你可以想象，当我们需要记录更多的样本时，最终会有大量的数据被输入到机器学习算法中，这将需要很长时间来训练。

另一方面，提取为图片的声谱图可以容易地调整到更小的尺寸，例如，这可能最终只是 28×28 像素的图像，这将导致 2 秒音频剪辑的 784 个数据点。

现在，我们已经介绍了如何在 JavaScript 中访问麦克风的实时数据，以及如何将其转换为声谱图可视化，从而让我们看到不同的声音如何创建不同的视觉模式，让我们看看如何训练机器学习模型来创建分类器。

### 训练分类器

我们不会为此创建一个定制的机器学习算法，而是使用一个专门针对声音数据的可教机器实验。你可以在 [`https://teachablemachine.withgoogle.com/train/audio`](https://teachablemachine.withgoogle.com/train/audio) 找到它。

![../images/496132_1_En_5_Chapter/496132_1_En_5_Fig10_HTML.jpg](../images/496132_1_En_5_Chapter/496132_1_En_5_Fig10_HTML.jpg)

图 5-10

可示教的机器界面

这个项目允许我们记录声音数据的样本，给它们贴标签，训练机器学习算法，测试输出，并在单一界面和浏览器中导出模型！

首先，我们需要使用下图中用红色突出显示的部分记录 20 秒的背景噪音。

![../images/496132_1_En_5_Chapter/496132_1_En_5_Fig11_HTML.jpg](../images/496132_1_En_5_Chapter/496132_1_En_5_Fig11_HTML.jpg)

图 5-11

突出显示背景噪音部分的可示教机器界面

然后，我们可以开始为我们希望模型稍后识别的任何声音录制一些样本。

样本的最小数量为 8 个，每个样本的长度需要为 2 秒。

![../images/496132_1_En_5_Chapter/496132_1_En_5_Fig12_HTML.jpg](../images/496132_1_En_5_Chapter/496132_1_En_5_Fig12_HTML.jpg)

图 5-12

突出显示自定义部分的可示教机器界面

由于这个实验使用迁移学习来快速重新训练已经用声音数据训练过的模型，所以我们需要使用原始模型训练时使用的相同格式。

八个样本是最少的，但是如果你愿意，你可以记录更多。样本越多越好。但是，不要忘记，这也会影响培训所需的时间。

一旦您记录了样本并贴上标签，您就可以在浏览器中开始实时培训，并确保不要关闭浏览器窗口。

![../images/496132_1_En_5_Chapter/496132_1_En_5_Fig13_HTML.jpg](../images/496132_1_En_5_Chapter/496132_1_En_5_Fig13_HTML.jpg)

图 5-13

可示教的机器界面–训练模型

当这一步完成后，你应该可以在实验的最后一步看到一些现场预测。在导出模型之前，您可以尝试重复录制的声音来验证预测的准确性。如果觉得不够准确，可以多录一些样本，重新开始训练。

![../images/496132_1_En_5_Chapter/496132_1_En_5_Fig14_HTML.jpg](../images/496132_1_En_5_Chapter/496132_1_En_5_Fig14_HTML.jpg)

图 5-14

可示教的机器界面–运行实时预测

如果你准备好继续前进，你可以将你的模型上传到一些谷歌服务器并提供一个链接，或者下载已经创建的机器学习模型。

如果你想更好地了解它在后台是如何工作的，机器学习模型是如何创建的，等等，我建议你看看 GitHub 上的源代码！

尽管我真的很喜欢 Teachable Machine 这样的界面，因为它们允许任何人快速入门和实验，但查看源代码可以揭示一些重要的细节。例如，下一张图片是我如何意识到这个项目使用了迁移学习。

在浏览代码以查看机器学习模型是如何创建的以及训练是如何完成的过程中，我注意到了下面的代码示例。

![../images/496132_1_En_5_Chapter/496132_1_En_5_Fig15_HTML.jpg](../images/496132_1_En_5_Chapter/496132_1_En_5_Fig15_HTML.jpg)

图 5-15

示例来自可教学机器的开源 GitHub 库

在第 793 行，我们可以看到方法`addExample`被调用。这与我们在本书专门讨论图像识别的章节中使用的方法相同，当时我们使用迁移学习来使用新的输入图像快速训练图像分类模型。

如果你决定自己尝试重新创建这个模型，而不通过可教的机器界面，注意这些细节是很重要的。

现在我们已经完成了训练过程，我们可以编写代码来生成预测。

### 预测

在开始编写这段代码之前，我们需要导入 TensorFlow.js 和语音命令模型。

```py
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.3.1/dist/tf.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/speech-commands@0.4.0/dist/speech-commands.min.js"></script>

Listing 5-11Import TensorFlow.js and the speech commands model in an HTML file

```

正如我前面提到的，这个实验使用了迁移学习，所以我们需要导入已经用音频数据训练过的语音命令模型，以便更简单、更快速地入门。

语音命令模型最初被训练来识别和分类口语单词，如“是”、“否”、“上”和“下”。然而，在这里，我们使用的是由活动产生的声音，所以它可能不像我们在样本中使用口语单词那样准确。

在浏览其余的代码示例之前，请确保您已经从 Teachable Machine platform 下载了您的训练模型，将其解压缩，并添加到您的应用程序文件夹中。

下面的代码示例将假设您的模型存储在应用程序根目录下一个名为 activities-model 的文件夹中。

总的来说，您的文件结构应该如下所示:

*   活动-模型/
    *   元数据. json

    *   模型. json

    *   weights.bin

*   index.html

*   索引. js

在我们的 JavaScript 文件中，我们需要创建一个函数来加载我们的模型并启动实时预测，但是在此之前，我们可以创建两个变量来保存模型和元数据文件的路径。

```py
let URL = "http://localhost:8000/activities-model/";
const modelURL = `${URL}/model.json`;
const metadataURL = `${URL}metadata.json`;

Listing 5-12Variables to refer to the model and its metadata

```

您可能已经注意到，我在前面的代码中使用了`localhost:8000`；但是，如果您决定将应用程序发布到生产环境中，请随意更改端口并确保更新它。

然后，我们需要加载模型，并确保在继续之前它已被加载。

```py
const model = window.speechCommands.create(
    "BROWSER_FFT",
    undefined,
    modelURL,
    metadataURL
);
await model.ensureModelLoaded();

Listing 5-13Loading the model

```

一旦模型准备好了，我们就可以通过调用模型上的`listen`方法来运行实时预测。

```py
model.listen(
    (prediction) => {
      predictionCallback(prediction.scores);
    },
    modelParameters
  );

Listing 5-14Live predictions

```

总之，setupModel 函数应该是这样的。

```py
async function setupModel(URL, predictionCB) {
  predictionCallback = predictionCB;
  const modelURL = `${URL}/model.json`;
  const metadataURL = `${URL}metadata.json`;

  model = window.speechCommands.create(
    "BROWSER_FFT",
    undefined,
    modelURL,
    metadataURL
  );
  await model.ensureModelLoaded();

  const modelParameters = {
    invokeCallbackOnNoiseAndUnknown: true, // run even when only background noise is detected
    includeSpectrogram: true,
    overlapFactor: 0.5, // how often per second to sample audio, 0.5 means twice per second
  };

  model.listen(
    (prediction) => {
      predictionCallback(prediction.scores);
    },
    modelParameters
  );
}

Listing 5-15Full code sample

```

调用时，此函数将在每次模型有预测时调用的回调中包含预测数据。

```py
document.body.onclick = () => {
  setupModel(URL, (data) => {
     console.log(data)
  });
}

Listing 5-16Calling the function

```

![../images/496132_1_En_5_Chapter/496132_1_En_5_Fig16_HTML.jpg](../images/496132_1_En_5_Chapter/496132_1_En_5_Fig16_HTML.jpg)

图 5-16

调用函数时返回的数据示例

这个包含预测结果的数组按所用标签排序。在前面的示例中，我用六个不同的标签训练了模型，因此返回的每个数组包含六个值。

在 ean ach 数组中，最接近 1 的值表示预测的标签。

为了将预测的数据与正确的标签相匹配，我们可以创建一个包含用于训练的标签的数组，并在调用`setupModel`函数时使用它。

```py
const labels = [
  "Coughing",
  "Phone ringing",
  "Speaking",
  "_background_noise_",
];
let currentPrediction;

document.body.onclick = () => {
  setupModel(URL, (data) => {
    let maximum = Math.max(...data);
    if (maximum > 0.7) {
      let maxIndex = data.indexOf(maximum);
      currentPrediction = labels[maxIndex];
      console.log(currentPrediction);
    }
  });
}

Listing 5-17Mapping scores to labels

```

在不到 100 行的 JavaScript 中，我们能够加载并运行一个可以对现场音频输入进行分类的机器学习模型！

```py
let model, predictionCallback;
let URL = "http://localhost:8000/activities-model/";

const labels = [
  "Coughing",
  "Phone ringing",
  "Speaking",
  "_background_noise_",
];

let currentPrediction, previousPrediction;
currentPrediction = previousPrediction;

document.body.onclick = () => {
  setupModel(URL, (data) => {
    let maximum = Math.max(...data);
    if (maximum > 0.7) {
      let maxIndex = data.indexOf(maximum);
      currentPrediction = labels[maxIndex];
      console.log(currentPrediction);
    }
  });
};

async function setupModel(URL, predictionCB) {
  const modelURL = `${URL}/model.json`;
  const metadataURL = `${URL}metadata.json`;

  model = window.speechCommands.create(
    "BROWSER_FFT",
    undefined,
    modelURL,
    metadataURL
  );
  await model.ensureModelLoaded();

  // This tells the model how to run when listening for audio
  const modelParameters = {
    invokeCallbackOnNoiseAndUnknown: true, // run even when only background noise is detected
    includeSpectrogram: true, // give us access to numerical audio data
    overlapFactor: 0.5, // how often per second to sample audio, 0.5 means twice per second
  };

  model.listen(
    (prediction) => {
      predictionCallback(prediction.scores);
    },
    modelParameters
  );
}

Listing 5-18Full code sample

```

### 5.1.6 迁移学习 API

在上一节中，为了简单起见，我们介绍了如何记录声音样本并使用可示教机器实验来训练模型。但是，如果您希望在自己的应用程序中实现这一点，并让用户自己运行相同的过程，您可以使用迁移学习 API。

这个 API 允许您构建自己的接口并调用 API 端点来记录样本、训练模型和运行实时预测。

#### 记录样本

让我们想象一个只有几个按钮的非常简单的 web 界面。

![../images/496132_1_En_5_Chapter/496132_1_En_5_Fig17_HTML.jpg](../images/496132_1_En_5_Chapter/496132_1_En_5_Fig17_HTML.jpg)

图 5-17

带有几个按钮元素的 Web 界面

其中一些按钮用于收集样本数据，一个按钮用于开始训练，最后一个按钮用于触发实时预测。

首先，我们需要一个包含这六个按钮和两个脚本标签的 HTML 文件来导入 TensorFlow.js 和语音命令模型。

```py
<html lang="en">
  <head>
    <title>Speech recognition</title>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.3.1/dist/tf.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/speech-commands@0.4.0/dist/speech-commands.min.js"></script>
  </head>
  <body>
    <section>
        <button id="red">Red</button>
        <button id="blue">Blue</button>
        <button id="green">Green</button>
        <button id="background">Background</button>
        <button id="train">Train</button>
        <button id="predict">Predict</button>
    </section>
    <script src="index.js"></script>
  </body>
</html>

Listing 5-19HTML file

```

在 JavaScript 文件中，在能够运行这些操作之前，我们需要创建模型，确保它被加载，并向我们的模型传递一个主标签，以创建一个包含音频样本的集合。

```py
const init = async () => {
  const baseRecognizer = speechCommands.create("BROWSER_FFT");
  await baseRecognizer.ensureModelLoaded();
  transferRecognizer = baseRecognizer.createTransfer("colors");
};

Listing 5-20Set up the recognizers

```

然后，我们可以在按钮上添加事件侦听器，这样它们就可以在单击时收集样本。为此，我们需要在识别器上调用`collectExample`方法，并向它传递一个我们希望用来标记样本的字符串。

```py
const redButton = document.getElementById("red");
redButton.onclick = async () => await transferRecognizer.collectExample("red");

Listing 5-21Collecting samples

```

为了开始训练，我们调用识别器上的`train`方法。

```py
const trainButton = document.getElementById("train");
trainButton.onclick = async () => {
  await transferRecognizer.train({
    epochs: 25,
    callback: {
      onEpochEnd: async (epoch, logs) => {
        console.log(`Epoch ${epoch}: loss=${logs.loss}, accuracy=${logs.acc}`);
      },
    },
  });
};

Listing 5-22Training

```

最后，为了在训练后对现场音频输入进行分类，我们调用了`listen`方法。

```py
const predictButton = document.getElementById("predict");
predictButton.onclick = async () => {
  await transferRecognizer.listen(
    (result) => {
      const words = transferRecognizer.wordLabels();
      for (let i = 0; i < words.length; ++i) {
        console.log(`score for word '${words[i]}' = ${result.scores[i]}`);
      }
    },
    { probabilityThreshold: 0.75 }
  );
};

Listing 5-23Predict

```

总之，该代码示例如下所示。

```py
let transferRecognizer;

const init = async () => {
  const baseRecognizer = speechCommands.create("BROWSER_FFT");
  await baseRecognizer.ensureModelLoaded();
  transferRecognizer = baseRecognizer.createTransfer("colors");
};

init();

const redButton = document.getElementById("red");
const backgroundButton = document.getElementById("background");
const trainButton = document.getElementById("train");
const predictButton = document.getElementById("predict");

redButton.onclick = async () => await transferRecognizer.collectExample("red");

backgroundButton.onclick = async () =>
  await transferRecognizer.collectExample("_background_noise_");

trainButton.onclick = async () => {
  await transferRecognizer.train({
    epochs: 25,
    callback: {
      onEpochEnd: async (epoch, logs) => {
        console.log(`Epoch ${epoch}: loss=${logs.loss}, accuracy=${logs.acc}`);
      },
    },
  });
};

predictButton.onclick = async () => {
  await transferRecognizer.listen(
    (result) => {
      const words = transferRecognizer.wordLabels();
      for (let i = 0; i < words.length; ++i) {
        console.log(`score for word '${words[i]}' = ${result.scores[i]}`);
      }
    },
    { probabilityThreshold: 0.75 }
  );
};

Listing 5-24Full code sample

```

### 应用程序

尽管到目前为止我在我们的代码示例(说话和咳嗽)中使用的例子可能看起来很简单，但这项技术目前的使用方式表明了它有多么有趣。

#### 健康

2020 年 7 月，苹果宣布发布新版本的 watchOS，其中包括一个当用户洗手时触发倒计时的应用程序。与公共卫生官员关于避免新冠肺炎病毒传播的建议相关，该应用程序使用手表的麦克风来检测流水的声音，并触发 20 秒倒计时。

![../images/496132_1_En_5_Chapter/496132_1_En_5_Fig19_HTML.jpg](../images/496132_1_En_5_Chapter/496132_1_En_5_Fig19_HTML.jpg)

图 5-19

苹果手表上的倒计时界面

![../images/496132_1_En_5_Chapter/496132_1_En_5_Fig18_HTML.jpg](../images/496132_1_En_5_Chapter/496132_1_En_5_Fig18_HTML.jpg)

图 5-18

用户洗手时触发倒计时

从最后几页显示的代码示例中，可以使用 JavaScript 和 TensorFlow.js 构建一个类似的应用程序。

![../images/496132_1_En_5_Chapter/496132_1_En_5_Fig20_HTML.jpg](../images/496132_1_En_5_Chapter/496132_1_En_5_Fig20_HTML.jpg)

图 5-20

使用 TensorFlow.js 的类似倒计时界面原型

#### 生物多样性研究和保护

这项技术我最喜欢的应用之一是生物多样性研究和濒危物种保护。

一个很好的例子是雨林连接集体。

这个集体使用旧手机和内置麦克风来探测森林中链锯的声音，并警告护林员非法砍伐森林的潜在活动。

使用太阳能电池板并将装置安装在树上，他们可以持续监控周围发生的事情，并进行实时预测。

![../images/496132_1_En_5_Chapter/496132_1_En_5_Fig21_HTML.jpg](../images/496132_1_En_5_Chapter/496132_1_En_5_Fig21_HTML.jpg)

图 5-21

由太阳能电池板和二手手机制成的装置示例。来源: [`https://www.facebook.com/RainforestCx/`](https://www.facebook.com/RainforestCx/)

如果这是一个你感兴趣的项目，他们也有一个名为雨林连接的移动应用程序，在其中你可以听听大自然的声音，从森林中直播，如果你想看看的话！

这项技术的另一个用途是保护虎鲸。谷歌、雨林连接、加拿大渔业和海洋部(DFO)之间的合作使用生物声学监测来跟踪、监测和观察动物在萨利希海的行为。

![../images/496132_1_En_5_Chapter/496132_1_En_5_Fig22_HTML.jpg](../images/496132_1_En_5_Chapter/496132_1_En_5_Fig22_HTML.jpg)

图 5-22

追踪虎鲸的网络界面。来源: [`https://venturebeat.com/2020/01/28/googles-ai-powers-real-time-orca-tracking-in-vancouver-bay/`](https://venturebeat.com/2020/01/28/googles-ai-powers-real-time-orca-tracking-in-vancouver-bay/)

#### 网页可访问性

您可能没有注意到的另一个应用程序目前正在您可能知道的服务中实现。事实上，如果你正在使用 YouTube，你可能会遇到现场环境声音字幕。

如果你曾经在 YouTube 视频上激活过字幕，你可能知道说的话会显示在底部的覆盖层上。

然而，视频中的信息比文字记录中的信息更多。

事实上，没有听力障碍的人受益于以背景声音的形式获得额外的信息，如音乐播放或视频中的雨声。

对于有听力障碍的人来说，只有在字幕中显示口语单词才能删除相当多的信息。

大约 3 年前，在 2017 年，YouTube 发布了现场环境声音字幕，使用声学识别将视频配乐中检测到的环境声音的细节添加到字幕中。

这里有一个例子。

![../images/496132_1_En_5_Chapter/496132_1_En_5_Fig23_HTML.jpg](../images/496132_1_En_5_Chapter/496132_1_En_5_Fig23_HTML.jpg)

图 5-23

YouTube 上的现场环境声音字幕示例

前面的截图是从加奈儿·梦奈和法瑞尔·威廉姆斯的访谈中截取的，字幕是激活的。

口语单词按预期显示，但我们也可以看到类似[掌声]的环境声音。

有听力障碍的人现在可以有机会获得比对话更多的视频信息。

目前，YouTube 视频上可以检测到的环境声音包括

*   喝彩

*   音乐播放

*   笑

这可能看起来不多，但同样，如果我们从来没有想过一些残疾人在这些平台上的经历，这是我们理所当然的事情。

此外，想想这个功能在大约 3 年前就已经实现了，这已经表明，像谷歌这样的主要技术公司一直在积极探索将机器学习用于音频数据的潜力，并一直在努力寻找有用的应用程序。

### 限制

既然我们已经介绍了如何在 JavaScript 和一些不同的应用程序中试验声学活动识别，那么了解这种技术的一些限制以更好地理解真正的机会是很重要的。

#### 数据的质量和数量

如果您决定从头构建一个类似的声学活动识别系统，并在不使用迁移学习和 TensorFlow.js 的语音命令模型的情况下编写自己的模型，您将需要收集比使用可示教机器时所需的最低 8 个声音样本多得多的声音样本。

要收集大量样本，您可以决定自己录制或从专业音频库中购买。

另一个要点是确保检查记录数据的质量。例如，如果您想要检测真空吸尘器运行的声音，请确保没有背景噪音，并且可以在音轨中清楚地听到真空吸尘器的声音。

从单个音频源生成数据样本的一个技巧是使用音频编辑软件来改变单个音频源的一些参数，以创建它的多个版本。例如，您可以修改混响、音高等。

![../images/496132_1_En_5_Chapter/496132_1_En_5_Fig24_HTML.jpg](../images/496132_1_En_5_Chapter/496132_1_En_5_Fig24_HTML.jpg)

图 5-24

转换声音。Gierad Laput，Karan Ahuja，Mayank Goel 和 Chris Harrison。2018.声学:即插即用声学活动识别。在第 31 届 ACM 用户界面软件和技术年会上。美国纽约州纽约市 ACM，213-224。土井: [`https://doi.org/10.1145/3242587.3242609`](https://doi.org/10.1145/3242587.3242609)

#### 单一活动

目前，这项技术似乎能有效地一次识别出一个声音。

例如，如果您训练您的模型识别某人说话的声音以及流水的声音，如果您将您的系统放置在厨房中，用户在说话的同时洗碗，则预测的活动将仅是返回的预测中得分最高的活动。

然而，当系统连续运行时，它可能会在两个活动之间混淆。它可能会在“说话”和“流水”之间交替，直到其中一个活动停止。

如果您构建的应用程序可以检测同时执行的活动产生的声音，这肯定会成为一个问题。

例如，假设您通常在洗澡时播放音乐，并且您构建了一个可以检测两种活动的应用程序:淋浴运行和说话的声音。

您希望能够在检测到淋浴正在运行时触发计数器，这样您就可以避免长时间淋浴并节约用水。

您还希望能够在检测到有人在浴室说话时降低扬声器的声音。

由于这两种活动可以同时发生(你可以在洗澡时说话)，系统可能会混淆这两种活动，并检测到淋浴正在进行一秒钟，而有人正在说话。

因此，它会在这一秒启动和停止扬声器，下一秒启动/停止计数器。这肯定不会创造理想的体验。

然而，这并不意味着使用声音活动识别构建应用程序没有潜力，只是意味着我们需要解决这一限制。

此外，一些研究正在围绕开发可以同时处理多种活动检测的系统进行。我们将在接下来的几页中对此进行研究。

#### 用户体验

当谈到用户体验时，像这样的新技术总是有一些挑战。

首先，**隐私**。

让设备监听用户总是会引发一些关于数据存储在哪里、如何使用、是否安全等问题。

考虑到一些发布物联网设备的公司并不总是将安全放在产品的第一位，这些担忧是非常正常的。

因此，消费者对这些设备的采用可能比预期的要慢。

不仅隐私和安全应该在这些系统中得到体现，还应该以一种清晰的方式传达给用户，让他们放心，并让他们有一种对自己数据的掌控感。

其次，另一个挑战是教用户**新的** **互动**。

例如，即使现在大多数现代手机都内置了语音助手，但通过询问 Siri 或谷歌来获取信息并不是主要的交互方式。

这可能是出于各种原因，包括隐私和技术本身的限制，但人们也有难以改变的习惯。

此外，考虑到这项技术目前的不完善状态，用户很容易在几次尝试后放弃，因为他们没有得到他们想要的响应。

减轻这种情况的一种方法是发布小应用程序来分析用户对它们的反应并进行调整。苹果公司在其新 watchOS 中实现水检测的工作就是一个例子。

最后，创建定制声音活动识别系统的一大挑战是收集样本数据的**和用户的培训**。

尽管您可以构建并发布一个应用程序来检测水龙头的运行声音，因为在大多数家庭中它很可能会发出类似的声音，但其他一些声音并不常见。

因此，让用户使用这项技术需要让他们记录自己的样本并训练模型，这样他们就有机会拥有一个定制的应用程序。

然而，由于机器学习算法需要用大量数据进行训练，才有机会产生准确的预测，这将需要用户付出大量努力，并且不可避免地不会成功。

幸运的是，一些研究人员正在试验解决这些问题的方法。

现在，尽管这项技术有一些限制，解决方案也开始出现。

例如，在保护用户隐私方面，bjrn Karmann 的一个名为 Project Alias 的开源项目试图为语音助手用户赋能。

这个项目是一个 DIY 的附加项目，由一个 Raspberry Pi 微控制器、一个扬声器和麦克风模块组成，所有这些都在一个 3D 打印的外壳中，旨在阻止亚马逊 Alexa 和谷歌 Home 等语音助手连续听人说话。

通过移动应用程序，用户可以训练 Alias 对自定义的唤醒词或声音做出反应。一旦经过训练，Alias 就可以控制家庭助理并为您激活它。当您不使用它时，该插件将通过向他们的麦克风发出白噪声来阻止助理收听。

Alias 的神经网络在本地运行，用户的隐私得到保护。

![../images/496132_1_En_5_Chapter/496132_1_En_5_Fig26_HTML.jpg](../images/496132_1_En_5_Chapter/496132_1_En_5_Fig26_HTML.jpg)

图 5-26

项目别名组件。来源: [`https://bjoernkarmann.dk/project_alias`](https://bjoernkarmann.dk/project_alias)

![../images/496132_1_En_5_Chapter/496132_1_En_5_Fig25_HTML.jpg](../images/496132_1_En_5_Chapter/496132_1_En_5_Fig25_HTML.jpg)

图 5-25

项目别名。来源: [`https://bjoernkarmann.dk/project_alias`](https://bjoernkarmann.dk/project_alias)

另一个名为合成传感器的项目旨在创造一个可以同时准确预测多种声音的系统。

该项目由卡内基梅隆大学的一个研究团队开发，涉及一个由多个传感器组成的定制硬件，包括加速度计、麦克风、温度传感器、运动传感器和颜色传感器。

使用从这些传感器收集的原始数据，研究人员创建了多个堆叠光谱图，并训练算法来检测由多个活动产生的模式，例如

*   微波炉门关闭

*   运行中的木锯

*   水壶开着

*   水龙头开着

*   厕所冲水

![../images/496132_1_En_5_Chapter/496132_1_En_5_Fig28_HTML.jpg](../images/496132_1_En_5_Chapter/496132_1_En_5_Fig28_HTML.jpg)

图 5-28

项目合成传感器光谱图和活动识别示例。吉拉·拉普特，张旸和克里斯·哈里逊。2017.合成传感器:走向通用传感。在 2017 CHI 计算系统中人的因素会议(CHI '17)的会议录中。美国纽约州纽约市 ACM，3986-3999。土井: [`https://doi.org/10.1145/3025453.3025773`](https://doi.org/10.1145/3025453.3025773)

![../images/496132_1_En_5_Chapter/496132_1_En_5_Fig27_HTML.jpg](../images/496132_1_En_5_Chapter/496132_1_En_5_Fig27_HTML.jpg)

图 5-27

项目合成传感器硬件。吉拉·拉普特，张旸和克里斯·哈里逊。2017.合成传感器:走向通用传感。在 2017 CHI 计算系统中人的因素会议(CHI '17)的会议录中。美国纽约州纽约市 ACM，3986-3999。土井:[T0](https://doi.org/10.1145/3025453.3025773)。

最后，在用户体验方面，一个名为 Listen Learner 的研究项目旨在让用户以最小的努力收集数据并训练一个模型来识别自定义声音。

这个项目的全称是“聆听学习者，自动课堂发现和活动识别的一次性互动”。

它旨在提供高分类准确度，同时最小化用户负担，通过连续收听其环境中的声音，通过相似声音的聚类对它们进行分类，并在收集了足够多的相似样本后询问用户声音是什么。

研究结果表明，该系统能够准确、自动地学习声音事件(例如，97%的准确率、87%的召回率)，同时坚持用户对非侵入式交互行为的偏好。

![../images/496132_1_En_5_Chapter/496132_1_En_5_Fig29_HTML.jpg](../images/496132_1_En_5_Chapter/496132_1_En_5_Fig29_HTML.jpg)

图 5-29

Wu，j .，Harrison，c .，Bigham，j .，Laput，G. 2020。用于声学活动识别的自动类别发现和一次性交互。第 38 届 SIGCHI 计算系统中人的因素年会论文集。池 20。纽约州纽约市 ACM。来源: [`www.chrisharrison.net/index.php/Research/ListenLearner`](https://www.chrisharrison.net/index.php/Research/ListenLearner)

## 5.2 身体和运动跟踪

在了解了如何对音频数据使用机器学习之后，我们来看看另一种类型的输入，即身体跟踪。

在本节中，我们将使用三种不同的 Tensorlfow.js 模型，通过网络摄像头使用来自身体运动的数据。

### 面网

我们要实验的第一个模型叫做 Facemesh。它是一个专注于人脸识别的机器学习模型，可以预测用户脸上 486 个 3D 面部标志的位置，返回带有 x，y 和 z 坐标的点。

![../images/496132_1_En_5_Chapter/496132_1_En_5_Fig31_HTML.jpg](../images/496132_1_En_5_Chapter/496132_1_En_5_Fig31_HTML.jpg)

图 5-31

关键点地图。来源: [`https://github.com/tensorflow/tfjs-models/tree/master/facemesh`](https://github.com/tensorflow/tfjs-models/tree/master/facemesh)

![../images/496132_1_En_5_Chapter/496132_1_En_5_Fig30_HTML.jpg](../images/496132_1_En_5_Chapter/496132_1_En_5_Fig30_HTML.jpg)

图 5-30

用 Facemesh 可视化人脸跟踪的例子。来源: [`https://github.com/tensorflow/tfjs-models/tree/master/facemesh`](https://github.com/tensorflow/tfjs-models/tree/master/facemesh)

该人脸识别模型与 face-tracking.js 等其他人脸跟踪 JavaScript 库的主要区别在于，TensorFlow.js 模型旨在逼近人脸的表面几何形状，而不仅仅是一些关键点的 2D 位置。

该模型提供了 3D 环境中的坐标，即使当用户在三维空间中旋转他们的面部时，也允许近似面部特征的深度以及跟踪关键点的位置。

![../images/496132_1_En_5_Chapter/496132_1_En_5_Fig32_HTML.jpg](../images/496132_1_En_5_Chapter/496132_1_En_5_Fig32_HTML.jpg)

图 5-32

使用网络摄像头和 3D 可视化的要点。来源: [`https://storage.googleapis.com/tfjs-models/demos/facemesh/index.html`](https://storage.googleapis.com/tfjs-models/demos/facemesh/index.html)

#### 加载模型

要开始使用这个模型，我们需要在 HTML 文件中使用下面两行来加载它。

```py
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
<script src='https://cdn.jsdelivr.net/npm/@tensorflow-models/facemesh'></script>

Listing 5-25Importing TensorFlow.js and Facemesh in an HTML file

```

因为我们将使用来自网络摄像头的视频来检测人脸，所以我们还需要在文件中添加一个视频元素。

总之，你最少需要的 HTML 如下。

```py
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Facemesh</title>
  </head>
  <body>

    <video></video>

    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/facemesh"></script>
    <script src="index.js"></script>
  </body>
</html>

Listing 5-26Core HTML code needed

```

然后，在您的 JavaScript 代码中，您需要使用以下代码加载模型和网络摄像头提要。

```py
let model;
let video;
const init = async () => {
  model = await facemesh.load();
  video = await loadVideo();
  main(); // This will be declared in the next code sample.
}

const loadVideo = async () => {
  const video = await setupCamera();
  video.play();
  return video;
};

const setupCamera = async () => {
  if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
    throw new Error(
      "Browser API navigator.mediaDevices.getUserMedia not available"
    );
  }

  video = document.querySelector("video");

  video.width = window.innerWidth;
  video.height = window.innerHeight;

  const stream = await navigator.mediaDevices.getUserMedia({
    audio: false,
    video: {
      facingMode: "user",
      width: window.innerWidth,
      height: window.innerHeight,
    },
  });
  video.srcObject = stream;

  return new Promise(
    (resolve) => (video.onloadedmetadata = () => resolve(video))
  );
};

Listing 5-27Load the model and set up the webcam feed

```

#### 预言

一旦模型和视频准备好了，我们就可以调用我们的`main`函数在输入流中找到面部标志。

```py
async function main() {
  const predictions = await model.estimateFaces(
    document.querySelector("video")
  );

  if (predictions.length > 0) {
    console.log(predictions);

    for (let i = 0; i < predictions.length; i++) {
      const keypoints = predictions[i].scaledMesh;

      // Log facial keypoints.
      for (let i = 0; i < keypoints.length; i++) {
        const [x, y, z] = keypoints[i];

        console.log(`Keypoint ${i}: [${x}, ${y}, ${z}]`);
      }
    }
  }
}

Listing 5-28Function to find face landmarks

```

此代码示例在浏览器控制台中的输出返回以下内容。

![../images/496132_1_En_5_Chapter/496132_1_En_5_Fig34_HTML.jpg](../images/496132_1_En_5_Chapter/496132_1_En_5_Fig34_HTML.jpg)

图 5-34

控制台中循环语句的输出

![../images/496132_1_En_5_Chapter/496132_1_En_5_Fig33_HTML.jpg](../images/496132_1_En_5_Chapter/496132_1_En_5_Fig33_HTML.jpg)

图 5-33

控制台中地标的输出

正如我们在前面两个截图中看到的，返回的预测包含了大量的信息。

注释按字母顺序由标志区域组织，并包含 x、y 和 z 坐标数组。

边界框包含两个主键`bottomRight`和`topLeft`，用于指示视频流中检测到的人脸位置的边界。这两个属性包含一个只有两个坐标 x 和 y 的数组，因为 z 轴在这种情况下没有用。

最后，`mesh`和`scaledMesh`属性包含了所有地标的坐标，并有助于在屏幕上呈现 3D 空间中的所有点。

#### 完整代码示例

总之，设置模型、视频提要并开始预测地标位置的 JavaScript 代码应该如下所示。

```py
let video;
let model;

const init = async () => {
  video = await loadVideo();
  await tf.setBackend("webgl");
  model = await facemesh.load();
  main();
};

const loadVideo = async () => {
  const video = await setupCamera();
  video.play();
  return video;
};

const setupCamera = async () => {
  if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
    throw new Error(
      "Browser API navigator.mediaDevices.getUserMedia not available"
    );
  }

  video = document.querySelector("video");

  video.width = window.innerWidth;
  video.height = window.innerHeight;

  const stream = await navigator.mediaDevices.getUserMedia({
    audio: false,
    video: {
      facingMode: "user",
      width: window.innerWidth,
      height: window.innerHeight,
    },
  });
  video.srcObject = stream;

  return new Promise(
    (resolve) => (video.onloadedmetadata = () => resolve(video))
  );
};

init();

async function main() {
  const predictions = await model.estimateFaces(
    document.querySelector("video")
  );

  if (predictions.length > 0) {
    console.log(predictions);

    for (let i = 0; i < predictions.length; i++) {
      const keypoints = predictions[i].scaledMesh;

      // Log facial keypoints.
      for (let i = 0; i < keypoints.length; i++) {
        const [x, y, z] = keypoints[i];

        console.log(`Keypoint ${i}: [${x}, ${y}, ${z}]`);
      }
    }
  }
  requestAnimationFrame(main);
}

Listing 5-29Full JavaScript code sample

```

#### 项目

为了将这个代码示例付诸实践，让我们构建一个快速原型，允许用户通过前后倾斜头部来向下滚动页面。

我们将能够重用以前编写的大部分代码，并使用检测到的一些地标进行一些小的修改来触发滚动。

我们将要用来检测头部运动的特定标志是`lipsLowerOuter`，更准确地说是它的 z 轴。

查看 annotations 对象中所有可用的属性，使用`lipsLowerOuter`最接近下巴，因此我们可以查看该区域的 z 坐标的预测变化，以确定头部是向后倾斜(下巴向前移动)还是向前倾斜(下巴向后移动)。

为了做到这一点，在我们的`main`函数中，一旦我们得到了预测，我们可以添加下面几行代码。

```py
if (predictions[0].annotations.lipsLowerOuter) {
   let zAxis = predictions[0].annotations.lipsLowerOuter[9][2];

   if (zAxis > 5) {
     // Scroll down
     window.scrollTo({
       top: (scrollPosition += 10),
       left: 0,
       behavior: "smooth",
     });
   } else if (zAxis < -5) {
     // Scroll up
     window.scrollTo({
      top: (scrollPosition -= 10),
      left: 0,
      behavior: "smooth",
    });
   }
}

Listing 5-30Triggering scroll when z axis changes

```

在这个代码示例中，我声明了一个名为`zAxis`的变量来存储我想要跟踪的 z 坐标的值。为了获得这个值，我查看了包含在 annotations 对象的`lipsLowerOuter`属性中的坐标数组。

根据返回的注释对象，我们可以看到`lipsLowerOuter`属性包含 10 个数组，每个数组包含 3 个值。

![../images/496132_1_En_5_Chapter/496132_1_En_5_Fig35_HTML.jpg](../images/496132_1_En_5_Chapter/496132_1_En_5_Fig35_HTML.jpg)

图 5-35

使用 lipsLowerOuter 值返回的批注

这就是为什么刚才显示的代码示例使用`predictions[0].annotations.lipsLowerOuter[9][2]`访问 z 坐标。

我决定访问`lipsLowerOuter`属性的最后一个元素([9])及其第三个值([2])，即该部分的 z 坐标。

值 5 是在反复试验之后选择的，并且是为了查看什么阈值适合这个特定的项目。这不是每次使用面网格模型时都需要使用的标准值。相反，在记录变量`zAxis`并在我前后倾斜头部时看到它的值在浏览器控制台中变化后，我决定这是我使用的正确阈值。

那么，假设你在代码前面声明了`scrollPosition`，并将其设置为一个值(我个人设置为 0)，当你向后倾斜头部时会发生“向上滚动”事件，当你向前倾斜头部时会发生“向下滚动”事件。

最后，我将属性`behavior`设置为“smooth ”,这样我们就有了一些平滑的滚动，在我看来，这创造了更好的体验。

如果您没有向 HTML 文件中添加任何内容，您将不会看到任何事情发生，所以不要忘记添加足够的文本或图像来测试一切是否正常！

在不到 75 行的 JavaScript 中，我们加载了一个人脸识别模型，设置了视频流，运行预测以获得面部标志的 3D 坐标，并编写了一些逻辑来触发当你的头向后或向前倾斜时的向上或向下滚动！

```py
let video;
let model;

const init = async () => {
  video = await loadVideo();
  await tf.setBackend("webgl");
  model = await facemesh.load();
  main();
};

const loadVideo = async () => {
  const video = await setupCamera();
  video.play();
  return video;
};

const setupCamera = async () => {
  if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
    throw new Error(
      "Browser API navigator.mediaDevices.getUserMedia not available"
    );
  }

  video = document.querySelector("video");
  video.width = window.innerWidth;
  video.height = window.innerHeight;

  const stream = await navigator.mediaDevices.getUserMedia({
    audio: false,
    video: {
      facingMode: "user",
      width: window.innerWidth,
      height: window.innerHeight,
    },
  });
  video.srcObject = stream;

  return new Promise(
    (resolve) => (video.onloadedmetadata = () => resolve(video))
  );
};

init();

let scrollPosition = 0;

async function main() {
  const predictions = await model.estimateFaces(
    document.querySelector("video")
  );

  if (predictions.length > 0) {
    if (predictions[0].annotations.lipsLowerOuter) {
      zAxis = predictions[0].annotations.lipsLowerOuter[9][2];

      if (zAxis > 5) {
        // Scroll down
        window.scrollTo({
          top: (scrollPosition += 10),
          left: 0,
          behavior: "smooth",
        });
      } else if (zAxis < -5) {
        // Scroll up
        window.scrollTo({
          top: (scrollPosition -= 10),
          left: 0,
          behavior: "smooth",
        });
      }
    }
  }
  requestAnimationFrame(main);
}

Listing 5-31Complete JavaScript code

```

该模型专门用于检测面部标志。接下来，我们将研究另一个，检测用户手中的关键点。

### 手选

我们要实验的第二个模型叫做 Handpose。该模型专门识别用户手中 21 个 3D 关键点的位置。

下面是这个模型的输出示例，一旦使用 Canvas API 在屏幕上可视化。

![../images/496132_1_En_5_Chapter/496132_1_En_5_Fig37_HTML.jpg](../images/496132_1_En_5_Chapter/496132_1_En_5_Fig37_HTML.jpg)

图 5-37

来自手选的关键点被可视化。来源: [`https://github.com/tensorflow/tfjs-models/tree/master/handpose`](https://github.com/tensorflow/tfjs-models/tree/master/handpose)

![../images/496132_1_En_5_Chapter/496132_1_En_5_Fig36_HTML.jpg](../images/496132_1_En_5_Chapter/496132_1_En_5_Fig36_HTML.jpg)

图 5-36

来自手选的关键点被可视化。来源: [`https://github.com/tensorflow/tfjs-models/tree/master/handpose`](https://github.com/tensorflow/tfjs-models/tree/master/handpose)

要实现这一点，如果您已经阅读了上一节，那么代码行将会非常熟悉。

#### 加载模型

我们首先需要 TensorFlow.js 和 Handpose 模型:

```py
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/handpose"></script>

Listing 5-32Import

TensorFlow.js and the Handpose model

```

类似于 Facemesh 模型的工作方式，我们将使用视频流作为输入，因此我们还需要在主 HTML 文件中添加一个视频元素。

然后，在您的 JavaScript 文件中，我们可以使用之前编写的相同函数来设置相机和加载模型。我们需要修改的唯一一行是我们在模型上调用`load`方法的那一行。

由于我们使用的是 Handpose 而不是 Facemesh，我们需要用`handpose.load()`替换`facemesh.load()`。

因此，总的来说，JavaScript 文件的基础应该有以下代码。

```py
let video;
let model;

const init = async () => {
  video = await loadVideo();
  await tf.setBackend("webgl");
  model = await handpose.load();
};

const loadVideo = async () => {
  const video = await setupCamera();
  video.play();
  return video;
};

const setupCamera = async () => {
  if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
    throw new Error(
      "Browser API navigator.mediaDevices.getUserMedia not available"
    );
  }

  video = document.querySelector("video");

  video.width = window.innerWidth;
  video.height = window.innerHeight;

  const stream = await navigator.mediaDevices.getUserMedia({
    audio: false,
    video: {
      facingMode: "user",
      width: window.innerWidth,
      height: window.innerHeight,
    },
  });
  video.srcObject = stream;

  return new Promise(
    (resolve) => (video.onloadedmetadata = () => resolve(video))
  );
};

init();

Listing 5-33Code to set up to load the model and video input

```

#### 预测关键点

一旦模型被加载并且网络摄像头馈送被设置，当一只手放在网络摄像头前面时，我们可以运行预测并检测关键点。

为此，我们可以复制我们在使用 Facemesh 时创建的`main()`函数，但将表达式`model.estimateFaces`替换为`model.estimateHands`。

因此，主要函数应该如下所示。

```py
async function main() {
  const predictions = await model.estimateHands(
    document.querySelector("video")
  );

  if (predictions.length > 0) {
    console.log(predictions);
  }
  requestAnimationFrame(main);
}

Listing 5-34Run predictions and log the output

```

此代码的输出将在浏览器的控制台中记录以下数据。

![../images/496132_1_En_5_Chapter/496132_1_En_5_Fig38_HTML.jpg](../images/496132_1_En_5_Chapter/496132_1_En_5_Fig38_HTML.jpg)

图 5-38

检测手时的输出

我们可以看到，这个数据的格式与使用 Facemesh 模型时的格式非常相似！

这使得试验更加容易和快速，因为您可以重用您在其他项目中编写的代码示例。它允许开发人员快速设置，专注于试验使用这种模型可以构建什么的可能性，而无需在配置上花费太多时间。

可以注意到的主要区别是注释中定义的属性、附加的`handInViewConfidence`属性以及缺少`mesh`和`scaledMesh`数据。

`handInViewConfidence`属性表示一只手出现的概率。它是一个介于 0 和 1 之间的浮点值。它越接近 1，模型越确信在视频流中发现了手。

在写这本书的时候，这个模型一次只能检测一只手。因此，您不能构建要求用户同时使用双手作为与界面交互方式的应用程序。

#### 完整代码示例

为了检查一切工作是否正常，下面是测试您的设置所需的完整 JavaScript 代码示例。

```py
let video;
let model;

const init = async () => {
  video = await loadVideo();
  await tf.setBackend("webgl");
  model = await handpose.load();
  main();
};

const loadVideo = async () => {
  const video = await setupCamera();
  video.play();
  return video;
};

const setupCamera = async () => {
  if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
    throw new Error(
      "Browser API navigator.mediaDevices.getUserMedia not available"
    );
  }

  video = document.querySelector("video");

  video.width = window.innerWidth;
  video.height = window.innerHeight;

  const stream = await navigator.mediaDevices.getUserMedia({
    audio: false,
    video: {
      facingMode: "user",
      width: window.innerWidth,
      height: window.innerHeight,
    },
  });
  video.srcObject = stream;

  return new Promise(
    (resolve) => (video.onloadedmetadata = () => resolve(video))
  );
};

init();

async function main() {
  const predictions = await model.estimateHands(
    document.querySelector("video")
  );

  if (predictions.length > 0) {
    console.log(predictions);
  }
  requestAnimationFrame(main);
}

Listing 5-35Full code sample

```

#### 项目

为了试验可以用这种模型构建的应用程序，我们将构建一个小型的“石头剪刀布”游戏。

为了理解我们将如何识别这三个手势，让我们看一下下面的可视化来理解每个手势的关键点的位置。

![../images/496132_1_En_5_Chapter/496132_1_En_5_Fig39_HTML.jpg](../images/496132_1_En_5_Chapter/496132_1_En_5_Fig39_HTML.jpg)

图 5-39

可视化的“摇滚”手势

前面的截图代表“摇滚”手势。正如我们所看到的，所有手指都是折叠的，因此指尖在 z 轴上应该比每个手指的第一个指骨末端的关键点更远。

否则，我们也可以考虑指尖的 y 坐标应该高于主要指关节的 y 坐标，记住屏幕顶部等于 0，关键点越低，y 值越高。

我们将能够试验 annotations 对象中返回的数据，看看这是否准确，是否可以用来检测“rock”手势。

![../images/496132_1_En_5_Chapter/496132_1_En_5_Fig40_HTML.jpg](../images/496132_1_En_5_Chapter/496132_1_En_5_Fig40_HTML.jpg)

图 5-40

“纸”手势可视化

在“纸”手势中，所有的手指都是直的，所以我们可以主要使用不同手指的 y 坐标。例如，我们可以检查每个手指的最后一点(指尖)的 y 值是否小于每个手指的手掌或指根的 y 值。

![../images/496132_1_En_5_Chapter/496132_1_En_5_Fig41_HTML.jpg](../images/496132_1_En_5_Chapter/496132_1_En_5_Fig41_HTML.jpg)

图 5-41

可视化的“剪刀”手势

最后，可以通过查看食指和中指之间的 x 轴空间以及其他手指的 y 坐标来识别“剪刀”手势。

如果无名指和小指指尖的 y 值低于它们的根部，那么它们很可能是折叠的。

重复使用我们在前面章节中浏览过的代码示例，让我们看看如何编写识别和区分这些手势的逻辑。

如果我们从**“摇摆”手势**开始，这里是我们如何检查每个手指的 y 坐标是否高于基本指节的 y 坐标。

```py
let indexBase = predictions[0].annotations.indexFinger[0][1];
let indexTip = predictions[0].annotations.indexFinger[3][1];

if (indexTip > indexBase) {
  console.log("index finger folded");
}

Listing 5-36Logic to check if the index finger is folded

```

我们可以从声明两个变量开始，一个存储食指根部的 y 位置，另一个存储同一个手指的指尖。

回头看看当手指出现在屏幕上时来自 annotations 对象的数据，我们可以看到，对于食指，我们得到一个 4 个数组的数组，代表每个关键点的 x、y 和 z 坐标。

![../images/496132_1_En_5_Chapter/496132_1_En_5_Fig42_HTML.jpg](../images/496132_1_En_5_Chapter/496132_1_En_5_Fig42_HTML.jpg)

图 5-42

检测到手时输出数据

第一个数组中的 y 坐标的值约为 352.27，最后一个数组中的 y 坐标的值约为 126.62，该值较低，因此我们可以推断出第一个数组表示食指根部的位置，最后一个数组表示食指指尖的关键点。

如果`indexTip`的值大于`indexBase`的值，我们可以通过编写前面显示的`if`语句来测试该信息是否正确，该语句记录“食指折叠”的消息。

而且很管用！

如果您将您的手放在摄像机前面，从伸直食指切换到折叠食指来测试这段代码，您应该会看到控制台中记录的消息！

如果我们想让它变得更快更简单，我们可以在这里停下来，决定这个单一的检查决定了“摇滚”手势。然而，如果我们想对我们的手势更有信心，我们可以对中指、无名指和小指重复同样的过程。

拇指会有一点不同，因为我们会检查 x 坐标而不是 y 坐标的差异，这是因为手指折叠的方式。

对于**“纸”手势**，当所有手指伸展时，我们可以检查每个手指的指尖比底部具有更小的 y 坐标。

下面是验证这一点的代码。

```py
let indexBase = predictions[0].annotations.indexFinger[0][1];
let indexTip = predictions[0].annotations.indexFinger[3][1];

let thumbBase = predictions[0].annotations.thumb[0][1];
let thumbTip = predictions[0].annotations.thumb[3][1];

let middleBase = predictions[0].annotations.middleFinger[0][1];
let middleTip = predictions[0].annotations.middleFinger[3][1];

let ringBase = predictions[0].annotations.ringFinger[0][1];
let ringTip = predictions[0].annotations.ringFinger[3][1];

let pinkyBase = predictions[0].annotations.pinky[0][1];
let pinkyTip = predictions[0].annotations.pinky[3][1];

let indexExtended = indexBase > indexTip ? true : false;
let thumbExtended = thumbBase > thumbTip ? true : false;
let middleExtended = middleBase > middleTip ? true : false;
let ringExtended = ringBase > ringTip ? true : false;
let pinkyExtended = pinkyBase > pinkyTip ? true : false;

if (indexExtended && thumbExtended && middleExtended && ringExtended &&
      pinkyExtended) {
      console.log("paper gesture!");
    } else {
      console.log("other gesture");
    }

Listing 5-37Check the y coordinate of each finger for the “paper” gesture

```

我们首先将感兴趣的坐标存储到变量中，然后比较它们的值，将扩展状态设置为真或假。

如果所有的手指都伸出来了，我们记录信息“纸手势！”。

如果一切正常，您应该能够将手放在摄像头前，手指张开，并在浏览器的控制台中查看日志。

如果您更改为另一种手势，应记录消息“另一种手势”。

![../images/496132_1_En_5_Chapter/496132_1_En_5_Fig44_HTML.jpg](../images/496132_1_En_5_Chapter/496132_1_En_5_Fig44_HTML.jpg)

图 5-44

在网络摄像头馈送中检测到的手和控制台中记录的其他手势的屏幕截图

![../images/496132_1_En_5_Chapter/496132_1_En_5_Fig43_HTML.jpg](../images/496132_1_En_5_Chapter/496132_1_En_5_Fig43_HTML.jpg)

图 5-43

在网络摄像头输入中检测到的手和记录在控制台中的纸手势的屏幕截图

最后，通过查看食指和中指指尖的 x 坐标变化，以及确保其他手指没有伸展，可以检测出****剪刀** **手势**。**

```py
let indexTipX = predictions[0].annotations.indexFinger[3][0];
let middleTipX = predictions[0].annotations.middleFinger[3][0];

let diffFingersX =
      indexTipX > middleTipX ? indexTipX - middleTipX : middleTipX - indexTipX;
console.log(diffFingersX);

Listing 5-38Check the difference in x coordinate for the index and middle finger tips

```

下面是我们从这个代码示例中获得的两个数据截图。

![../images/496132_1_En_5_Chapter/496132_1_En_5_Fig46_HTML.jpg](../images/496132_1_En_5_Chapter/496132_1_En_5_Fig46_HTML.jpg)

图 5-46

执行另一个手势时输出数据

![../images/496132_1_En_5_Chapter/496132_1_En_5_Fig45_HTML.jpg](../images/496132_1_En_5_Chapter/496132_1_En_5_Fig45_HTML.jpg)

图 5-45

执行“剪刀”手势时输出数据

我们可以看到，当我们做“剪刀”手势时，`diffFingersX`变量的值比两根手指并拢时高得多。

查看这些数据，我们可以决定我们的阈值可以是 100。如果`diffFingersX`的值大于 100，并且无名指和小指合拢，则手势为“剪刀”的可能性很大。

因此，总的来说，我们可以用下面的代码样本来检查这个手势。

```py
let ringBase = predictions[0].annotations.ringFinger[0][1];
let ringTip = predictions[0].annotations.ringFinger[3][1];
let pinkyBase = predictions[0].annotations.pinky[0][1];
let pinkyTip = predictions[0].annotations.pinky[3][1];

let ringExtended = ringBase > ringTip ? true : false;
let pinkyExtended = pinkyBase > pinkyTip ? true : false;

let indexTipX = predictions[0].annotations.indexFinger[3][0];
let middleTipX = predictions[0].annotations.middleFinger[3][0];

let diffFingersX =
      indexTipX > middleTipX ? indexTipX - middleTipX : middleTipX - indexTipX;

if (diffFingersX > 100 && !ringExtended && !pinkyExtended) {
  console.log("scissors gesture!");
}

Listing 5-39Detect “scissors” gesture

```

![../images/496132_1_En_5_Chapter/496132_1_En_5_Fig47_HTML.jpg](../images/496132_1_En_5_Chapter/496132_1_En_5_Fig47_HTML.jpg)

图 5-47

“剪刀”手势检测工作的屏幕截图

既然我们已经分别编写了检测手势的逻辑，让我们把它们放在一起。

```py
let indexBase = predictions[0].annotations.indexFinger[0][1];
let indexTip = predictions[0].annotations.indexFinger[3][1];
let thumbBase = predictions[0].annotations.thumb[0][1];
let thumbTip = predictions[0].annotations.thumb[3][1];
let middleBase = predictions[0].annotations.middleFinger[0][1];
let middleTip = predictions[0].annotations.middleFinger[3][1];
let ringBase = predictions[0].annotations.ringFinger[0][1];
let ringTip = predictions[0].annotations.ringFinger[3][1];
let pinkyBase = predictions[0].annotations.pinky[0][1];
let pinkyTip = predictions[0].annotations.pinky[3][1];

let indexExtended = indexBase > indexTip ? true : false;
let thumbExtended = thumbBase > thumbTip ? true : false;
let middleExtended = middleBase > middleTip ? true : false;
let ringExtended = ringBase > ringTip ? true : false;
let pinkyExtended = pinkyBase > pinkyTip ? true : false;

if (
      indexExtended &&
      thumbExtended &&
      middleExtended &&
      ringExtended &&
      pinkyExtended
) {
  console.log("paper gesture!");
}

/* Rock gesture */
if (!indexExtended && !middleExtended && !ringExtended && !pinkyExtended) {
   console.log("rock gesture!");
}

/* Scissors gesture */
let indexTipX = predictions[0].annotations.indexFinger[3][0];
let middleTipX = predictions[0].annotations.middleFinger[3][0];

let diffFingersX =
      indexTipX > middleTipX ? indexTipX - middleTipX : middleTipX - indexTipX;

if (diffFingersX > 100 && !ringExtended && !pinkyExtended) {
  console.log("scissors gesture!");
}

Listing 5-40Logic for detecting all gestures

```

如果一切正常，当你做每一个手势时，你应该看到正确的信息被记录在控制台中！

一旦你验证了这个逻辑是可行的，你就可以继续使用`console.log`来构建一个游戏，或者使用这些手势作为你界面的控制器，等等。

最重要的是理解模型是如何工作的，熟悉使用坐标构建逻辑，以便探索机会，并意识到一些限制。

### 波塞尼特

最后，我们要讲的最后一个身体追踪模型叫做 PoseNet。

PoseNet 是一个姿势检测模型，可以估计图像或视频中的单个姿势或多个姿势。

与 Facemesh 和 Handpose 模型类似，PoseNet 可以跟踪用户身体中关键点的位置。

以下是这些关键点的可视化示例。

![../images/496132_1_En_5_Chapter/496132_1_En_5_Fig48_HTML.jpg](../images/496132_1_En_5_Chapter/496132_1_En_5_Fig48_HTML.jpg)

图 5-48

PoseNet 检测到的关键点的可视化。来源: [`https://github.com/tensorflow/tfjs-models/tree/master/posenet`](https://github.com/tensorflow/tfjs-models/tree/master/posenet)

该身体跟踪模型可以检测 17 个关键点及其 2D 坐标，由零件 ID 索引。

![../images/496132_1_En_5_Chapter/496132_1_En_5_Fig49_HTML.jpg](../images/496132_1_En_5_Chapter/496132_1_En_5_Fig49_HTML.jpg)

图 5-49

关键点及其 ID 列表。来源: [`https://github.com/tensorflow/tfjs-models/tree/master/posenet`](https://github.com/tensorflow/tfjs-models/tree/master/posenet)

尽管这个模型也专门使用网络摄像头来跟踪一个人的身体，但是在您的代码中使用它与我们在前面几节中讨论的两个模型有一点不同。

#### 导入和加载模型

导入和加载模型遵循与本书中大多数代码示例相同的标准。

```py
const net = await posenet.load();

Listing 5-42Loading the model in JavaScript

```

```py
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/posenet"></script>

Listing 5-41Import TensorFlow.js and the PoseNet model in HTML

```

这种加载 PoseNet 的默认方式使用基于 MobileNetV1 架构的更快更小的模型。速度的代价是较低的精度。

如果您想试验参数，也可以这样加载。

```py
const net = await posenet.load({
  architecture: 'MobileNetV1',
  outputStride: 16,
  inputResolution: { width: 640, height: 480 },
  multiplier: 0.75
});

Listing 5-43Alternative ways of loading the model

```

如果您想尝试可用的第二种配置，您可以指明您想使用基于 ResNet50 架构的另一个模型，该模型具有更好的准确性，但是是一个更大的模型，因此将需要更多的时间来加载。

```py
const net = await posenet.load({
  architecture: 'ResNet50',
  outputStride: 32,
  inputResolution: { width: 257, height: 200 },
  quantBytes: 2
});

Listing 5-44Loading the model using the ResNet50 architecture

```

如果您对不同的参数感到有点困惑，不要担心，当您开始使用时，使用提供的默认参数是完全可以的。如果你想进一步了解它们，你可以在 TensorFlow 官方文档中找到更多信息。

一旦模型被加载，你就可以专注于预测姿态。

#### 预言

要从模型中获得预测，您主要需要在模型上调用 estimateSinglePose 方法。

```py
const pose = await net.estimateSinglePose(image, {
  flipHorizontal: false
});

Listing 5-45Predicting single poses

```

image 参数可以是某个 imageData、HTML 图像元素、HTML canvas 元素或 HTML video 元素。它表示您想要对其进行预测的输入图像。

`flipHorizontal`参数表示您是否想要水平翻转/镜像姿势。默认情况下，其值设置为 false。

如果您正在使用视频，并且默认情况下视频是水平翻转的(例如，当使用网络摄像头时)，则应将它设置为 true。

前面的代码示例将变量 pose 设置为单个 pose 对象，该对象将包含置信度得分和检测到的关键点数组，以及它们的 2D 坐标、身体部位的名称和概率得分。

下面是将返回的对象的示例。

```py
{
  "score": 0.32371445304906,
  "keypoints": [
    {
      "position": {
        "y": 76.291801452637,
        "x": 253.36747741699
      },
      "part": "nose",
      "score": 0.99539834260941
    },
    {
      "position": {
        "y": 71.10383605957,
        "x": 253.54365539551
      },
      "part": "leftEye",
      "score": 0.98781454563141
    },
    {
      "position": {
        "y": 71.839515686035,
        "x": 246.00454711914
      },
      "part": "rightEye",
      "score": 0.99528175592422
    },
    {
      "position": {
        "y": 72.848854064941,
        "x": 263.08151245117
      },
      "part": "leftEar",
      "score": 0.84029853343964
    },
    {
      "position": {
        "y": 79.956565856934,
        "x": 234.26812744141
      },
      "part": "rightEar",
      "score": 0.92544466257095
    },
    {
      "position": {
        "y": 98.34538269043,
        "x": 399.64068603516
      },
      "part": "leftShoulder",
      "score": 0.99559044837952
    },
    {
      "position": {
        "y": 95.082359313965,
        "x": 458.21868896484
      },
      "part": "rightShoulder",
      "score": 0.99583911895752
    },
    {
      "position": {
        "y": 94.626205444336,
        "x": 163.94561767578
      },
      "part": "leftElbow",
      "score": 0.9518963098526
    },
    {
      "position": {
        "y": 150.2349395752,
        "x": 245.06030273438
      },
      "part": "rightElbow",
      "score": 0.98052614927292
    },
    {
      "position": {
        "y": 113.9603729248,
        "x": 393.19735717773
      },
      "part": "leftWrist",
      "score": 0.94009721279144
    },
    {
      "position": {
        "y": 186.47859191895,
        "x": 257.98034667969
      },
      "part": "rightWrist",
      "score": 0.98029226064682
    },
    {
      "position": {
        "y": 208.5266418457,
        "x": 284.46710205078
      },
      "part": "leftHip",
      "score": 0.97870296239853
    },
    {
      "position": {
        "y": 209.9910736084,
        "x": 243.31219482422
      },
      "part": "rightHip",
      "score": 0.97424703836441
    },
    {
      "position": {
        "y": 281.61965942383,
        "x": 310.93188476562
      },
      "part": "leftKnee",
      "score": 0.98368924856186
    },
    {
      "position": {
        "y": 282.80120849609,
        "x": 203.81164550781
      },
      "part": "rightKnee",
      "score": 0.96947449445724
    },
    {
      "position": {
        "y": 360.62716674805,
        "x": 292.21047973633
      },
      "part": "leftAnkle",
      "score": 0.8883239030838
    },
    {
      "position": {
        "y": 347.41177368164,
        "x": 203.88229370117
      },
      "part": "rightAnkle",
      "score": 0.8255187869072
    }
  ]
}

Listing 5-46Complete object returned as predictions

```

如果您想要检测多个姿势，如果您希望一个图像或视频包含多个人，您可以将调用的方法更改如下。

```py
const poses = await net.estimateMultiplePoses(image, {
  flipHorizontal: false,
  maxDetections: 5,
  scoreThreshold: 0.5,
  nmsRadius: 20
});

Listing 5-47Predicting multiple poses

```

我们可以看到传入了一些额外的参数。

*   **maxDetections** 表示我们想要检测的姿势的最大数量。值 5 是默认值，但是您可以将其更改为更大或更小。

*   **scoreThreshold** 表示如果对象根处的分数值高于设置的值，则只希望返回实例。默认值为 0.5。

*   **nmsRadius** 代表非最大值抑制，表示应该分离检测到的多个姿态的像素数量。该值必须严格为正，默认值为 20。

使用这个方法会将变量 poses 的值设置为一个 pose 对象数组，如下所示。

```py
[
  // Pose 1
  {
    // Pose score
    "score": 0.42985695206067,
    "keypoints": [
      {
        "position": {
          "x": 126.09371757507,
          "y": 97.861720561981
        },
        "part": "nose",
        "score": 0.99710708856583
      },
      {
        "position": {
          "x": 132.53466176987,
          "y": 86.429876804352
        },
        "part": "leftEye",
        "score": 0.99919074773788
      },
      ...
    ],
  },
  // Pose 2
  {

    // Pose score
    "score": 0.13461434583673,
    "keypoints": [
      {
        "position": {
          "x": 116.58444058895,
          "y": 99.772533416748
        },
        "part": "nose",
        "score": 0.0028593824245036
      }
      {
        "position": {
          "x": 133.49897611141,
          "y": 79.644590377808
        },
        "part": "leftEye",
        "score": 0.99919074773788
      },
      ...
    ],
  }
]

Listing 5-48Output array when detecting multiple poses

```

#### 完整代码示例

总之，设置图像中姿态预测的代码示例如下。

```py
const imageElement = document.getElementsByTagName("img")[0];

posenet
  .load()
  .then(function (net) {
    const pose = net.estimateSinglePose(imageElement, {
      flipHorizontal: true,
    });
    return pose;
  })
  .then(function (pose) {
    console.log(pose);
  })
  .catch((err) => console.log(err));

Listing 5-50JavaScript code

```

```py
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>PoseNet</title>
  </head>
  <body>
    <!—- you can replace the path to the asset with any you'd like -—>
    <img src="image-pose.jpg" alt="" />
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/posenet"></script>
    <script src="index.js"></script>
  </body>
</html>

Listing 5-49HTML code to detect poses in an image

```

对于来自网络摄像头馈送的视频，代码应该如下所示。

```py
let video;
let model;

const init = async () => {
  video = await loadVideo();
  model = await posenet.load();
  main();
};

const loadVideo = async () => {
  const video = await setupCamera();
  video.play();
  return video;
};

const setupCamera = async () => {
  if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
    throw new Error(
      "Browser API navigator.mediaDevices.getUserMedia not available"
    );
  }

  video = document.querySelector("video");
  video.width = window.innerWidth;
  video.height = window.innerHeight;

  const stream = await navigator.mediaDevices.getUserMedia({
    audio: false,
    video: {
      facingMode: "user",
      width: window.innerWidth,
      height: window.innerHeight,
    },
  });
  video.srcObject = stream;

  return new Promise(
    (resolve) => (video.onloadedmetadata = () => resolve(video))
  );
};

init();

const main = () => {
  const pose = model
    .estimateSinglePose(video, {
      flipHorizontal: true,
    })
    .then((pose) => {
      console.log(pose);
    });

  requestAnimationFrame(main);
};

Listing 5-52JavaScript code to detect poses in a video from the webcam

```

```py
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>PoseNet</title>
  </head>
  <body>
    <video></video>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/posenet"></script>
    <script src="index.js"></script>
  </body>
</html>

Listing 5-51HTML code to detect poses in a video

```

#### 可视化关键点

到目前为止，我们主要使用`console.log`来查看从模型返回的结果。但是，您可能希望在页面上显示它们，以确保身体跟踪正在工作，并且关键点放置在正确的位置。

为此，我们将使用 Canvas API。

我们需要从在 HTML 文件中添加一个 HTML canvas 元素开始。然后，我们可以创建一个函数来访问这个元素及其上下文，检测姿态，并绘制关键点。

访问 canvas 元素及其上下文是用下面几行代码完成的。

```py
const canvas = document.getElementById("output");
const ctx = canvas.getContext("2d");
canvas.width = window.innerWidth;
canvas.height = window.innerHeight;

Listing 5-53Accessing the canvas element

```

然后，我们可以创建一个函数，调用`estimateSinglePose`方法来开始检测，在画布上绘制视频，并循环遍历找到的关键点，以将它们呈现在画布元素上。

```py
async function poseDetectionFrame() {
    const pose = await net.estimateSinglePose(video, {
      flipHorizontal: true
    });

    ctx.clearRect(0, 0, videoWidth, videoHeight);
    ctx.save();
    ctx.scale(-1, 1);
    ctx.translate(-videoWidth, 0);
    ctx.drawImage(video, 0, 0, window.innerWidth, window.innerHeight);
    ctx.restore();

    drawKeypoints(pose.keypoints, 0.5, ctx);
    drawSkeleton(pose.keypoints, 0.5, ctx);

    requestAnimationFrame(poseDetectionFrame);
}

Listing 5-54Start the detection, draw the webcam feed on a canvas element, and render the keypoints

```

`drawKeypoints`和`drawSkeleton`函数使用一些 Canvas API 方法在检测到的关键点的坐标处绘制圆和线。

```py
const color = "aqua";
const lineWidth = 2;

const toTuple = ({ y, x }) => [y, x];

function drawPoint(ctx, y, x, r, color) {
  ctx.beginPath();
  ctx.arc(x, y, r, 0, 2 * Math.PI);
  ctx.fillStyle = color;
  ctx.fill();
}

function drawSegment([ay, ax], [by, bx], color, scale, ctx) {
  ctx.beginPath();
  ctx.moveTo(ax * scale, ay * scale);
  ctx.lineTo(bx * scale, by * scale);
  ctx.lineWidth = lineWidth;
  ctx.strokeStyle = color;
  ctx.stroke();
}

function drawSkeleton(keypoints, minConfidence, ctx, scale = 1) {
  const adjacentKeyPoints = posenet.getAdjacentKeyPoints(
    keypoints,
    minConfidence
  );

  adjacentKeyPoints.forEach((keypoints) => {
    drawSegment(
      toTuple(keypoints[0].position),
      toTuple(keypoints[1].position),
      color,
      scale,
      ctx
    );
  });
}

function drawKeypoints(keypoints, minConfidence, ctx, scale = 1) {
  for (let i = 0; i < keypoints.length; i++) {
    const keypoint = keypoints[i];

    if (keypoint.score < minConfidence) {
      continue;
    }

    const { y, x } = keypoint.position;
    drawPoint(ctx, y * scale, x * scale, 3, color);
  }
}

Listing 5-55Some helper functions to draw the keypoints onto the canvas element

```

一旦加载了视频和模型，就应该调用`poseDetectionFrame`函数。

总之，完整的代码示例应该如下所示。

```py
const color = "aqua";
const lineWidth = 2;

function toTuple({ y, x }) {
  return [y, x];
}

export function drawPoint(ctx, y, x, r, color) {
  ctx.beginPath();
  ctx.arc(x, y, r, 0, 2 * Math.PI);
  ctx.fillStyle = color;
  ctx.fill();
}

export function drawSegment([ay, ax], [by, bx], color, scale, ctx) {
  ctx.beginPath();
  ctx.moveTo(ax * scale, ay * scale);
  ctx.lineTo(bx * scale, by * scale);
  ctx.lineWidth = lineWidth;
  ctx.strokeStyle = color;
  ctx.stroke();
}

export function drawSkeleton(keypoints, minConfidence, ctx, scale = 1) {
  const adjacentKeyPoints = posenet.getAdjacentKeyPoints(
    keypoints,
    minConfidence
  );

  adjacentKeyPoints.forEach((keypoints) => {
    drawSegment(
      toTuple(keypoints[0].position),
      toTuple(keypoints[1].position),
      color,
      scale,
      ctx
    );
  });
}

export function drawKeypoints(keypoints, minConfidence, ctx, scale = 1) {
  for (let i = 0; i < keypoints.length; i++) {
    const keypoint = keypoints[i];

    if (keypoint.score < minConfidence) {
      continue;
    }

    const { y, x } = keypoint.position;
    drawPoint(ctx, y * scale, x * scale, 3, color);
  }
}

Listing 5-58JavaScript code to visualize keypoints in utils.js

```

```py
import { drawKeypoints, drawSkeleton } from "./utils.js";
const videoWidth = window.innerWidth;
const videoHeight = window.innerHeight;

async function setupCamera() {
  if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
    throw new Error(
      "Browser API navigator.mediaDevices.getUserMedia not available"
    );
  }

  const video = document.getElementById("video");
  video.width = videoWidth;
  video.height = videoHeight;

  const stream = await navigator.mediaDevices.getUserMedia({
    audio: false,
    video: {
      facingMode: "user",
      width: videoWidth,
      height: videoHeight,
    },
  });
  video.srcObject = stream;

  return new Promise((resolve) => {
    video.onloadedmetadata = () => {
      resolve(video);
    };
  });
}

async function loadVideo() {
  const video = await setupCamera();
  video.play();

  return video;
}

function detectPoseInRealTime(video, net) {
  const canvas = document.getElementById("output");
  const ctx = canvas.getContext("2d");

  const flipPoseHorizontal = true;

  canvas.width = videoWidth;
  canvas.height = videoHeight;

  async function poseDetectionFrame() {
    let minPoseConfidence;
    let minPartConfidence;

    const pose = await net.estimateSinglePose(video, {
      flipHorizontal: flipPoseHorizontal,
    });

    minPoseConfidence = 0.1;
    minPartConfidence = 0.5;

    ctx.clearRect(0, 0, videoWidth, videoHeight);

    ctx.save();
    ctx.scale(-1, 1);
    ctx.translate(-videoWidth, 0);
    ctx.drawImage(video, 0, 0, videoWidth, videoHeight);
    ctx.restore();

    drawKeypoints(pose.keypoints, minPartConfidence, ctx);
    drawSkeleton(pose.keypoints, minPartConfidence, ctx);

    requestAnimationFrame(poseDetectionFrame);
  }

  poseDetectionFrame();
}

let net;
export async function init() {
  net = await posenet.load();

  let video;

  try {
    video = await loadVideo();
  } catch (e) {
    throw e;
  }
  detectPoseInRealTime(video, net);
}

navigator.getUserMedia =
  navigator.getUserMedia ||
  navigator.webkitGetUserMedia ||
  navigator.mozGetUserMedia;
init();

Listing 5-57JavaScript code to visualize keypoints in index.js

```

```py
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>PoseNet</title>
  </head>
  <body>
    <video id="video"></video>
    <canvas id="output" />
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/posenet"></script>
    <script src="utils.js" type="module"></script>
    <script src="index.js" type="module"></script>
  </body>
</html>

Listing 5-56Complete HTML code to visualize keypoints

```

这段代码的输出应该像下面这样可视化关键点。

![../images/496132_1_En_5_Chapter/496132_1_En_5_Fig51_HTML.jpg](../images/496132_1_En_5_Chapter/496132_1_En_5_Fig51_HTML.jpg)

图 5-51

完整代码示例的输出

![../images/496132_1_En_5_Chapter/496132_1_En_5_Fig50_HTML.jpg](../images/496132_1_En_5_Chapter/496132_1_En_5_Fig50_HTML.jpg)

图 5-50

完整代码示例的输出

现在，我们已经完成了检测姿势、访问身体不同部位的坐标并在画布上可视化它们的代码，请随意使用这些数据来创建探索新交互的项目。

## 5.3 硬件数据

对于本章的最后一节和本书的最后一部分，将包含代码示例，我们将研究一些更高级和实验性的东西。接下来的几页将重点关注使用硬件生成的数据，并建立一个自定义的机器学习模型来检测手势。

平时做硬件的时候，我用 Arduino 或者 Raspberry Pi 之类的微控制器；然而，为了让可能无法接触到这类材料的读者更容易理解这本书，下一节将使用另一种内置硬件组件的设备，即您的手机！

这是假设你拥有一部至少带有加速度计和陀螺仪的现代手机。

为了在 JavaScript 中访问这些数据，我们将使用通用传感器 API。

这个 API 是相当新的和实验性的，目前只在 Chrome 中有浏览器支持，所以如果你决定编写下面的代码示例，请确保使用 Chrome 作为你的浏览器。

![../images/496132_1_En_5_Chapter/496132_1_En_5_Fig52_HTML.jpg](../images/496132_1_En_5_Chapter/496132_1_En_5_Fig52_HTML.jpg)

图 5-52

通用传感器 API 的浏览器支持。来源: [`https://caniuse.com/#search=sensor%20api`](https://caniuse.com/#search=sensor%2520api)

为了构建我们的手势分类器，我们将访问和记录来自您手机中的加速度计和陀螺仪的数据，将这些数据保存到您项目的文件中，创建一个机器学习模型，训练它，并对新的实时数据进行预测。

为了做到这一点，我们将需要一点 Node.js、带有 socket.io 的 web sockets、通用传感器 API 和 TensorFlow.js。

如果您不熟悉其中的一些技术，不要担心，我将解释每一部分并提供您应该能够理解的代码示例。

### 5.3.1 网络传感器 API

由于我们在本节中使用硬件数据，我们需要做的第一件事是验证我们可以访问正确的数据。

如前所述，我们需要记录陀螺仪和加速度计的数据。

陀螺仪为我们提供有关设备方向及其角速度的详细信息，而加速度计则专注于提供有关加速度的数据。

尽管如果我们愿意，我们可以只使用其中一个传感器，但我相信，结合陀螺仪和加速度计的数据可以为我们提供更精确的运动信息，并有助于手势识别。

![../images/496132_1_En_5_Chapter/496132_1_En_5_Fig54_HTML.jpg](../images/496132_1_En_5_Chapter/496132_1_En_5_Fig54_HTML.jpg)

图 5-54

手机上的陀螺仪轴。来源: [`https://www.sitepoint.com/using-device-orientation-html5/`](https://www.sitepoint.com/using-device-orientation-html5/)

![../images/496132_1_En_5_Chapter/496132_1_En_5_Fig53_HTML.jpg](../images/496132_1_En_5_Chapter/496132_1_En_5_Fig53_HTML.jpg)

图 5-53

手机上的加速度计轴。来源: [`https://developers.google.com/web/fundamentals/native-hardware/device-orientation`](https://developers.google.com/web/fundamentals/native-hardware/device-orientation)

### 访问传感器数据

为了使用通用传感器 API 访问数据，我们需要从声明几个变量开始:一个将引用`requestAnimationFrame`语句，以便我们稍后可以取消它，另外两个将包含陀螺仪和加速度计数据。

```py
let dataRequest;
let gyroscopeData = {
    x: '',
    y: '',
    z: ''
}

let accelerometerData = {
    x: '',
    y: '',
    z: ''
}

Listing 5-59In index.js. Declaring variables to contain hardware data

```

然后，要访问手机的传感器数据，您需要实例化一个新的陀螺仪和加速度计接口，使用`reading`事件监听器获取设备运动的 x、y 和 z 坐标，并调用`start`方法开始跟踪。

```py
function initSensors() {
    let gyroscope = new Gyroscope({frequency: 60});

    gyroscope.addEventListener('reading', e => {
        gyroscopeData.x = gyroscope.x;
        gyroscopeData.y = gyroscope.y;
        gyroscopeData.z = gyroscope.z;
    });
    gyroscope.start();

    let accelerometer = new Accelerometer({frequency: 60});

    accelerometer.addEventListener('reading', e => {
        accelerometerData.x = accelerometer.x;
        accelerometerData.y = accelerometer.y;
        accelerometerData.z = accelerometer.z;
    });
    accelerometer.start();
}

Listing 5-60In index.js. Get accelerometer and gyroscope data

```

最后，当我们执行一个特定的手势时，我们对记录数据感兴趣，所以我们只需要在用户通过`touchstart`事件按下移动屏幕时调用前面的函数。我们也应该在`touchend`取消它。

```py
function getData() {
  let data = {
    xAcc: accelerometerData.x,
    yAcc: accelerometerData.y,
    zAcc: accelerometerData.z,
    xGyro: gyroscopeData.x,
    yGyro: gyroscopeData.y,
    zGyro: gyroscopeData.z,
  };
  document.body.innerHTML = JSON.stringify(data);
  dataRequest = requestAnimationFrame(getData);
}

window.onload = function () {
  initSensors();

  document.body.addEventListener("touchstart", (e) => {
    getData();
  });
  document.body.addEventListener("touchend", (e) => {
    cancelAnimationFrame(dataRequest);
  });
};

Listing 5-61In index.js. Use the touchstart event listener to start displaying data

```

此时，如果您想要检查这段代码的输出，您将需要使用 ngrok 之类的工具在您的移动电话上访问这个页面，例如，创建一个到您的本地主机的隧道。

你应该看到的是当你按下它时屏幕上显示的实时加速度计和陀螺仪数据，当你释放它时，数据应该不会再更新了。

在这一点上，我们在页面上显示数据，这样我们就可以仔细检查每件事情是否如预期的那样工作。

然而，我们真正需要的是在记录手势时将这些数据存储在文件中。为此，我们将需要 web 套接字将数据从前端发送到后端服务器，后端服务器将负责将数据写入我们的应用程序文件夹中的文件。

### 5.3.3 设置 web 套接字

为了设置 web 套接字，我们将使用 socket.io。

到目前为止，在所有前面的例子中，我们只处理 HTML 和 JavaScript 文件，没有任何后端。

如果你以前从未编写过任何 Node.js，你将需要安装它以及 **npm** 或 **yarn** 来安装软件包。

设置好这两个工具后，在终端的项目文件夹的根目录下，编写`npm init`来生成一个 package.json 文件，该文件将包含项目的一些细节。

一旦您的 package.json 文件生成，在您的终端中，编写`npm install socket.io`来安装这个包。

完成后，在 HTML 文件中添加以下脚本标记。

```py
<script type="text/javascript" src="./../socket.io/socket.io.js"></script>

Listing 5-62Import the socket.io script in the HTML file

```

现在，你应该可以在前端使用 socket.io 了。在您的 JavaScript 文件中，首先用`const socket = io()`实例化它。

如果您对设置软件包有任何问题，请随意参考官方文档。

然后，在我们的`touchstart`的事件监听器中，我们可以使用 socket.io 用下面的数据向服务器发送数据。

```py
socket.emit("motion data", `${accelerometerData.x} ${accelerometerData.y} ${accelerometerData.z} ${gyroscopeData.x} ${gyroscopeData.y} ${gyroscopeData.z}`);

Listing 5-63Send motion data via web sockets

```

我们将运动数据作为字符串发送，因为我们希望将这些值写入文件。

在`touchend`时，我们需要发送另一个事件，表明我们想要停止用`socket.emit('end motion data')`发送数据。

总之，我们的第一个 JavaScript 文件应该如下所示。

```py
const socket = io();
let gyroscopeData = {
  x: "",
  y: "",
  z: "",
};
let accelerometerData = {
  x: "",
  y: "",
  z: "",
};

let dataRequest;

function initSensors() {
  let gyroscope = new Gyroscope({ frequency: 60 });

  gyroscope.addEventListener("reading", (e) => {
    gyroscopeData.x = gyroscope.x;
    gyroscopeData.y = gyroscope.y;
    gyroscopeData.z = gyroscope.z;
  });
  gyroscope.start();

  let accelerometer = new Accelerometer({ frequency: 60 });

  accelerometer.addEventListener("reading", (e) => {
    accelerometerData.x = accelerometer.x;
    accelerometerData.y = accelerometer.y;
    accelerometerData.z = accelerometer.z;
  });
  accelerometer.start();
}

function getData() {
  dataRequest = requestAnimationFrame(getData);
  socket.emit(
    "motion data",
    `${accelerometerData.x} ${accelerometerData.y} ${accelerometerData.z} ${gyroscopeData.x} ${gyroscopeData.y} ${gyroscopeData.z}`
  );
}

window.onload = function () {
  initSensors();

  document.body.addEventListener("touchstart", (e) => {
    getData();
  });
  document.body.addEventListener("touchend", (e) => {
    socket.emit("end motion data");
    cancelAnimationFrame(dataRequest);
  });
};

Listing 5-64Complete JavaScript code in the index.js file

```

现在，让我们实现这个项目的服务器端，以服务于我们的前端文件，接收数据，并将其存储到文本文件中。

首先，我们需要创建一个新的 JavaScript 文件。我个人给它取名`server.js`。

为了提供我们的前端文件，我们将使用`express` npm 包。要安装它，请在您的终端中键入`npm install express —-save`。

安装完成后，编写下面的代码来创建一个为我们的 index.html 文件服务的`'/record'`路径。

```py
const express = require("express");
const app = express();
var http = require("http").createServer(app);

app.use("/record", express.static(__dirname + '/'));

http.listen(process.env.PORT || 3000);

Listing 5-65Initial setup of the server.js file

```

您应该能够在您的终端中键入`node server.js`，在您的浏览器中访问`http://localhost:3000/record`，并且它应该服务于我们之前创建的 index.html 文件。

现在，让我们通过请求 socket.io 包来测试我们的 web sockets 连接，并编写将从前端接收消息的后端代码。

在 server.js 文件的顶部，需要带有`const io = require('socket.io')(http)`的包。

然后，建立连接并使用以下数据监听事件。

```py
io.on("connection", function (socket) {
  socket.on("motion data", function (data) {
    console.log(data);
  });

  socket.on("end motion data", function () {
    console.log('end');
  });
});

Listing 5-66In server.js. Web sockets connection

```

现在，重新启动服务器，访问您手机上的“/record”页面，当您触摸手机屏幕时，您应该会看到记录在您终端中的运动数据。

如果您没有看到任何东西，**再次检查您的页面是否使用了 https** 。

此时，我们知道 web sockets 连接已经正确设置，接下来的步骤是将这些数据保存到我们应用程序的文件中，这样我们就可以用它来训练机器学习算法。

为了保存文件，我们将使用 Node.js 文件系统模块，所以我们需要从用`const fs = require('fs');`要求它开始。

然后，我们将编写一些能够处理启动服务器时传递的参数的代码，这样我们就可以轻松地记录新的样本。

例如，如果我们想要记录三个手势，一个在空中表演字母 A，第二个字母 B，第三个字母 C，我们希望能够键入`node server.js letterA 1`来指示我们当前正在记录字母 A 手势的数据(`letterA`参数)，并且这是第一个样本(`1`参数)。

下面的代码将处理这两个参数，将它们存储在变量中，并使用它们来命名创建的新文件。

```py
let stream;
let sampleNumber;
let gestureType;
let previousSampleNumber;

process.argv.forEach(function (val, index, array) {
  gestureType = array[2];
  sampleNumber = parseInt(array[3]);
  previousSampleNumber = sampleNumber;
  stream = fs.createWriteStream(
    `data/sample_${gestureType}_${sampleNumber}.txt`,
    { flags: "a" }
  );
});

Listing 5-67In server.js. Code to handle arguments passed in to generate file names dynamically

```

现在，当启动服务器时，您将需要传递这两个参数(手势类型和样本号)。

要实际将数据从前端写入这些文件，我们需要编写以下代码行。

```py
socket.on("motion data", function (data) {
    /* This following line allows us to record new files without having to start/stop the server. On motion end, we increment the sampleNumber variable so when receiving new data, we deduce it is related to a new gesture and create a file with the correct sample number. */
    if (sampleNumber !== previousSampleNumber) {
      stream = fs.createWriteStream(
        `./data/sample_${gestureType}_${sampleNumber}.txt`,
        { flags: "a" }
      );
    }
    stream.write(`${data}\r\n`);
});

socket.on("end motion data", function () {
    stream.end();
    sampleNumber += 1;
});

Listing 5-68In server.js. Code to create a file and stream when receiving data

```

我们还在收到“结束动作数据”事件时关闭流，这样当用户停止触摸手机屏幕时，我们就停止写入动作数据，因为这意味着他们已经停止执行我们想要记录的手势。

为了测试这个设置，首先在你的应用程序中创建一个名为“数据”的空文件夹，然后在你的终端中键入`node server.js letterA 1`，在你的手机上访问网页，并在按下屏幕的同时在空中执行字母 A 的手势，当释放时，你应该在`data`文件夹中看到一个名为`sample_letterA_1.text`的新文件，它应该包含手势数据！

在这个阶段，我们能够获得加速度计和陀螺仪数据，使用 web sockets 将其发送到我们的服务器，并将其保存到我们应用程序的文件中。

```py
const express = require("express");
const app = express();
const http = require("http").createServer(app);
const io = require('socket.io')(http);
const fs = require('fs');
let stream;
let sampleNumber;
let gestureType;
let previousSampleNumber;

app.use("/record", express.static(__dirname + '/'));

process.argv.forEach(function (val, index, array) {
  gestureType = array[2];
  sampleNumber = parseInt(array[3]);
  previousSampleNumber = sampleNumber;
  stream = fs.createWriteStream(
    `data/sample_${gestureType}_${sampleNumber}.txt`,
    { flags: "a" }
  );
});

io.on("connection", function (socket) {
  socket.on("motion data", function (data) {
    if (sampleNumber !== previousSampleNumber) {
      stream = fs.createWriteStream(
        `./data/sample_${gestureType}_${sampleNumber}.txt`,
        { flags: "a" }
      );
    }
    stream.write(`${data}\r\n`);
});

  socket.on("end motion data", function () {
    stream.end();
    sampleNumber += 1;
  });
});

http.listen(process.env.PORT || 3000);

Listing 5-69Complete code sample in the server.js file

```

在继续编写负责格式化我们的数据和创建机器学习模型的代码之前，请确保为我们的三个手势中的每一个记录一些数据样本；越多越好，但我建议**每个手势**至少记录 20 个样本。

### 数据处理

对于本节，我建议创建一个新的 JavaScript 文件。我个人称之为`train.js`。

在这个文件中，我们将通读上一步中记录的文本文件，将数据从字符串转换为张量，并创建和训练我们的模型。下面的一些代码示例与 TensorFlow.js 没有直接关系(读取文件夹和文件，并将数据格式化为多维数组)，所以我不会过多地钻研它们。

这里的第一步是浏览我们的数据文件夹，获取每个样本和手势的数据，并将其组织到特征和标签的数组中。

为此，我使用了行阅读器 npm 包，所以我们需要使用`npm install line-reader`来安装它。

我们还需要用`npm install @tensorflow/tfjs-node`安装 TensorFlow。

然后，我创建了两个函数`readDir`和`readFile`来遍历数据文件夹中的所有文件，并针对每个文件，遍历每一行，将字符串转换为数字，并返回一个包含该手势的标签和特征的对象。

```py
const lineReader = require("line-reader");
var fs = require("fs");
const tf = require("@tensorflow/tfjs-node");

const gestureClasses = ["letterA", "letterB", "letterC"];
let numClasses = gestureClasses.length;

let numSamplesPerGesture = 20; // the number of times you recorded each gesture.
let totalNumDataFiles = numSamplesPerGesture * numClasses;
let numPointsOfData = 6; // x, y, and z for both accelerometer and gyroscope
let numLinesPerFile = 100; // Files might have a different amount of lines so we need a value to truncate and make sure all our samples have the same length.
let totalNumDataPerFile = numPointsOfData * numLinesPerFile;

function readFile(file) {
  let allFileData = [];

  return new Promise((resolve, reject) => {
    fs.readFile(`data/${file}`, "utf8", (err, data) => {
      if (err) {
        reject(err);
      } else {
        lineReader.eachLine(`data/${file}`, function (line) {
          // Turn each line into an array of floats.
          let dataArray = line
            .split(" ")
            .map((arrayItem) => parseFloat(arrayItem));
          allFileData.push(...dataArray);
          let concatArray = [...allFileData];

          if (concatArray.length === totalNumDataPerFile) {
            // Get the label from the filename
            let label = file.split("_")[1];
            let labelIndex = gestureClasses.indexOf(label);
            // Return an object with data as features and the label index
            resolve({ features: concatArray, label: labelIndex });
          }
        });
      }
    });
  });
}

const readDir = () =>
  new Promise((resolve, reject) =>
    fs.readdir(`data/`, "utf8", (err, data) =>
      err ? reject(err) : resolve(data)
    )
  );

(async () => {
  const filenames = await readDir();
  let allData = [];
  filenames.map(async (file) => {
    let originalContent = await readFile(file);
    allData.push(originalContent);
    if (allData.length === totalNumDataFiles) {
      console.log(allData);
    }
  });
})();

Listing 5-70In train.js. Loop through files to transform raw data into objects of features and labels

```

我不打算深入研究前面的代码示例，但是我添加了一些行内注释来提供帮助。

如果使用`node train.js`运行这段代码，应该会得到类似下图的输出。

![../images/496132_1_En_5_Chapter/496132_1_En_5_Fig55_HTML.jpg](../images/496132_1_En_5_Chapter/496132_1_En_5_Fig55_HTML.jpg)

图 5-55

格式化数据的输出示例

此时，我们的变量`allData`保存了每个手势样本的所有特征和标签，但是我们还没有完成。在将这些数据输入机器学习算法之前，我们需要将其转换为 tensors，TensorFlow.js 使用的数据类型。

下面的代码示例将会更加复杂，因为我们需要进一步格式化数据，创建张量，将它们分成训练集和测试集以验证我们未来的预测，然后生成模型。

我添加了行内注释来解释每一步。

所以，我们在前面的代码中写了`console.log(allData)`的地方，用`format(allData)`代替，下面将展示这个函数的实现。

```py
let justFeatures = [];
let justLabels = [];

const format = (allData) => {
  // Sort all data by label to get [{label: 0, features: ...}, {label: 1, features: ...}];
  let sortedData = allData.sort((a, b) => (a.label > b.label ? 1 : -1));

 // Tensorflow works with arrays and not objects so we need to separate labels and tensors.
  sortedData.map((item) => {
    createMultidimentionalArrays(justLabels, item.label, item.label);
    createMultidimentionalArrays(justFeatures, item.label, item.features);
  });
};

function createMultidimentionalArrays(dataArray, index, item) {
  !dataArray[index] && dataArray.push([]);
  dataArray[index].push(item);
}

Listing 5-71In train.js. Sorting and formatting the data

```

运行这个应该会导致`justFeatures`和`justLabels`分别是包含特性和标签索引的多维数组。

例如，`justLabels`应该看起来像[ [ 0，0，0，0，0，0，0，0，0，0，0，0，0，0，0，0，0，0，0，0 ]，[ 1，1，1，1，1，1，1，1，1，1，1，1，1，1，1，1，1，1，1，1，1，1，2，2，2，2，2，2，2，2，2，2

既然我们越来越接近 TensorFlow 可以处理的格式，我们仍然需要将这些多维数组转换成张量。为此，让我们首先创建一个名为`transformToTensor`的函数。

```py
const [
    trainingFeatures,
    trainingLabels,
    testingFeatures,
    testingLabels,
] = transformToTensor(justFeatures, justLabels);

const transformToTensor = (features, labels) => {
  return tf.tidy(() => {
    // Preparing to split the dataset between training set and test set.
    const featureTrainings = [];
    const labelTrainings = [];
    const featureTests = [];
    const labelTests = [];
    // For each gesture trained, convert the data to tensors and store it between training set and test set.
    for (let i = 0; i < gestureClasses.length; ++i) {
      const [
        featureTrain,
        labelTrain,
        featureTest,
        labelTest,
      ] = convertToTensors(features[i], labels[i], 0.2);
      featureTrainings.push(featureTrain);
      labelTrainings.push(labelTrain);
      featureTests.push(featureTest);
      labelTests.push(labelTest);
    }

    // Return all data concatenated
    return [
      tf.concat(featureTrainings, 0),
      tf.concat(labelTrainings, 0),
      tf.concat(featureTests, 0),
      tf.concat(labelTests, 0),
    ];
  });
};

Listing 5-72In train.js. Transforming multidimensional arrays into tensors

```

前面的代码调用了一个名为`convertToTensors`的函数，让我们来定义它。

```py
const convertToTensors = (featuresData, labelData, testSplit) => {
  if (featuresData.length !== labelData.length) {
    throw new Error(
      "features set and labels set have different numbers of examples"
    );
  }
  // Shuffle the data to avoid having a model that gets used to the order of the samples.
  const [shuffledFeatures, shuffledLabels] = shuffleData(
    featuresData,
    labelData
  );

  // Create the tensor
  const featuresTensor = tf.tensor2d(shuffledFeatures, [
    numSamplesPerGesture,
    totalNumDataPerFile,
  ]);

  // Create a 1D `tf.Tensor` to hold the labels, and convert the number label from the set {0, 1, 2} into one-hot encoding (e.g., 0 --> [1, 0, 0]).
  const labelsTensor = tf.oneHot(
    tf.tensor1d(shuffledLabels).toInt(),
    numClasses
  );

  // Split all this data into training set and test set and return it.
  return split(featuresTensor, labelsTensor, testSplit);
};

Listing 5-73In train.js. Convert data to tensors

```

这个函数调用另外两个函数，`shuffleData`和`split`。

```py
const split = (featuresTensor, labelsTensor, testSplit) => {
  // Split the data into a training set and a test set, based on `testSplit`.
  const numTestExamples = Math.round(numSamplesPerGesture * testSplit);
  const numTrainExamples = numSamplesPerGesture - numTestExamples;

  const trainingFeatures = featuresTensor.slice(
    [0, 0],
    [numTrainExamples, totalNumDataPerFile]
  );
  const testingFeatures = featuresTensor.slice(
    [numTrainExamples, 0],
    [numTestExamples, totalNumDataPerFile]
  );
  const trainingLabels = labelsTensor.slice(
    [0, 0],
    [numTrainExamples, numClasses]
  );

  const testingLabels = labelsTensor.slice(
    [numTrainExamples, 0],
    [numTestExamples, numClasses]
  );

  return [trainingFeatures, trainingLabels, testingFeatures, testingLabels];
};

Listing 5-75In train.js. Split the data into training and test set

```

```py
const shuffleData = (features, labels) => {
  const indices = [...Array(numSamplesPerGesture).keys()];
  tf.util.shuffle(indices);

  const shuffledFeatures = [];
  const shuffledLabels = [];

  features.map((featuresArray, index) => {
    shuffledFeatures.push(features[indices[index]]);
    shuffledLabels.push(labels[indices[index]]);
  });

  return [shuffledFeatures, shuffledLabels];
};

Listing 5-74In train.js. Shuffle the data

```

此时，如果您在代码中添加一个`console.log`语句来记录`format`函数中的`trainingFeatures`变量，您应该会得到一个张量作为输出。

```py
Tensor {
  kept: false,
  isDisposedInternal: false,
  shape: [ 12, 600 ],
  dtype: 'float32',
  size: 7200,
  strides: [ 600 ],
  dataId: {},
  id: 70,
  rankType: '2',
  scopeId: 0
}

Listing 5-76Example of output tensor

```

“`shape`”数组中的值将根据您训练的数据样本数量和每个文件的行数而有所不同。

总之，从`format`函数开始的代码示例应该如下所示。

```py
const format = (allData) => {
  let sortedData = allData.sort((a, b) => (a.label > b.label ? 1 : -1));

  sortedData.map((item) => {
    createMultidimentionalArrays(justLabels, item.label, item.label);
    createMultidimentionalArrays(justFeatures, item.label, item.features);
  });

  const [
    trainingFeatures,
    trainingLabels,
    testingFeatures,
    testingLabels,
  ] = transformToTensor(justFeatures, justLabels);
};

function createMultidimentionalArrays(dataArray, index, item) {
  !dataArray[index] && dataArray.push([]);
  dataArray[index].push(item);
}

const transformToTensor = (features, labels) => {
  return tf.tidy(() => {
    const featureTrainings = [];
    const labelTrainings = [];
    const featureTests = [];
    const labelTests = [];
    for (let i = 0; i < gestureClasses.length; ++i) {
      const [
        featureTrain,
        labelTrain,
        featureTest,
        labelTest,
      ] = convertToTensors(features[i], labels[i], 0.2);
      featureTrainings.push(featureTrain);
      labelTrainings.push(labelTrain);
      featureTests.push(featureTest);
      labelTests.push(labelTest);
    }

    return [
      tf.concat(featureTrainings, 0),
      tf.concat(labelTrainings, 0),
      tf.concat(featureTests, 0),
      tf.concat(labelTests, 0),
    ];
  });
};

const convertToTensors = (featuresData, labelData, testSplit) => {
  if (featuresData.length !== labelData.length) {
    throw new Error(
      "features set and labels set have different numbers of examples"
    );
  }

  const [shuffledFeatures, shuffledLabels] = shuffleData(
    featuresData,
    labelData
  );

  const featuresTensor = tf.tensor2d(shuffledFeatures, [
    numSamplesPerGesture,
    totalNumDataPerFile,
  ]);

  const labelsTensor = tf.oneHot(
    tf.tensor1d(shuffledLabels).toInt(),
    numClasses
  );

  return split(featuresTensor, labelsTensor, testSplit);
};

const shuffleData = (features, labels) => {
  const indices = [...Array(numSamplesPerGesture).keys()];
  tf.util.shuffle(indices);

  const shuffledFeatures = [];
  const shuffledLabels = [];

  features.map((featuresArray, index) => {
    shuffledFeatures.push(features[indices[index]]);
    shuffledLabels.push(labels[indices[index]]);
  });

  return [shuffledFeatures, shuffledLabels];
};

const split = (featuresTensor, labelsTensor, testSplit) => {
  const numTestExamples = Math.round(numSamplesPerGesture * testSplit);
  const numTrainExamples = numSamplesPerGesture - numTestExamples;

  const trainingFeatures = featuresTensor.slice(
    [0, 0],
    [numTrainExamples, totalNumDataPerFile]
  );
  const testingFeatures = featuresTensor.slice(
    [numTrainExamples, 0],
    [numTestExamples, totalNumDataPerFile]
  );
  const trainingLabels = labelsTensor.slice(
    [0, 0],
    [numTrainExamples, numClasses]
  );

  const testingLabels = labelsTensor.slice(
    [numTrainExamples, 0],
    [numTestExamples, numClasses]
  );

  return [trainingFeatures, trainingLabels, testingFeatures, testingLabels];
};

Listing 5-77In train.js. Full code sample for formatting the data

```

如果你是机器学习和 TensorFlow.js 的新手，可能会有很多东西需要理解，但我们就快到了。我们的数据被格式化并在训练集和测试集之间分割，所以最后一步是模型和训练的创建。

### 5.3.5 创建和训练模型

这部分代码有点随意，因为有多种方法来创建模型和选择参数值。但是，您可以复制以下代码作为起点，并在以后尝试不同的值，看看它们如何影响模型的准确性。

```py
const createModel = async (featureTrain, labelTrain, featureTest, labelTest) => {
  const params = { learningRate: 0.1, epochs: 40 };
  // Instantiate a sequential model
  const model = tf.sequential();
  // Add a few layers
  model.add(
    tf.layers.dense({
      units: 10,
      activation: "sigmoid",
      inputShape: [featureTrain.shape[1]],
    })
  );
  model.add(tf.layers.dense({ units: numClasses, activation: "softmax" }));
  model.summary();

  const optimizer = tf.train.adam(params.learningRate);
  model.compile({
    optimizer: optimizer,
    loss: "categoricalCrossentropy",
    metrics: ["accuracy"],
  });
  // Train the model with our features and labels
  await model.fit(featureTrain, labelTrain, {
    epochs: params.epochs,
    validationData: [featureTest, labelTest],
  });
  // Save the model in our file system.
  await model.save("file://model");
  return model;
};

Listing 5-78In train.js. Create, train, and save a model

```

在我们的`format`函数的末尾，使用`createModel(trainingFeatures, trainingLabels, testingFeatures, testingLabels)`调用这个 createModel 函数。

现在，如果一切正常，并且您在您的终端中运行了`node train.js`，您应该看到模型训练并在您的应用程序中找到一个`model`文件夹！

如果有些东西没有按预期工作，那么完整的`train.js`文件应该是这样的。

```py
const lineReader = require("line-reader");
var fs = require("fs");
const tf = require("@tensorflow/tfjs-node");

let justFeatures = [];
let justLabels = [];
const gestureClasses = ["letterA", "letterB", "letterC"];
let numClasses = gestureClasses.length;

let numSamplesPerGesture = 5;
let totalNumDataFiles = numSamplesPerGesture * numClasses;
let numPointsOfData = 6;
let numLinesPerFile = 100;
let totalNumDataPerFile = numPointsOfData * numLinesPerFile;

function readFile(file) {
  let allFileData = [];

  return new Promise((resolve, reject) => {
    fs.readFile(`data/${file}`, "utf8", (err, data) => {
      if (err) {
        reject(err);
      } else {
        lineReader.eachLine(`data/${file}`, function (line) {
          let dataArray = line
            .split(" ")
            .map((arrayItem) => parseFloat(arrayItem));
          allFileData.push(...dataArray);
          let concatArray = [...allFileData];

          if (concatArray.length === totalNumDataPerFile) {
            let label = file.split("_")[1];
            let labelIndex = gestureClasses.indexOf(label);
            resolve({ features: concatArray, label: labelIndex });
          }
        });
      }
    });
  });
}

const readDir = () =>
  new Promise((resolve, reject) =>
    fs.readdir(`data/`, "utf8", (err, data) =>
      err ? reject(err) : resolve(data)
    )
  );

(async () => {
  const filenames = await readDir();
  let allData = [];
  filenames.map(async (file) => {
    let originalContent = await readFile(file);
    allData.push(originalContent);
    if (allData.length === totalNumDataFiles) {
      format(allData);
    }
  });
})();

const format = (allData) => {
  let sortedData = allData.sort((a, b) => (a.label > b.label ? 1 : -1));
  sortedData.map((item) => {
    createMultidimentionalArrays(justLabels, item.label, item.label);
    createMultidimentionalArrays(justFeatures, item.label, item.features);
  });

  const [trainingFeatures, trainingLabels, testingFeatures, testingLabels] = transformToTensor(justFeatures, justLabels);

  createModel(trainingFeatures, trainingLabels, testingFeatures, testingLabels);
};

function createMultidimentionalArrays(dataArray, index, item) {
  !dataArray[index] && dataArray.push([]);
  dataArray[index].push(item);
}

const transformToTensor = (features, labels) => {
  return tf.tidy(() => {
    const featureTrainings = [];
    const labelTrainings = [];
    const featureTests = [];
    const labelTests = [];
    for (let i = 0; i < gestureClasses.length; ++i) {
      const [featureTrain, labelTrain, featureTest, labelTest] = convertToTensors(features[i], labels[i], 0.2);
      featureTrainings.push(featureTrain);
      labelTrainings.push(labelTrain);
      featureTests.push(featureTest);
      labelTests.push(labelTest);
    }

    const concatAxis = 0;
    return [
      tf.concat(featureTrainings, concatAxis),
      tf.concat(labelTrainings, concatAxis),
      tf.concat(featureTests, concatAxis),
      tf.concat(labelTests, concatAxis),
    ];
  });
};

const convertToTensors = (featuresData, labelData, testSplit) => {
  if (featuresData.length !== labelData.length) {
    throw new Error(
      "features set and labels set have different numbers of examples"
    );
  }

  const [shuffledFeatures, shuffledLabels] = shuffleData(
    featuresData, labelData);

  const featuresTensor = tf.tensor2d(shuffledFeatures, [
    numSamplesPerGesture,
    totalNumDataPerFile,
  ]);
  const labelsTensor = tf.oneHot(
    tf.tensor1d(shuffledLabels).toInt(),
    numClasses
  );

  return split(featuresTensor, labelsTensor, testSplit);
};

const shuffleData = (features, labels) => {
  const indices = [...Array(numSamplesPerGesture).keys()];
  tf.util.shuffle(indices);

  const shuffledFeatures = [];
  const shuffledLabels = [];

  features.map((featuresArray, index) => {
    shuffledFeatures.push(features[indices[index]]);
    shuffledLabels.push(labels[indices[index]]);
  });

  return [shuffledFeatures, shuffledLabels];
};

const split = (featuresTensor, labelsTensor, testSplit) => {
  const numTestExamples = Math.round(numSamplesPerGesture * testSplit);
  const numTrainExamples = numSamplesPerGesture - numTestExamples;

  const trainingFeatures = featuresTensor.slice(
    [0, 0],
    [numTrainExamples, totalNumDataPerFile]
  );
  const testingFeatures = featuresTensor.slice(
    [numTrainExamples, 0],
    [numTestExamples, totalNumDataPerFile]
  );
  const trainingLabels = labelsTensor.slice(
    [0, 0],
    [numTrainExamples, numClasses]
  );
  const testingLabels = labelsTensor.slice(
    [numTrainExamples, 0],
    [numTestExamples, numClasses]
  );

  return [trainingFeatures, trainingLabels, testingFeatures, testingLabels];
};

const createModel = async (xTrain, yTrain, xTest, yTest) => {
  const params = { learningRate: 0.1, epochs: 40 };
  const model = tf.sequential();
  model.add(
    tf.layers.dense({
      units: 10,
      activation: "sigmoid",
      inputShape: [xTrain.shape[1]],
    })
  );
  model.add(tf.layers.dense({ units: numClasses, activation: "softmax" }));
  model.summary();

  const optimizer = tf.train.adam(params.learningRate);
  model.compile({
    optimizer: optimizer,
    loss: "categoricalCrossentropy",
    metrics: ["accuracy"],
  });

  await model.fit(xTrain, yTrain, {
    epochs: params.epochs,
    validationData: [xTest, yTest],
  });

  await model.save("file://model");
  return model;
};

Listing 5-79Complete code sample in train.js

```

您应该在终端中看到的培训步骤应该如下图所示。

![../images/496132_1_En_5_Chapter/496132_1_En_5_Fig56_HTML.jpg](../images/496132_1_En_5_Chapter/496132_1_En_5_Fig56_HTML.jpg)

图 5-56

训练步骤的输出示例

模型的输出让我们看到训练的最后一步显示了 0.9 的准确率，真的很好！

现在，为了用实时数据测试这一点，让我们继续这个项目的最后一步，使用我们的模型生成预测。

### 实时预测

对于这最后一步，让我们创建一个名为`predict.js`的新 JavaScript 文件。

我们将创建一个名为“`/predict`”的新端点，为我们的 index.html 文件提供服务，使用类似的 web 套接字代码将运动数据从我们的手机发送到我们的服务器，并运行实时预测。

第一个小的修改是在我们前端代码的初始`index.js`文件中。我们需要用下面的数据来代替字符串形式的运动数据。

```py
let data = {
    xAcc: accelerometerData.x,
    yAcc: accelerometerData.y,
    zAcc: accelerometerData.z,
    xGyro: gyroscopeData.x,
    yGyro: gyroscopeData.y,
    zGyro: gyroscopeData.z,
};
socket.emit("motion data", data);

Listing 5-80In index.js. Update the shape of the motion data sent via web sockets

```

因为必须将实时数据输入到模型中，所以发送一个数字对象要比我们在训练过程中使用的格式更容易。

然后，我们的`predict.js`文件将看起来非常类似于我们的`server.js`文件，除了一个额外的`predict`函数，该函数向模型提供实时数据并生成关于手势的预测。

```py
const tf = require("@tensorflow/tfjs-node");
const express = require("express");
const app = express();
var http = require("http").createServer(app);
const io = require("socket.io")(http);

let liveData = [];
let predictionDone = false;
let model;
const gestureClasses = ["letterA", "letterB", "letterC"];

// Create new endpoint
app.use("/predict", express.static(__dirname + "/"));

io.on("connection", async function (socket) {
  // Load the model
  model = await tf.loadLayersModel("file://model/model.json");
  socket.on("motion data", function (data) {
    predictionDone = false;
    // This makes sure the data has the same shape as the one used during training. 600 represents 6 values (x,y,z for accelerometer and gyroscope), collected 100 times.
    if (liveData.length < 600) {
      liveData.push(
        data.xAcc,
        data.yAcc,
        data.zAcc,
        data.xGyro,
        data.yGyro,
        data.zGyro
      );
    }
  });

  socket.on("end motion data", function () {
    if (!predictionDone && liveData.length) {
      predictionDone = true;
      predict(model, liveData);
      liveData = [];
    }
  });
});

const predict = (model, newSampleData) => {
  tf.tidy(() => {
    const inputData = newSampleData;
    // Create a tensor from live data
    const input = tf.tensor2d([inputData], [1, 600]);
    const predictOut = model.predict(input);
    // Access the highest probability
    const winner = gestureClasses[predictOut.argMax(-1).dataSync()[0]];
    console.log(winner);
  });
};

http.listen(process.env.PORT || 3000);

Listing 5-81In predict.js. Complete code for the predict.js file

```

如果您使用`node predict.js`运行前面的代码示例，请在您的手机上访问`'/predict'`上的页面，并执行我们训练的三种手势中的一种。按住屏幕时，一旦松开屏幕，您应该会在终端中看到一个预测！

运行实时预测时，您可能会遇到以下错误。当手势执行得太快，并且收集的数据量低于我们的 600 值时，就会发生这种情况，这意味着数据没有正确的形状供模型使用。如果你再尝试慢一点，它应该会工作。

![../images/496132_1_En_5_Chapter/496132_1_En_5_Fig57_HTML.jpg](../images/496132_1_En_5_Chapter/496132_1_En_5_Fig57_HTML.jpg)

图 5-57

手势执行过快时可能出错

现在，我们的实时预测工作了，您可以继续更改用于创建模型的一些参数，以查看它如何影响预测，或者训练不同的手势，甚至使用 web 套接字将预测发送回前端，以创建一个交互式应用程序。这最后一节的主要目标是涵盖创建您自己的机器学习模型所涉及的步骤。

在过去的几页中，我们学习了如何使用通用传感器 API 访问硬件数据，设置服务器和网络套接字来通信和共享数据，将运动数据保存到文件中，处理和转换数据，以及创建、训练和使用模型来预测实时手势！

希望它能让你更好地了解机器学习和 TensorFlow.js 提供的所有可能性。

然而，如果你是新手，这是很多新的信息，特别是最后一部分是非常先进的和实验性的，所以我不指望你理解一切并感到完全舒适。

如果您感兴趣的话，可以随意回顾代码示例，慢慢来，并尝试构建小型原型。**