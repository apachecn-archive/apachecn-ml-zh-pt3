# 8.机器学习和人工智能伦理

> *人们担心计算机会变得太聪明并接管世界，但真正的问题是它们太笨了，它们已经接管了世界。*
> 
> —佩德罗·多明戈斯

从超市收银台到机场值机，从数字医疗到网上银行，数据和人工智能在决策中的应用无处不在。在过去二十年中，数据可用性有了天文数字的增长，首先是连通性，现在是物联网。传统的数据科学团队专注于使用数据来创建、实施、验证和评估可用于预测分析的机器学习模型。行为心理学嵌入在数字健康中。

从推动应用程序用户提交健康数据、调查问题框架和内容演示，到向你的快餐应用程序订单追加销售一份炸薯条，机器学习正在影响我们的决策，它的影响需要治理。

在过去十年中，有大量数据驱动的人工智能和技术进步，特别是在医疗保健领域，提高了生活质量:

*   无创血糖水平:无创血糖管理有许多进展。谷歌已经开发了一种隐形眼镜，可以根据眼泪确定用户的血糖水平[96]。法国公司 PKvitality 开发了一种小型手表，可以确定血液中的葡萄糖水平，并且在通过使用远红外信号确定血糖水平方面取得了进展[97]。

*   人工胰腺:作为系统的组合，人工胰腺使用两个装置。第一个设备使用传感器测量血糖，并与第二个设备(或贴片)通信，第二个设备向患者施用胰岛素。胰岛素输送系统可以根据血糖水平调整胰岛素剂量[99]。

*   非传染性疾病的逆转/缓解:数字治疗学是一门新兴的数字健康学科，它将二型糖尿病等疾病重新定义为非慢性和渐进性疾病。数字疗法已被证明能够持续减轻体重，改善血糖控制，并使二型糖尿病病和糖尿病前期得到缓解[91]。

*   皮肤构造的生物打印:3d 打印已被用于复制血管和皮肤细胞，以促进烧伤患者的伤口愈合[100]。

*   同伴支持和关系:Diabetes.co.uk 等同伴支持社区在研究中证明了提高定性和定量健康结果的能力[101]。

*   足部溃疡检测:通过机器学习，可以越来越多地检测足部溃疡和瘀伤等问题以及更广泛的糖尿病足部问题，以加快溃疡检测，防止截肢，确定有效的治疗方法，并改善愈合时间[102]。

*   开源数据共享:数据存储库和公共 API 支持系统间的数据共享。历史上保守的公司现在正在接受开源分析、人工智能和数据管理软件。在许多组织中，员工在是否使用专有工具的问题上被极力劝阻或给予自主权。成本和性能是开源数据的驱动因素，主要是因为开源数据已经变得更加强大和被接受。最新一代的数据科学家和大学毕业生意识到开源数据的安全性和功能，也推动了这种采用[103]。

通过机器学习开发的问题的新解决方案本身正在导致道德和伦理问题。目前，治理正随着行业本身的步伐前进。人工智能中有许多场景是没有先例、规则或法律的。最重要的是考虑创造智能系统的伦理和道德含义。

世界卫生组织(WHO)报告称，到 2050 年，慢性病的患病率将上升至全球人口的 57%[104]。不幸的是，世界卫生组织还报告称，全球范围内医疗工作者的短缺越来越严重，到 2035 年将达到 1290 万人[105]。缺乏专业人员来提供医疗保健服务描绘了一幅严峻的未来图景，给人类带来了严重后果。人工智能、数字干预、物联网和其他数字技术正在抵消医疗保健专业人员的短缺，这些技术不仅可以取代人工和认知工作任务，还可以改善医疗保健的覆盖面、精确度和可用性。与此同时，疾病检测和诊断、基因组学、药理学、干细胞和器官治疗、数字医疗保健和机器人手术方面的进步有望最大限度地降低治疗疾病的成本。

随着人工智能渗透到人类的日常活动中，哲学、道德、伦理和法律问题也随之产生。这在医疗保健领域被放大了，在那里，临床决策可能意味着生死之别。

即使人工智能可以帮助诊断疾病或预测未来的死亡风险，人类会更喜欢人工智能的建议而不是医生的建议吗？随着人类变得习惯于与智能系统并肩生活，有许多障碍需要克服。

## 什么是伦理？

伦理或道德哲学是指塑造人们决策和行为的道德行为准则(或一套道德原则)。道德是指区分好的/正确的和坏的/错误的行为的原则。

例如，工作场所的道德规范通常通过员工必须遵守的职业行为准则来传达。

### 什么是数据科学伦理？

数据科学伦理是伦理的一个分支，关注隐私、决策和数据共享。

数据科学道德包括三个主要方面:

*   数据伦理

    数据科学道德的这一领域侧重于数据的生成、收集、使用、所有权、安全性和传输。

*   智力伦理学

    数据科学道德的这一领域涵盖了数据用于开发的预测分析的输出或结果。

*   实践伦理

    实践伦理是由弗洛里迪和轩辕洛尹提出的，指的是创新和系统的道德来指导新出现的问题[106]。

### 数据伦理

世界上存在的智能手机比人多——手机、平板电脑和数字设备以及应用程序、可穿戴设备和传感器每天都在创造数百万个数据点。有超过 72 亿部手机在使用，每年售出 1.12 亿部可穿戴设备，超过 100，000 个医疗保健应用程序可供下载到您的手机上[107–109]。IBM 报告称，每天创建的数据超过 2.5 万亿字节(2.5×10<sup>18</sup>)[110]。数据无处不在。此外，它是有价值的。

通过脸书-剑桥分析公司丑闻等高调的惨败，数据伦理的话题被推到了公众的聚光灯下。脸书是世界上最大和最受信任的数据收集组织之一，通过在其平台上举办的测验收集用户数据。

150 万名完成测试者的行为和人口统计数据被出售给剑桥分析公司。这些数据在很大程度上被认为被用来瞄准和影响美国 2017 年选举的结果[111]。更令人担忧的是，这种安全漏洞是在最初的数据泄露后两年多才被报告的。

我们处在一个假新闻比真相传播更快的时代。社会正处于其发展的关键点，必须共同解决数据的使用、接受和依赖问题，以就如何合乎道德地处理数据开展对话并制定指导原则。人们知道假新闻和错误信息。人工智能需要基于伦理和可解释的。

数据使用的伦理和道德含义是广泛的，并通过一个例子得到了最好的证明。在本章中，我们将参考以下假设场景。

Scenario A

约翰，2 型糖尿病，30 岁，严重低血糖发作，被紧急送往医院。

约翰昏迷不醒，已被送往医院接受治疗。卡车司机约翰在医生的建议下被开了胰岛素来治疗他的二型糖尿病，这很可能是他低血糖的原因。在这个过程中会产生大量数据:在医院里，由医疗保健专业人员产生，以及在约翰的 Apple Watch 上产生——他的心率、心率变异性、活动细节和血氧饱和度，以寻找糖尿病昏迷的迹象。约翰的血液样本也被采集，他的基因组被确认。让我们假设所有这些数据都被使用了，而且是有用的。

约翰的苹果手表在去急诊室的路上被用来监测他的心脏，通过它怀疑他有不规则的心跳，后来用医院的医疗设备证实了这一点。

从低血糖发作中醒来后，约翰很高兴地得知，基因检测并不总是带来坏消息，因为他患前列腺癌的风险降低了，因为他携带了 2020 年已知导致这种疾病的几种基因的低风险变体。然而，从他的基因分析中，约翰被告知他患老年痴呆症、结肠癌和中风的风险增加。

## 知情同意

知情同意是指用户(或患者)知道他们的数据将被用于什么目的。知情同意是指个人在法律上能够给予同意。一般来说，这要求一个人年满 18 岁，心智健全，能够行使选择权。同意最好是自愿的。

场景 A 展示了如何将有用的数据赋予特定的上下文(或用例),并展示了知情同意的许多复杂性。

## 选择的自由

选择的自由指的是决定你的数据是否共享以及与谁共享的自主权。这是指主动决定与任何第三方共享您的数据。例如，现在是否应该要求患有二型糖尿病病的约翰在被允许再次驾驶卡车之前证明他的血糖水平得到控制并在建议的范围内？

一个人对是否共享其数据的选择可能会导致未来人们被免除机会，直到他们的数据以其他方式证明。在理想世界中，每个人都应该有权选择是否共享他们的数据。

实际上，这既不现实也不合理。

约翰不同意或不想同意医疗团队使用他的 Apple Watch 数据，也就是说，约翰的 Apple Watch 数据属于约翰，这涉及道德问题。紧急响应小组是否应该仅使用 John 的心率来确保其正在跳动，或者随后诊断 John 的房颤是否符合伦理？在知情同意的情况下，John 可以做出选择，这是数据伦理的一个基本支柱。

这是病人以前不需要做的选择。在现代生活数据化之前，获取这种数据的设备很少，预测未来事件的手段也远没有那么复杂。

人工智能和机器学习在医疗保健领域的进步意味着，今天有两类人:一类人寻求健康信息来帮助他们规划和管理未来的场景，另一类人乐于生活在未知的状态中。随着万物数据化的继续，那些乐于生活在无知幸福状态中的人发现这样做的机会越来越少。

## 一个人的数据同意应该被推翻吗？

在理想的世界里，个人分享数据的决定应该得到尊重。但是，作为一个绝对的概念，这既不现实，也不可信。例如，让我们假设约翰拒绝同意使用他的数据。假设紧急响应小组的职责是保护其患者的健康，John 的数据有助于监测他的生命体征，这有助于他的生存。可以说，如果应急响应团队在给定时刻不使用他们可以获得的各种数据，他们的行为将是不道德的。如果这意味着增加他的生存机会，甚至约翰自己也会否决他的同意。

先例展示了选择的自由，而同意最终是一个乌托邦式的概念。在德国，一名男子被指控强奸并谋杀了一名 19 岁的医科学生，他的智能手机上的健康数据在审判中被用来指控他[113]。犯罪现场发现的一根头发确认了嫌疑人的身份，他拒绝向警方提供自己智能手机的密码。警方获得了慕尼黑一家网络取证公司的帮助，该公司侵入了该设备。嫌疑人 iPhone 上的数据包括他的脚步和高度，警方对这些数据进行了分析。警方表示，嫌疑人的海拔(爬楼梯)数据可能与他将受害者拖下河岸并爬上来有关。除了定位嫌疑人的运动，手机还显示了剧烈活动的时段，其中包括两个高峰，车载智能手机应用程序将其归因于爬楼梯。

警方调查人员模仿了他们认为嫌疑人是如何处理尸体的，并展示了在嫌疑人的 iPhone 上检测到的两个相同的爬楼梯高峰。

## 公众理解

脸书-剑桥分析公司的惨败进一步凸显了公众对数据隐私的无知。美国参议院对脸书首席执行官马克·扎克伯格的审讯表明了公众对技术话题的无知和无知。参议员们对脸书作为一个免费平台如何赚钱感到困惑，提到了通过 WhatsApp 发送电子邮件，并询问加密信息是否可以用来提供定向广告[114]。

关键利益相关者对数据治理主题的误解和无知令人震惊。

需要提高公众的认识和理解，以教育公众如何使用数据，并使人们能够决定如何、在哪里以及由谁来使用他们的数据。

## 谁拥有我的数据？

人们每天产生数千个数据点。几乎每一笔交易和行为都会在智能手机、电视、智能手表、移动应用、健康设备、非接触式卡、汽车甚至冰箱等设备上留下数据痕迹。然而，谁拥有这些数据？数据所有权的主题是一个复杂的首次出现的问题的例子，它促进了国际政策和治理的发展。从历史上看，用户数据归公司所有，而非个人所有。

EHR 实现了基于机器学习的预测分析，最终将使提供商能够提供更高级别的护理。尽管患者似乎不太可能不想分享他们的健康数据以改善发病率和死亡率，但人们可以合理地假设取消这种特权将变得更加困难，这与闭路电视(CCTV)非常类似，在闭路电视中，同意往往不被注意到或不明显，并且被广泛认为具有长期的功利利益。

如果数据共享被禁用，许多平台会阻止用户访问他们的全部服务。这是因为组织需要将数据存储在一个强大的中央存储库中，以实现安全、治理和改进。

举一个患者的例子，他使用连接的血糖设备和移动应用程序来跟踪和记录他们的血糖。血糖数据从用户的血糖仪传送到他们的移动应用程序。尽管这些数据会显示在用户的手机上，但根据使用条款和条件，这些数据会保存在移动应用程序提供商的数据库中。当今的许多网站、移动应用程序、联网设备和医疗服务都声明，数据可以而且将会以匿名和聚合的格式使用，或者在某些情况下，由提供服务的组织以可识别的格式使用，通常还会与选定的合作伙伴一起使用。

数据收集、使用和共享已经成为支持数据驱动的质量改进过程的基础部分。有几种类型的数据可以共享。

### 匿名数据

匿名数据是删除了可识别特征的数据。可识别特征是使某人能够识别数据来源的特征。例如，如果姓名、出生日期和患者编号被删除，肿瘤科病房的患者电子表格将被匿名化。

### 可识别数据

可识别数据是指可用于识别个人身份的数据。例如，如果一个肿瘤科病房的病人电子表格报告了姓名、出生日期和病人编号，那么它就是可识别的。

### 汇总数据

汇总数据是指已经合并并报告总数的数据。以肿瘤学为例，如果病房的电子表格涵盖 10 名患者，则汇总数据报告可能包括患者的男女比例或年龄段。数据集内的总体数据是累积报告的。

### 个性化数据

个性化数据与聚合数据相反。数据不是被组合，而是为数据集中的每个人报告数据。个性化数据不一定是可识别的。

### 数据控制器和处理器

对数据隐私的担忧引发了全球反应。2018 年 5 月，GDPR(或通用数据保护条例)在整个欧洲生效。GDPR 立法管理组织如何使用和共享用户数据[115]。

由于 GDPR，组织被迫收集其成员的选择同意，并向用户声明他们将如何使用用户数据，以及他们应该与谁共享这些数据。GDPR 立法将数据控制权牢牢掌握在用户手中。

数据驱动系统的用户现在能够行使他们的权利，查看他们持有的数据，与谁共享他们的数据以及为什么共享，被遗忘的权利和被删除的权利。GDPR 还为那些收集和处理数据的人定义了标签。

#### 数据控制器

数据控制者是指控制、存储和使用数据的人或组织。

#### 数据处理程序

数据处理者是指代表数据控制者处理数据的人或组织。基于这个定义，诸如计算器之类的代理可以被认为是数据处理器。

对于那些处理数据的人来说，理解数据处理器和控制器之间的区别以及各方的责任是很有用的。必须确定这些责任，并且已经在欧洲等地区通过通用数据保护法规(GDPR)进行了规定。

GDPR 展示了数据安全变得多么重要。GDPR 正在加强数据访问、安全和管理，雄心勃勃地试图保护欧盟的 5 亿公民。它独自重新定位了对用户的控制。违反 GDPR 法规的最严重违规者的最高罚款为 2000 万€(1760 万英镑)，对于大型组织，最高罚款为全球收入的 4%[115]。不幸的是，法规影响到所有的企业和组织，一系列的同意请求充斥着互联网。正如马克·扎克伯格(Mark Zuckerberg)被欧洲议会(European Parliament)传唤时所显示的那样，人们对 GDPR 法律的适用存在很多困惑。就像在美国一样，关键人物对数据和连接在 21 世纪如何运作一无所知。

#### 被遗忘的权利…从记忆中？

机器学习中用户被遗忘的权利这一话题已经提出了几个引人注目的伦理问题。例如，如果在机器学习算法中使用约翰的数据来预测严重低血糖的可能性，并且如果约翰请求删除他的数据，那么将约翰的数据与所学习的机器学习模型分离几乎是不可能的。如何从模型的内存中删除 John 的数据？如果存在功利利益，约翰的同意应该被推翻吗？

此外，如果约翰的数据由于某种原因在开发诊断和预测疾病的算法中是有价值的，那么可以声称约翰拒绝共享他的数据或请求从这样的算法中删除他的数据是不道德的。如果约翰是世界上唯一一个有基因缺陷的病人，或者如果他的数据出于某种原因对医学理解的进步有用，那么约翰的数据如何以及为什么对人类有用是显而易见的。

## 我的数据可以用来做什么？

数据已经被用来推动各种决策。几十年来，雇主一直使用心理测量和参数测试来了解潜在的候选人。如今，雇主们也关注数据的支持来源。雇主们经常在潜在员工的社交媒体档案中搜寻声誉风险。同样，历史上充斥着因(被认为不恰当的)社交媒体使用而失业的人。

个人数据的用途引发了伦理和道德上的担忧。身份和自由意志的概念受到这样一种观念的挑战，即个人的数据可能被用来作出对该人可能不知道的直接影响的决定。

汽车保险是一个不断发展以优化数据使用的行业。从历史上看，汽车保险费是以事故索赔数据为基础的，而事故索赔数据是针对部分人群的。黑匣子的出现使基于各种人口统计和行为因素的更精确的保费成为可能，例如司机的年龄、汽车的使用次数、驾驶速度和不稳定加速的频率等。

如果驾驶员是明智的，该数据还可以用于降低汽车保险费的成本，并且如果驾驶员没有遵守保险公司制定的规定，则能够增加汽车保险费。其他人将这种持续的实时数据分析(以及后续的反馈)视为“老大哥”。

同样，人寿和健康保险也开始步汽车保险的后尘。人寿和健康保险公司越来越多地提供激励性产品，奖励积极行为，惩罚消极行为。例如，人寿保险公司 Vitality 提供了一种产品，向其会员提供苹果手表，以鼓励积极健康的行为。类似地，英国组织糖尿病数字媒体向世界各地的保险公司和账单支付者提供基于证据的数字健康干预，以激励健康。

个人应该意识到，数据也可能被用于愤世嫉俗、更加邪恶的目的。例如，患者数据可用于拒绝个人的治疗、手术或机会。公众对数据的使用非常信任，因此需要强大的数据治理来防止未来的滥用。GDPR 被视为提高理解和数据治理的第一步。一些人认为 GDPR 对数据科学的革命性不亚于互联网的出现。

## 隐私:谁可以看到我的数据？

关于数据所有权的对话自然会引出谁能看到你的数据。关键在于确保只有经过批准的服务和组织才能访问您的数据。例如，你不太可能希望一家人寿保险公司在未经你同意的情况下获得你的医疗数据，尤其是如果这些数据可能会影响你的保险范围。

### 数据共享

应用程序之间的数据共享很常见，API 支持独立服务之间的可访问性和更快的连接。例如，用户可以将 MyFitnessPal 等应用程序中的营养数据导入他们的糖尿病管理或健身应用程序中。这些服务经常在各种独立的体系结构中复制用户数据，这导致了对管理批准的数据访问的关注。支持数据集成的应用程序还必须提供分离患者数据的功能。系统必须能够验证导入的数据，以进行数据治理、审计和患者安全。

围绕脸书和剑桥分析公司的溃败继续表明，即使你信任一个数据聚合者，第三方也有可能在你不知情的情况下处理这些数据。脸书要求剑桥分析公司删除其脸书用户数据。尽管剑桥分析公司同意了这一请求，并表示他们已经删除了用户数据，但后来证明他们并没有删除这些数据[111]。这种欺骗行为导致了责任问题、数据泄露的后果以及在第三方数据泄露的情况下谁应该负责。

### 匿名并不等同于隐私

此外，即使数据被匿名化，也不一定能保证隐私。网飞最近公布了来自 50 万会员的 1000 万部电影排名，作为改善网飞推荐系统的挑战的一部分。

虽然数据被匿名化以删除成员的个人信息，但德克萨斯大学的研究人员通过将类似的排名和时间戳与 IMDb(互联网电影数据库)的公开信息进行比较，能够对一些网飞数据进行去匿名化[116]。匿名数据存在天然的安全问题——主要是这并不意味着你是匿名的。

### 数据对不同的人有不同的价值

当涉及到共享数据时，区分人和病人是有用的。一般公众对医疗领域数据使用的态度远比其对非医疗数据使用的态度更受欢迎。患者似乎理解或至少希望，通过共享健康数据，他们正在推进医学理解和治疗。然而，人们对有益的数据使用持怀疑态度。

保密性是数据伦理的核心原则。

## 数据将如何影响未来？

共享患者数据和聚合大数据集正被用于增强诊断、治疗和护理。随着数据类型和质量的提高，医疗保健的精确度将在接下来讨论的领域变得更加精确。

### 优先处理

大数据医疗数据集可以实现预测分析，从而为不同人群确定最佳治疗路径。有可能只优先考虑那些可能在治疗中看到疗效的人，这就引出了如何最好地支持那些没有看到疗效的人的问题。

### 确定新的治疗和管理途径

对真实世界经验数据、临床研究、随机临床试验(RCT)和药理学数据的分析有助于治疗和管理发现。

数字健康干预已被证明可以逆转二型糖尿病，减少癫痫患者的发作次数[91，117]。数据有可能开发新的药物干预措施，并打破传统的治疗模式。

### 更多真实世界的证据

越来越多的证据表明，来自患者社区、数字教育项目和健康跟踪应用的真实世界证据(RWE)的大规模使用改善了自我管理的前景。对现实世界证据的担忧通常是缺乏学术稳健性。

然而，RWE 正越来越多地用于研究，以确定人口的使用和利益。真实世界的证据是人工智能伦理之旅的重要组成部分，患者和提供者之间必须有信任。

### 药理学的增强

RCT 和学术研究所需的精确度可以通过数字平台更快、更容易地确定。这有很多好处，比如更快的项目招聘时间、更大的潜力和多来源比较。现实世界的数据正被用于开发更好、更有效的药物。

## 网络安全

真正统一的系统会带来严重的隐私问题。如果这样一个系统被破坏了会怎么样？人们，或者病人，想要一个真正统一的系统，还是用它来做坏事的可能性大于潜在的好处？漏洞的后果—无论是安全、运营还是技术—在统一系统中被放大。

多年来，大公司一直在讨论如何结合和分析他们分别收集的关于人的巨大数据库——智能手机的位置数据、银行的金融数据、社交网络应用程序的关系数据和浏览器的搜索数据——来构建一个人行为的完整画面。

据报道，脸书提出将医院掌握的关于患者个人的数据与从社交网站上收集的社交信息进行匹配，例如这个人有多少朋友，或者他们是否在网站上与他人交往。在剑桥分析丑闻引发隐私担忧后，该公司暂停了该项目。

此外，物联网设备也存在安全问题，从温度计、汽车和洗衣机到血糖仪、连续血糖监测仪和胰岛素泵。作为智能设备运行的网络连接要求使得所有设备都容易受到攻击。

分布式拒绝服务(DDoS)攻击被用于物联网设备，以侵入并留下恶意代码来开发僵尸网络、泄漏数据或以其他方式危害设备。WeLiveSecurity 报告了 73，000 个使用默认密码的安全摄像头[118]。从中学习的一个基本但关键的是，更改使用默认密码的凭证总是明智的。DDoS 攻击并不新鲜，但是漏洞的程度只有通过研究和首次出现的问题或安全漏洞才能被发现。佛罗里达大学的研究生能够在不到 15 秒的时间内破解谷歌的 Nest 恒温器。

DDoS 攻击对任何组织来说都是毁灭性的。制定缓解程序并确保网络基础设施允许进出网络的流量可见。制定 DDoS 防御计划是一个很好的做法，应该保持并定期更新和演练。

## 人工智能和机器学习伦理

机器学习在医疗保健中的主要应用涉及患者诊断和治疗。人工智能模型被用来帮助医生诊断患者，特别是在涉及相对罕见的疾病或结果难以预测的情况下。

想象一下未来，医生知道你上个月吃了多少次快餐，并且与你的医疗记录相关联。这些数据被用来建议吃什么食物，将你的健康记录与成百上千的其他人进行比较，以确定谁和你一样。此外，想象一下这可能会直接影响人寿保险或健康保险，对积极的行为和避免不良食品可能会有奖励。想象一下，一个系统能够实时预测你患任何特定疾病的可能性。

### 什么是机器学习伦理？

机器学习的伦理特指围绕使用数据的机器学习模型的输出的道德问题，这些问题伴随着它们的伦理问题。

机器学习已经被用于开发智能系统，这些系统能够根据健康生物标志物预测死亡风险和寿命。人工智能已被用于分析 EHR 数据，以高度确定性预测心力衰竭的风险。

此外，机器学习可用于根据患者真实世界和临床数据确定最有效的药物剂量学习，从而降低患者和提供者的医疗保健成本。人工智能不仅可用于确定剂量，还可用于确定病人的最佳药物。随着基因数据变得可用，针对艾滋病和糖尿病等疾病的药物将适应种族、民族和个体对特定药物的反应之间的差异。在相同的数据中，可以跟踪药物相互作用和副作用。在临床试验和 FDA 批准要求着眼于受控环境的情况下，真实世界的大数据为我们提供了实时数据，如药物相互作用以及人口统计学、药物、遗传和其他因素对结果的实时影响。

随着技术的局限性受到考验，还有伦理和法律问题需要克服。

### 机器偏差

机器偏差是指机器学习模型表现出偏差的方式。机器偏差可能是由多种原因造成的，例如创建者对用于训练模型的数据的偏差。偏见通常被认为是第一次出现的问题，具有微妙而明显的后果。所有机器学习算法都依赖于统计偏差来预测看不见的数据。然而，机器偏见反映了数据开发者的偏见。

人工智能在速度和处理能力方面的能力远远超过人类。因此，不能总是相信它是公正和中立的。谷歌及其母公司 Alphabet 在人工智能方面处于领先地位，正如谷歌的照片服务所示，人工智能用于识别人、物体和场景。但它仍然可能出错，例如当搜索引擎显示白人和黑人青少年的比较搜索结果不敏感时。用来预测未来罪犯的软件已经被证明对黑人有偏见[120]。

人工智能系统是由人类创造的，而人类是有偏见和主观判断的。如果使用正确，并且被那些积极想要影响人类进步的人使用，人工智能将催化积极的变化。

### 数据偏差

偏差是指偏离预期的结果。有偏见的数据会导致糟糕的决策。偏见无处不在，包括数据本身。做好准备，最大限度地减少有偏差数据的影响。了解可能渗入数据并影响分析和决策的各种偏差。为最佳实践数据治理制定正式的记录程序。

### 人类偏见

只要人类参与决策，偏见就会一直存在。微软臭名昭著的 AI 聊天机器人 Tay 与其他 Twitter 用户互动，并从与其他人的互动中学习。一旦推特圈抓住了泰，巨魔们就把对话从积极的互动引向了与巨魔和喜剧演员的互动。几个小时内，Tay 就在推特上发布了性别歧视、种族歧视和暗示性的帖子。可悲的是，泰只活了 24 小时。这个实验提出了一个问题，如果人工智能从人类行为中学习，它是否真的安全[121]。

### 智力偏见

机器学习模型的好坏取决于它们接受训练的数据，这往往会导致某种形式的偏差。具有人类思维的人工智能系统已经证明了偏差放大，并导致许多数据科学家讨论人工智能技术的道德使用。建立在人口数据基础上的早期思维系统在性别、种族、社会地位和其他问题上表现出明显的偏见。

一个臭名昭著的算法偏见的例子可以在刑事司法系统中找到。在威斯康辛州的一个法庭案例中，对替代制裁的矫正罪犯管理剖析(COMPAS)算法进行了剖析。关于非裔美国人犯罪的不成比例的数据被输入到一个犯罪预测模型中，该模型随后输出对来自黑人社区的人的偏见。有很多有偏算法的例子和定义[122]。

例如，评估家庭保险风险的算法基于索赔数据对居住在特定地区的人有偏见。数据标准化是关键。如果数据没有针对这种敏感性进行标准化，系统没有得到适当验证，人类就有可能使机器学习模型偏向少数群体，并低估许多群体的代表性。

去除偏差不代表模型不会有偏差。即使创造了一个绝对无偏见的模型，我们也不能保证人工智能不会学习到和我们一样的偏见。

### 偏差校正

偏差修正始于承认偏差的存在。研究人员在 1985 年开始讨论机器学习伦理，当时 James Moor 定义了隐式和显式伦理代理[123]。隐式代理是合乎道德的，因为它们固有的编程或目的。显式代理是在不确定或未知的情况下，被给予原则或范例来学习以做出道德决策的机器。

克服偏差可能涉及关于模型校准的后处理。分类器应该被校准以对于敏感特征的所有子组具有相同的性能。数据重采样有助于平滑倾斜的样本。但是，由于许多原因，收集更多的数据并不容易，可能会导致预算或时间问题。

数据科学界必须积极努力消除偏见。工程必须诚实地质疑对过程、智能系统的先入之见，或者偏见如何暴露在数据或预测中。这可能是一个很难解决的问题，许多组织雇佣外部机构来挑战他们的实践。

工作场所的多样性也在防止偏见渗入智力。如果创建我们的人工智能系统的研究人员和开发人员本身缺乏多样性，那么人工智能系统解决的问题和使用的训练数据都会基于这些数据科学家输入人工智能训练数据的内容而变得有偏见。多样性确保了思维、道德和心态的多样性。这促进了更少偏见和更多样化的机器学习模型。

虽然算法可能是为了最好地避免偏见而编写的，但这样做是非常具有挑战性的。例如，甚至编写人工智能系统的人的动机也可能与医生和其他护理人员的动机不匹配，这可能会导致偏见。

### 偏见是坏事吗？

偏见提出了一个哲学问题，前提是机器学习假设偏见通常是不好的。想象一下，一个系统被它的评估者解释为有偏见，因此用新数据重新训练模型。如果模型输出类似的偏差结果，评估可能希望考虑这是输出的准确反映，因此需要重新考虑存在什么偏差。

这是两个物种之间社会和哲学冲突的开始。

### 预测伦理

随着先进的机器学习算法和模型的开发，更准确和可靠的结论将在短时间内实现。目前，技术正被用于解释各种图像，包括来自超声波、磁共振成像(MRI)、X 射线和视网膜扫描的图像。机器学习算法已经可以有效地识别眼睛图像上的潜在关注区域，并开发可能的假设。

通过良好的治理和透明度来确保对您的目标、数据和组织的信任是非常关键的。这是人工智能向前发展的基础。

#### 解释预测

随着人工智能算法变得更加智能，它们也变得更加复杂。对机器学习系统的构造保持无知或允许它们被构造为黑盒可能会导致伦理上有问题的结果。如果一个代理被发现预测不正确，那么发现导致隐藏的、实际上不可发现的事件的行为将是一项艰巨的任务。

数据和机器学习模型的可解释性是智能系统的一个重要方面。这不仅确保了模型的完整性，还确保了它试图解决正确的问题。嵌入数据科学的解决方案的用户总是更喜欢可以理解和解释的体验。数据科学家也可以使用可解释性指标作为验证和改进的基础。

机器学习黑盒可能会通过永远不为人知的程序员和数据选择来隐藏偏见、不公平和歧视。神经网络是一个典型的无法解释的算法的例子。反向传播算法的计算值无法解释。随着人工智能在模仿人类的准确性方面的发展，将会有更大的要求来确保人工智能不会染上人类的坏习惯。

### 防止错误

智能来自学习，不管你是人还是机器。智能机器就像人类一样，从错误中学习。数据科学家通常会开发带有培训、测试和验证阶段的机器学习模型，以确保系统在定义的容差内检测到正确的模式。机器学习模型开发的验证阶段无法涵盖现实世界中可能接收到的所有可能的参数排列。这些系统可以被人类无法愚弄的方式愚弄。需要治理和定期审计来确保人工智能系统按预期运行，并且人们不能影响模型来为自己的目的使用它。

预测的不正确分类会导致结果为假阳性或假阴性的情况。这两者的影响应该在您的领域环境中考虑。以乳腺癌的错误诊断为例。在假阳性的情况下，患者会被告知他们患有乳腺癌，而实际上他们并没有。被告知这种分类是不正确的将会给病人带来一些宽慰。假阴性将导致患者的疾病进展和最终正确的重新诊断。应该考虑假阴性预测导致的精神和身体创伤。应该随时告知患者结果的准确程度。

患者将使用的系统(面向患者)的信息治理，无论是预测性的还是非预测性的，都应该为结果错误提供可靠的风险缓解程序。对于因错误诊断而遭受创伤的患者，应考虑给予情感或心理支持。

除了作为创新的催化剂，敏捷数字技术推向市场的速度也是 AI 最大的陷阱。不仅技术会失败；他们可以被认定为表现不公平。

科技公司 LG 在消费电子展(Consumer Electronics Show)上推出了一款人工智能机器人，更为人所知的是 CES，这是由消费者技术协会举办的一年一度的贸易展，它无视演示者的指示。人工智能机器人被宣传为提供创新的便利，但它没有对主人的任何评论做出反应，要么出现故障，要么选择忽略命令[124]。

2020 年，有一种声音在说黑人和少数民族是如何看到技术被用来针对他们的。2020 年，在全球因一名黑人被警察杀害而感到沮丧之际，抖音可耻地通过算法隐藏了包含黑人的命也是命或乔治·弗洛伊德标签的帖子。Instagram 还指出，在此期间，它需要研究其系统内的影子禁令和偏见。影子禁令是“在没有透明度的情况下过滤人们，并限制他们的影响范围”的过程[127]。

无论是否在医疗保健领域，人工智能系统性能指标都应该透明，并定期接受审计，以确保人们接触到的是绝对和真实的。成本太高了，尤其是在医疗保健方面。医疗保健中的人工智能系统已经被证明失败了。在英国，NHS 乳腺癌筛查系统未能适当邀请女性进行筛查，观察家称多达 270 名女性可能因此死亡[128，129]。

在这种特殊情况下，负责运行系统的组织归咎于承包商。这种错误的道德影响和公众认知是巨大的。

### 有效期

需要确保机器学习模型随时间推移的有效性，以确保模型可以一般化并且一般化是有效的。定期测试和验证模型对于保持机器学习模型的完整性和精确性至关重要。次优的预测分析模型将提供不可靠的结果并破坏完整性。

### 防止算法变得不道德

算法可以，也确实，已经做出了不道德的行为。迄今为止，由于设计的原因，人工智能算法主要以不道德的方式执行。优步和大众等组织在医疗保健领域之外已经很好地证明了这一点。优步的灰球算法试图预测哪些乘客是便衣警察，并使用它来识别和拒绝运输[129]。大众汽车的算法允许车辆在测试阶段通过减少氮氧化物排放来通过排放测试[130]。

这两个组织因其公开欺骗和缺乏透明度而受到国际谴责。世界上最大的公司的先例表明，需要内部和外部审计来验证算法及其组织的完整性和道德性。

人们真正担心的是，人工智能不仅会从它的创造者那里，还会从它的经验中学会不道德的行为。

瑞士洛桑联邦理工学院智能系统实验室开展了一个项目，该项目监测机器人，旨在合作搜索积极资源并忽略危险物品[131，132]。机器人被设计成基因代理，配备有传感器和灯光，用于标记阳性资源的识别，阳性资源的数量有限。

每种药剂的基因组决定了它对刺激的反应，并会经历数百代的突变。代理人的学习通过代理人收到识别积极资源的积极标记和接近有毒物品的消极标记而得到加强。前 200 个表现最好的基因组被随机交配和突变，产生下一代药剂。在他们的第一代中，当他们发现一个积极的资源时，他们就会打开灯。这使得其他代理能够找到积极的资源。由于正面资源数量有限，并非所有代理都能受益。过度拥挤意味着一些代理人也会远离他们最初发现的积极资源。到了第 500 代，大多数智能体已经进化到当他们发现一个积极的资源时就关灯，三分之一的智能体进化到以一种与他们的编程完全相反的方式行动。一些代理人已经开始识别厌恶光线的说谎代理人。最初被设计为合作的代理最终由于稀缺性而相互撒谎。

当应用于激励性的临床决策支持系统时，这些问题会被放大，这些系统可以为其设计师或提供商带来更多利润，这些系统会推荐他们持有股份的临床测试、治疗或设备，或者通过改变转诊模式。

在医疗保健领域，这种情况非常令人担忧。所有的机器学习模型，不管它们的用途如何，都必须被管理和验证。被激励做出决策的机器学习模型也应该遵守透明、道德和审查的强大标准。

### 意想不到的后果

人工智能在医学中的应用越来越普遍；因此，首次出现的问题或意想不到的后果将决定公众对人工智能伦理的看法和方向。很少有技术已经嵌入到医疗保健中而没有非预期的或不利的影响。问题很快就变成了数据科学家如何代表人类减轻这些风险。

科幻作家艾萨克·阿西莫夫在 1942 年发表的《三大定律》对人工智能的伦理和意外后果有着重要的指导意义[133]。

阿西莫夫定律见于公元 2058 年第 56 版的《机器人学手册》中，它是为了防止机器人伤害人类而设计的安全特性:

1.  机器人不得伤害人类，也不得坐视人类受到伤害。

2.  机器人必须服从人类的命令，除非它与第一定律相冲突。

3.  机器人必须保护自己的存在，只要

    这种保护不会与第一或第二定律相冲突。

在阿西莫夫的故事中，类人的机器以违反直觉的方式运行。这些行为是代理商将阿西莫夫定律应用于其环境的意外后果。仅仅 70 年后，阿西莫夫的科幻幻想正诡异地变成现实。韩国公布了机器人伦理宪章，以防止弊病和 malin tent[135]；IEEE 和英国标准协会都发布了道德代理工程的最佳实践指南[136]。设计道德上合理的代理人的最佳实践指南通常基于阿西莫夫定律。

从根本上说，智能代理是由二进制代码组成的，不包含也不遵守阿西莫夫定律。他们的人类建筑师负责实施法律和减少意外后果的风险。

AI 不太可能像好莱坞描绘的那样变得邪恶并开启人性。相反，缺乏上下文可能会导致人工智能做出意想不到的灾难性行动。例如，一个负责在全球人口中根除艾滋病毒的智能代理最终可能会得出一些结论，即为了实现其目标，它应该杀死地球上的所有人。把 AI 智能框在一个负面的设定里更容易。

如果没有仔细的管理，人工智能代理的效用函数可能会允许潜在的有害场景。假设一个人工智能主体会有如此极端的适应性是有一定基础的，但这是值得考虑的。毫无疑问，人工智能有许多积极的意想不到的后果，可以拯救人类自身。

## 人类如何控制一个复杂的智能系统？

数百万年来，人类结合聪明才智创造了控制其他物种的方法和工具。作为一个物种，人类已经进化到支配地位，这主要是由从自己或他人的错误中学习的能力驱动的。通过这一点，人类已经开始开发工具来掌握更大、更快、更强的动物和技术，如精神或身体训练，以实现这些任务的最佳表现。

### 当 AI 比人类更聪明时会发生什么？

人造人工智能已经能够在利基领域超越人类的认知。Alphabet 开发的强化学习代理 AlphaGo Zero 充当了它的老师，掌握了围棋的棋局[137]。AlphaGo 能够在数千次游戏互动中，在没有人类或历史数据集帮助的情况下进行自我学习。人类注定要被进化中的 AI 种族所掌握吗？这个概念被称为奇点——人工智能超过人类的点。也不能指望一个智力超过人类的人工智能可以被关闭。一个相当聪明的代理人可能会预料到这一行动，并可能为自己辩护。

有了比人类更聪明的人工智能，特别是通过超智能自我学习，人类不再能够预测结果，因此必须为这种可能性做好准备。

## 智力

智能指的是人工智能模型的结果以及如何使用它们。智能为你看到的广告、你下载的应用、你在互联网上看到的内容、你租用的出租车以及你为贷款和抵押贷款等项目支付的价格提供了动力。

智能伦理学提出了一些问题，比如自动驾驶汽车是否应该保护司机或其他人。汽车应该优先保护乘客还是保护其他司机和公众？如果一个事故造成生命损失，那么应该采取什么方法？一旦自动驾驶汽车流行起来，涉及的人类越来越少，这可能就不是什么问题了。在此之前，如果自动驾驶且没有人类驾驶的自动驾驶汽车确实发生了事故，谁应该承担责任？涉及自动驾驶汽车事故的法庭案例表明，驾驶员经常在碰撞时使用自动驾驶仪[138]。

评估表明，人类不想要公平、功利的方法，而希望他们的车辆保护驾驶员。对这些主题的监管将有助于自动驾驶汽车的长期采用。

智能的另一个例子可以在虚拟助手中找到，如 Alexa，它彻底改变了人机交互和参与的方法。

随着人工智能的使用越来越深入到日常生活中，我们不能假定人类会保持警惕和警觉，组织应该采取措施向公众保证。谷歌的双工机器人的发声技巧在其开发者大会上得到了展示，在会上展示了它预约理发的场景。舞台上的揭幕仪式包括 Duplex 软件与一名发廊接待员进行对话。计算机生成的声音使用了人类语言中常见的停顿、口语和绕圈子。这种声音由谷歌的 DeepMind WaveNet 软件控制，该软件已经通过大量对话进行了训练，因此它知道人类的声音是什么样的，并且可以有效地模仿他们。虽然受到一些人的欢迎，但许多人担心智能人工智能故意欺骗人类，试图成为人类，并成功冒充人类。这扩大了公众的声音，即人类在与人工智能接触时需要明确的确认[139，140]。

自然语言可以分为两个部分:风格和内容。对交往、电子邮件或消息等通信的分析可以轻松识别情绪和观点。

例如，负面词汇可以用来理解某人的情绪健康状况。无论是公开还是私下，这都意味着巨大的影响。对社交媒体档案和其他非结构化数据源的分析可能会导致雇主或保险公司根据可供挖掘的参与度、沟通或情绪数据对人们做出预测。

## 健康智能

健康智能特指通过医疗保健人工智能开发的智能。在患者健康、成本和资源分配方面的改善推动下，健康智能正被应用于多个行业领域:

*   医疗保健服务:患者越来越多地使用预测分析服务来诊断疾病。例如，通过 Alphabet 和皇家马斯登医院合作开发的人工智能系统，可以检测到糖尿病视网膜病变。糖尿病数字媒体开发了一种足部溃疡检测算法，以便更早、更快地转诊到足病诊所。

*   药理学:通过学习真实世界的数据和患者资料，新药正在被发现。这使得制药公司能够开发新的更精确的药物。

*   人寿和健康保险:具有健康状况(或风险)的患者，如二型糖尿病和糖尿病前期患者，传统上可以获得高额保费，但他们可以使用数字健康干预来管理和改善他们的状况，并且由于他们的持续参与，可以获得降低的保费来激励健康。人寿和健康保险公司的收益是巨大的。保险公司不仅能够通过将保险与改善的健康状况相结合来吸引更多的人群，还可以通过减少索赔和降低辅助医疗和药物成本来节省资金，改善他们的风险状况，并优化风险算法和承保。

健康智能无法在公众的道德人工智能要求被忽视的环境中开发，例如个人数据。人工智能的伦理应该是开发健康智能系统不可或缺的一部分，在危险发生之前识别出危险。健康智能伦理对于理解数据的意图和医学结果的使用是不可或缺的。

## 谁应负责任？

人工智能在医疗保健中的应用是一个令人兴奋的机会，可以在短时间内改善患者护理并节省成本。准确、及时的决策会影响各种疾病患者的诊断、治疗和预后。

医疗专业人员可以分析有限数量的图像、测试和样本；临床决策仍然容易出现人为错误。利用人工智能，可以通过近乎实时地分析无限样本，以不同的信心做出临床决策。人工智能的医学和技术局限性似乎比潜在的道德和法律问题更容易克服。如果乳腺癌诊断算法错误地预测了乳腺癌，或者算法未能在眼部扫描中识别出糖尿病视网膜病变的迹象，谁应该受到指责？

在患者疾病的诊断中有三种责任可能性:

*   人类医生在没有外部代理的帮助下决定患者的诊断。当症状明显时，人类医生可能非常准确，并且通常一眼就能发现不太明显的问题。在这种情况下，责任总是在医生身上。

*   智能代理预测病人的诊断，准确率达 99%。病人很少会被误诊，但是像死亡这样的错误既不是代理人的责任，也不是人的责任。如果人工智能犯了这样的错误，病人会如何提出他们的问题？

*   一个人类医生由一个智能代理辅助。在这种情况下，可能更难确定责任，因为预测是共享的。即使人类的决定是最终的，也可以认为人工智能代理的无数经验将是影响决策的关键因素。

人工智能代理人的行为责任可理解为属于以下情况:

*   开发人工智能代理的组织。

*   设计人工智能代理的人类团队对任何意外的功能或不准确的预测负责。

*   人工智能代理本身对任何意外行为负责。

人工智能代理的开发通常是众多工程师和合作者的成果，因此让开发者承担责任不仅难以管理，而且可能会阻止潜在的工程师加入这个行业。让人工智能开发组织承担责任听起来是遵守道德的最佳途径。

我们如何确保人工智能系统不会推翻人类？我们的安全受到哪些规定的约束？Friedler 和 Diakopoulos 建议使用五个核心原则来定义组织问责制[141]:

*   可审计性:外部机构应该能够分析和探测算法行为。

*   准确性:确保使用良好的干净数据。应该使用评估指标的识别、准确性的定期跟踪和基准来计算和审核准确性。

*   可解释性:代理决策应该以一种可理解的方式向所有利益相关者解释。

*   公平:应该对代理人的歧视进行评估。

*   责任:应确定单一联系点来管理意外后果和意外输出。这类似于数据保护官，但专门针对 AI 伦理。

## 首次出现的问题

现实世界的数据已经表明，二型糖尿病的管理在过去 50 年中被误导了。当现实世界的经验表明传统治疗被误导时，谁应该承担责任？

基于患者数据开发的监督和非监督模型正在创造医疗保健领域的新概念，这些概念正在定义人工智能伦理。第一次出现问题是一种意想不到的后果。数据现在可以在人工智能中用于检测疾病的风险，如二型糖尿病、高血压和胰腺癌。如果一个应用程序能够告诉你一个你无能为力的绝症，你会想知道吗？你想知道有选择的余地吗？基因档案服务根据患者的基因档案提供疾病风险反馈。如果有人被告知他们患某些疾病的风险降低，他们可能会倾向于更危险的行为。如果患者被告知他们对肺癌的易感性低于平均水平，这是否会导致他们成为吸烟者或其他危险行为的风险更高？

类似地，知道患上特定疾病的风险会导致精神和情感上的后果。随着时间和证据的发展，了解疾病风险对人类心理和行为的影响将被发现。

与人工智能相关的首次问题的一个例子是错误信息和假新闻。假新闻内容被创造出来，然后混合使用机器人和目标来放大人们的确认偏见，这些内容通过分享和参与进一步传播。基于感知证据扭曲现实感知的操纵威胁非常令人担忧。另一个问题是深度伪造——使用人工神经网络将图像或视频替换成其他人的肖像。世界正在慢慢了解如何最好地对抗这些网络威胁。验证来自可信来源的内容的区块链技术可能被证明是部分修复[143]。然而，随着深度假货的增多，对数字内容的信任也在减少。

## 定义公平

人工智能正在改变我们与产品和服务互动和互动的方式。随着数据可访问性的增长，我们如何确保人工智能代理公正地对待人们？例如，推荐系统会极大地影响我们的体验。但是我们怎么知道它们是公平的呢？我们被给予的选择所偏见了吗？

如果没有明确的公平定义，机器学习模型就不可能是公平的。社会科学家和工程师之间的公平和协作有许多定义，人工智能研究人员需要确定对公平的明确认可。公平要求数据滥用应该产生公共、法律和道德后果。

## 机器如何影响我们的行为和互动？

人工智能的快速进化正促使人类评估它对人类意味着什么。

### 人类

随着人工智能机器人在模拟与人类的对话和互动方面变得越来越好，它们的普及程度也越来越高。Alexa、Siri 和 Google Assistant 等虚拟助手掌握在超过 1 亿人的手中，基于语音的电话代理也很常见[144]。代理不仅能够模仿会话语言，而且情感和反应建模比以往任何时候都更加复杂和可信。2015 年，一个名叫尤金的机器人赢得了图灵挑战，该挑战要求人类对他们与未知智能体的通信质量进行评级，并猜测它是人类还是 AI 智能体。尤金能够在超过一半的人类评分者面前伪装成人类[145]。

人类和机器人之间的模糊正在引发探索性的物种间关系。2017 年，中国人工智能工程师郑佳佳与机器人莹莹结婚，这是他为缓解婚姻压力而创造的机器人配偶[103]。他并不孤单。

替代人类的性爱机器人现在很常见，联网的人工智能机器人拥有类似人类的皮肤和温暖，以及减轻电击或更糟糕情况的安全措施。

无论是应用于银行、医疗保健、交通还是其他任何领域，人类都处于一个转折点的开端，人类与人工智能代理的接触频率就像他们也是人类一样。人类的注意力、善良、同情心和精力是有限的，而人工智能机器人是发展和维持关系的无限资源。

### 行为和成瘾

我们的行为已经受到科技的影响和操纵。Web 登录页面、共享链接和用户体验都通过多元测试进行了优化。这是一种捕捉人类注意力的简单算法。在美国大选中，唐纳德·特朗普(Donald Trump)的竞选团队以每天使用和测试超过 50，000 个广告变体来微目标选民而闻名[149]。人体实验已经进行了几个世纪，所以这不是什么新鲜事。然而，现在有了前所未有的影响选择和自由的能力。组织有道德责任分析人工智能的影响，并保护特定人群，包括脆弱的成年人和儿童，免受轻推或社交验证等操纵工具的影响。技术实验中的行为实验需要行为规范。

#### 技术成瘾

技术成瘾正成为一个越来越令人担忧的问题。南加州大学教授罗伯特·卢斯蒂格发现，人类大脑对技术的反应类似于对其他成瘾物质的反应[150]。网络成瘾在许多国家都有记录；青少年似乎面临更大的风险，部分原因是前额叶皮层是大脑中最后发育的部分。根据成瘾慈善行动，三分之一的人对某种东西上瘾[151]。网瘾被越来越多地探究。一名 38 岁男子在台北一家网吧打了 5 天电子游戏后被发现死亡[152]。害羞、孤独和逃避似乎在网瘾中起了一定的作用。

现实世界和学术证据表明，技术成瘾是一个日益严重的问题。人工智能有提高生产力和发现的潜力。然而，人类有责任采取措施，防止人工智能导致破坏性的数字成瘾，使我们更加孤立、过度依赖和昏昏欲睡。

### 经济和就业

乔布斯结束后会发生什么？超市已经使用自动结账，甚至麦当劳也已经用数字界面取代了人与人之间的点餐程序。劳动力等级主要与自动化有关，医疗保健也是如此。药房、装配线和登记都是常见的自动化系统。虽然这似乎是人类的一个担忧，但它可以促进人类更复杂的角色，使物种更多地从体力劳动转向认知劳动。COVID 证明，虽然数字技术是一种好的工具，但关键员工需要确保系统保持运转。

探索人工智能使用的医疗保健组织正在了解如何重新培训或重新部署他们的员工，而不是让他们成为多余的人。随着这些人工智能系统变得越来越普遍，容量和准确性也在增长，对话正在超越手工和认知劳动，转向放射学等专业将受到怎样的影响。例如，使用图像处理人工智能的医疗团队越来越多地面临如何处理专家团队的困境。

人工智能可用于重定向资源，以扩大患者护理，并有更多机会与患者及其治疗互动。人工智能可以被理解为一种高效和有效的工具，可以以人类不可想象的速度和效率分析和检测患者数据中的模式，几乎没有误解。然而，人工智能应该被视为增加人类的自然知识和智力，而不是让它做出最终决定。

医疗保健是独一无二的，因为只有数字的方法是不可想象的。似乎总是需要人为干预来降低风险和确认预测。即使是最复杂的人工智能，在一项健康调查的 1000 名受访者中，53%的人表示他们希望人类验证机器领导的诊断[155]。

## 影响未来

许多迹象表明，人们使用技术的方式正在改变儿童和成人的发展方式。在许多事情中，研究表明过多的屏幕时间与大脑的结构和功能变化有关，影响注意力、决策和认知控制[156]。

格拉斯哥大学对 390，089 人进行的另一项研究发现，长时间坐在数字显示器前与健康状况不佳有关联。此外，科学家表示，如果屏幕时间减少到 120 分钟或更少，患癌症和心血管疾病的风险就会降低[159]。屏幕消耗时间被认为是久坐行为。过多的屏幕时间已经被证明会损害大脑，特别是额叶，它被人类用于生活的各个方面。

未来几十年将展示互联技术对人类生理、心理、进化以及最终生存的影响。

## 扮演上帝

阿德莱德大学的工程师开发了一种人工智能代理，它能够预测你何时死亡。人工智能代理分析了 16，000 个图像特征，以了解患病器官的迹象[160]。基于神经网络的人工智能模型能够预测 5 年内的死亡率，准确率为 69%。数据访问、人工智能和预测分析的增强使人类能够预测更精确的发病率和死亡率风险，这重新引发了关于人工智能是扮演上帝还是使人类(通过数据驱动的方法)扮演上帝的辩论。

## 过度宣传和危言耸听

通过营销和简化的媒体表现，人工智能的能力在历史上被夸大了。媒体危言耸听，以及更多的公开数据泄露，也无助于人工智能的事业。

存在混乱和误解的迷雾，特别是在数据和输出的可能使用(和误用)方面，需要解决这些问题，以便利益相关者认真考虑将人工智能应用到医疗保健系统中。

## 利益相关方的认可和协调

确保所有利益相关者在一个项目可以提供的健康情报以及如何使用健康情报方面保持一致是至关重要的。例如，医疗保健应用的人工智能开发者的价值观可能并不总是与临床医生的价值观一致。例如，可能存在这样的诱惑，即引导系统采取临床行动来改善质量指标，但不一定改善患者护理，或者在潜在监管者审查时扭曲为公众评估提供的数据。

## 政策、法律和法规

未来十年将是人工智能伦理学的关键时刻。法律、道德准则和行业政策形式的监管将影响机器学习的方向。在没有这种政策的情况下，机器学习也会受到影响，而这种政策被认为存在很大的风险。由于未能设定可接受性的界限，我们有可能导致更广泛的意想不到的后果。

需要技术治理，特别是对于 AI 的应用。至关重要的是，让关键的意见领袖、利益相关者、立法者和破坏者参与制定和培育这些政策，以确保在没有不当限制的情况下实现机器学习的潜力。人工智能在非医疗保健领域的应用表明，当大规模部署时，算法学习存在潜在的伦理问题。

法律决策过程中的立法者和关键利益相关者必须理解人工智能的复杂性，并欢迎它带来的首次问题的挑战。此外，不能假定人工智能将仅用于慈善目的；人工智能已经在战争中使用，美国和中国花费巨大[161]。英国已经决定关注人工智能伦理[162]。

使用人工智能的医疗保健专业人员必须了解如何创建部署的人工智能算法，评估用于建模的数据，并了解代理如何防止错误。监管者必须准备好单独处理每个问题，因为所有的人工智能都是个体的，类似于人类。

只有通过由工程师、人道主义者和社会科学家组成的多样化、包容性和多学科团队，才能实现安全和包容性的人工智能开发。

组织和政府之间就大赦国际提出的关键议题进行国际接触是必须的。伦理、责任、就业、安全和进化等问题是对话的例子，这些对话需要公共部门、私营部门和学术部门的利益相关者在人工智能嵌入社会并确保人类民主化繁荣的过程中进行合作。

## 数据和信息治理

在医疗保健领域实施人工智能需要应对伦理挑战，例如不道德或作弊算法的可能性，用不完整或有偏见的数据训练的算法，对算法的局限性或程度缺乏了解，以及人工智能对医生和患者之间基本信托关系的影响。

数据治理策略是一组记录在案的指导原则，用于确保对组织的数据(数字数据或其他数据)进行适当的管理。这些指导方针可能涉及业务流程管理(BPM)和企业风险规划(ERP)的策略，以及安全性、数据质量和隐私。这不同于信息治理，信息治理是一组有文档记录的指导原则，用于确保组织信息的正确管理。信息治理政策将涵盖预测分析结果的使用。

道德行为守则或道德治理政策将涵盖组织的方向和意图，并引导治理。

### 有没有政策太多这种事？

可能会有太多的政策，阻碍进展和利益相关者的参与。人工智能推动了包括数字医疗在内的所有行业的创新。初创企业引领了人工智能的普及，因为与规模更大、通常对环境变化反应更慢的组织相比，它们的官僚作风和繁文缛节更少。审核您的政策，以确保倾听用户声音的空间，并将其纳入道德创新的循环中。

## 全球标准和模式

有几个机构共享共同的道德和责任准则，但没有全球强制执行的图表。一个预测你死亡风险的人工智能代理，除了你或它预测的其他人死亡之外，几乎没有其他验证。这样的代理应该如何验证？它必须坚持哪些标准？代理人，或者它的创造者，必须提供证据吗？到目前为止，人工智能还没有明确的行业标准来测试技术。因此，很难保证你所从事的人工智能的质量。在人工智能这样一个快节奏的行业中，人们的担忧是新奇而直接的。需要全球标准来设定对人工智能系统的最低期望。

医疗保健也存在从循证医学到临床接受的时滞。例如，患有二型糖尿病的人已经将他们的二型糖尿病置于缓解状态超过十年，但是卫生系统一直很慢，无法将患者登记为将他们的二型糖尿病置于缓解状态。因此，患者在历史上要么被贴上逆转了二型糖尿病病毒的标签，要么被贴上二型糖尿病病毒携带者的标签。如果患者被标记为已经逆转了他们的糖尿病，他们通常不会接受确认他们的二型糖尿病处于缓解状态的测试，并有可能不会接受检查。同样，如果病人被贴上了二型糖尿病的标签(实际上没有)，这将对保险产生影响，从技术上来说是不真实的。在模式上的合作和平行的采用步伐将有助于减少国际差异。

## 我们需要人道地对待人工智能吗？

强化的基本概念类似于人类和其他动物的学习方式。例如，当训练一只狗时，顺从和预期的表现通常会得到奖赏。强化学习系统的不服从会受到惩罚。这类似于人工智能代理的强化学习，我们建立奖励和惩罚机制。积极的表现会得到虚拟的奖励，而消极的行为会受到惩罚以增加逃避。

大多数人工智能系统目前都相当简单和简化。随着人工智能的发展，我们可以期待它们变得更加复杂和逼真。人类已经开始发展与机器人的关系，在许多情况下，机器人被视为另一个人的替代品，无论是为了友谊还是肉欲。可以认为对人工智能系统的惩罚输入是有害的或负面的输入。当遗传算法删除不再有用的世代时，这是一种谋杀吗？当人工智能系统可以反映人类的行为和外表时，我们需要在什么时候考虑人工智能的人道待遇是催化的？

如果人工智能系统被认为能够感知、感受和响应刺激，那么考虑它们作为一个物种、法律地位甚至国籍的地位并不是一个巨大的飞跃。一个美国制造的人工智能代理甚至被授予沙特阿拉伯公民身份[163]。

是否应该把 AI 当成一种智力堪比人类的动物？机器能真正感受到痛苦吗？我们如何对待人工智能代理人以及减轻痛苦和负面结果风险的道德问题，是负责任地推动人工智能朝着改善人类生活的巨大潜力发展的基础。

## 在您的组织内使用数据和人工智能道德

许多组织都有行为准则，作为对预期行为、要求和参与的内部沟通，并为外部利益相关者提供信心和保证。

行为准则应与员工共享，并进行适当的培训，以确保员工理解并遵守行为准则。组织的所有利益相关者都应该实践并促进行为准则的遵守。

### 道德规范

道德规范或道德准则是用于管理一个组织的道德行为的文件。道德准则表明一个组织致力于负责任的商业和技术进步。

道德规范描述了组织提倡的行为，以及那些被认为对组织自身的道德准则、声誉或客户有害的行为。它可能不包括非法行为，但通常会说明不遵守行为的后果以及如何报告此类违规行为。应该让员工意识到对他们来说不明显的项目，然后帮助他们避免无意的但可能有害的行为。道德准则还应包括使用数据的动机及其在组织目标中的目的的总结，并反映企业的盈利能力、诚信和声誉。

道德准则应该没有技术和哲学术语，直接传达对员工的期望。保持简单，直奔主题。在新员工加入时设定对他们的期望，并培养一种坚持的组织文化。确保所有利益相关者了解修订和审查。不要只参考招聘人员或客户的行为准则，要经常参考道德准则，在内部和外部利益相关者中灌输你的组织的道德方向。在检查将新技术纳入决策过程的道德风险时，请参考《道德准则》。

从头开始制定道德行为准则时，咨询员工和利益相关者的意见。需要考虑的问题包括:

*   AI 伦理对你意味着什么？

*   我们所做的如何能改善人类？

*   我们的组织应如何负责任地实现其[在此插入目标]的目标？

*   我们希望实现的目标的潜在好处是什么？

*   我们希望实现的目标的潜在缺点是什么？

*   我们如何改善我们的道德规范？

*   《道德准则》中是否有令人困惑或需要更多解释的条款？

*   道德准则对决策有用吗？

*   组织的道德规范与你自己的道德观点一致吗？

一旦组织的不同部门回答了上述所有问题，就可以就实施计划达成共识。组织道德守则是对不符合标准者采取纪律行动的参照点。道德准则为识别和应对道德挑战提供了坚实的基础。

### 道德框架考虑

组织有责任在收集和使用患者数据时接受道德指导。组织需要一种数据隐私方法来防止违规并加强安全性。不遵守法规可能会导致罚款、声誉受损和客户流失。有多种方法可以降低不当数据收集的风险，同时利用大数据带来的机遇。有几种技术可以保护道德数据收集。

#### 收集最少量的数据

保护用户数据的第一步是只收集手头任务所需的数据。更多的数据并不总是意味着更有用的数据。虽然机器学习通常欢迎更多的数据，但数据收集必须保持谨慎和简洁。例如，如果一个人正在申请信用卡，知道他的种族和身体质量指数就没有什么用了；但是，在确定一个人患二型糖尿病病的风险时，同样的数据也很重要。

#### 识别和清除敏感数据

所有敏感数据都应得到识别和保护。数据科学家应该知道哪些数据被视为个人数据，以及如何使用这些信息。例如，在未经消费者同意的情况下收集的消费者数据应该清除掉个人身份数据。最重要的是，只有拥有适当权限的个人才能观察敏感数据。许多组织也不鼓励使用 u 盘和外部数据驱动器，以确保数据在现场保持安全。

#### 遵守适用的法律法规

通过遵守适当的地方和国家标准、政策和法律，可以获得对您组织的数据和信息管理方法的信任。有几项数据和信息治理法律法规设定了合法和适当数据使用的界限。遵守所有的规章制度是至关重要的。

美国 FDA(美国美国食品药品监督管理局)和欧盟 MHRA(欧盟药品和医疗保健产品监管机构)等机构为数据的合法、道德和合规性收集、使用和管理设定了界限。符合 GDPR 标准并获得监管相关认证机构的批准，表明您的组织对数据使用的道德态度，并与您的用户建立信任。

#### 国际标准

许多国家级和国际级标准会有一定程度的重叠，需要适当的数据报告、文档、透明度和风险缓解程序。例如，在欧洲，遵守 GDPR 等法规是必要的，欧盟有超过 27 个国家/地区都有此类法规。ISO 27001 等国际标准确保良好的数据和信息管理标准。ISMS ISO 27001(正式名称为 ISO/IEC 27001:2005)是信息安全管理系统的规范。ISMS 是一个政策和程序框架，包括组织信息风险管理流程中涉及的所有法律、物理和技术控制措施。

许多数据法规是自我认可的。这意味着，作为一个组织，您必须收集证明遵守标准所需的文档，而不需要由外部机构进行验证。自我认证的问题是显而易见的，尤其是在医疗保健领域。

强大的数据和信息治理是数据伦理的基础。

#### 创建数据伦理

彻底的数据和信息治理是数据伦理的基础。治理涉及如何在数据和信息的收集、使用、分析和处置过程中保护数据和信息。良好的治理涵盖各种主题，包括数据架构、基础设施、风险评估、审计报告、风险处理计划、设计工作流程、信息安全政策、投诉程序、医疗治理、企业责任和灾难恢复规划。一个组织的道德准则指引着良好治理的方向。

#### 审核您的框架

对道德数据和信息治理程序进行定期审核，并预演最坏的情况，以确保在最坏的情况发生时，您的组织能够在尽可能短的时间内将风险降至最低。风险缓解演习应至少每年演练两次。

回顾行为准则的每一部分，确保它代表了您组织的价值观。技术组织发展迅速，因此请确保符合与您所在行业相关的最新标准、准则和政策。处理医疗数据的组织可能会提出新的预测方法或新的人工智能能力。考虑您的行为准则中缺失的方面，尤其是当您的组织正在扩展或面临新的用例时。例如，一个历史上以业务为导向的组织开始与患者接触，将需要一个与患者接触的新部分。

没有理由不让每一位员工参与制定和审查你的行为准则。数字调查是收集意见的快捷方式。多样化的贡献造就了多样化的、包容的、多学科的人工智能方法。就你认为员工应该了解的新章节或概念征求意见。对话可以突出员工可能需要进一步培训、认知或示范的领域。与员工开展对话，在更广泛的团队中植入组织文化，并营造一个有道德意识的环境。让尽可能多的利益相关者参与到组织的行为准则中，可能会提高员工的积极性和生产力。

医疗保健组织中的道德框架，特别是对于那些使用人工智能的组织，为内部和外部利益相关者提供了最大化参与所需的安全性、透明度、信任和方向。坚持行业标准，让组织成员参与制定和维护道德框架，将会加快员工和组织的采用。此外，良好的策略文档、培训和审计有助于维护组织 AI 的完整性。

### 数据科学家的希波克拉底誓言

有权访问敏感数据的数据科学家通常需要签署文档，以确保他们在开始工作时遵守风险缓解和安全政策。有人建议，为了在人工智能中根植一种安全和积极进步的文化，数字和非数字部门的数据科学家必须宣誓效忠希波克拉底誓言，不做任何伤害。这将与规章制度、内部宣言、标准和组织运作的行为准则并列[164]。

## 结论

医疗保健的有意义的变化将来自创新、伦理和深思熟虑的正确结合。随着发现的边界受到考验，为患者提供精确的循证医学和护理的潜力正在实现；因此，我们有义务探索和预测创新的社会、经济、政治和伦理后果。

大数据和人工智能在医疗保健中的采用最好作为一个渐进的发展来管理，这将有助于利益相关者有时间绘制和理解其实施的可能后果。审计和理解人工智能模型需要稳健的标准，以维护质量保证和信任。

人工智能标准的发展需要一个多元化的公共论坛，该论坛在学术上是强大的，并根据最新的证据基础定期进行审查。

需要医疗专业人员和患者群体的认识。医疗保健专业人员需要人工智能应用程序如何在各自的医疗环境中发挥作用的基本知识，以了解人工智能如何在日常工作中促进和帮助他们的优势、劣势和后果。

患者必须习惯于与人工智能系统打交道，并探索其对自己的好处，新冠肺炎无疑加快了这一过程。人工智能在医疗保健中如何优化效率、优化资源和改善人口健康结果的可证明的例子只会加强公众对人工智能的认知和采用。

从事人工智能工作的组织需要就在医学中使用人工智能的潜在优势和风险与公众进行交流和互动。开发人工智能的组织必须与学术界合作，采取必要的步骤，以便能够独立地衡量人工智能系统的成功和有效性。鼓励负担得起的人工智能解决方案以使人工智能成为 21 世纪的听诊器是至关重要的；并且公布的有效证据、指南和文献对 AI 的持续信任、采用和道德是必不可少的。