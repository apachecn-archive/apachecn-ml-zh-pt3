# 1.介绍

在本书中，我假设您熟悉 Python 编程。在这一介绍性章节中，我解释了为什么数据科学家应该选择 Python 作为编程语言。然后我强调了 Python 不是一个好选择的一些情况。最后，我描述了应用程序开发中的一些良好实践，并给出了数据科学家在日常工作中需要的一些编码示例。

## 为什么是 Python？

那么，为什么要选择 Python 呢？

*   它有多功能的图书馆。对于任何类型的应用程序，Python 中都有现成的库。从统计编程到深度学习到网络应用到网页抓取再到嵌入式系统，Python 中总会有一个现成的库。如果你学习这种语言，你不必坚持特定的用例。r 有一套丰富的分析库，但如果你在做一个物联网(IoT)应用，需要在设备端的嵌入式系统中编码，在 r 中会比较困难。
*   这是非常高的性能。Java 也是一种通用语言，有很多库，但是 Java 代码运行在 Java 虚拟机上，这增加了额外的延迟。Python 使用其他语言构建的高性能库。例如，SciPy 使用 LAPACK，这是一个用于线性代数应用程序的 Fortran 库。TensorFlow 用的是 CUDA，是并行 GPU 处理的 C 库。
*   它很简单，给你很大的编码自由。Python 语法就像一种自然语言。它很容易记忆，并且在变量中没有约束(像常量或者`public` / `private`)。

## 何时避免使用 Python

Python 也有一些缺点。

*   当你编写非常具体的代码时，Python 可能并不总是最好的选择。例如，如果您正在编写只处理统计数据的代码，R 是更好的选择。如果你只编写 MapReduce 代码，Java 是比 Python 更好的选择。
*   Python 在编码方面给了你很大的自由度。因此，当许多开发人员正在开发一个大型应用程序时，Java/C++是一个更好的选择，这样一个开发人员/架构师可以使用`public` / `private`和常量关键字对另一个开发人员的代码进行约束。
*   对于极高性能的应用程序，C/C++是无可替代的。

## Python 中的 OOP

在继续之前，我将解释 Python 环境中面向对象编程(OOP)的一些特性。

任何现代应用程序的最基本元素都是对象。对于程序员或架构师来说，世界是一个对象的集合。对象由两种类型的成员组成:属性和方法。成员可以是私有的、公共的或受保护的。类是对象的数据类型。每个对象都是一个类的实例。一个类可以在子类中继承。可以使用组合将两个类关联起来。

在 Python 上下文中，Python 没有 public、private 或 protected 关键字，因此封装(对外界隐藏成员)在 Python 中并不隐含。和 C++一样，它支持多级和多重继承。和 Java 一样，它有一个`abstract`关键字。类和方法都可以是抽象的。

以下代码是一个通用网络爬虫的示例，它在 Skytrax 站点上被实现为航空公司的网络爬虫，在 Mouthshut.com 站点上被实现为零售爬虫。我将在第 2 章回到网络爬行的话题。

```py
from abc import ABCMeta, abstractmethod
import BeautifulSoup
import urllib
import sys
import bleach
#################### Root Class (Abstract) ####################
class SkyThoughtCollector(object):
       __metaclass__ = ABCMeta

       baseURLString = "base_url"
       airlinesString = "air_lines"
       limitString = "limits"

       baseURl = ""
       airlines = []
       limit = 10

       @abstractmethod

       def collectThoughts(self):
             print "Something Wrong!! You're calling an abstract method"

       @classmethod
       def getConfig(self, configpath):
             #print "In get Config"
             config = {}
             conf = open(configpath)
             for line in conf:
                    if ("#" not in line):
                          words = line.strip().split('=')
                          config[words[0].strip()] = words[1].strip()
             #print config

             self.baseURl = config[self.baseURLString]
             if config.has_key(self.airlinesString):
                    self.airlines = config[self.airlinesString].split(',')
             if config.has_key(self.limitString):
                    self.limit = int(config[self.limitString])
             #print self.airlines

       def downloadURL(self, url):
             #print "downloading url"
             pageFile = urllib.urlopen(url)
             if pageFile.getcode() != 200:
                    return "Problem in URL"
             pageHtml = pageFile.read()
             pageFile.close()
             return "".join(pageHtml)

       def remove_junk(self, arg):
             f = open('junk.txt')
             for line in f:
                    arg.replace(line.strip(),'')
             return arg

       def print_args(self, args):
             out =''
             last = 0
             for arg in args:
                    if args.index(arg) == len(args) -1:
                          last = 1
                    reload(sys)
                    sys.setdefaultencoding("utf-8")
                    arg = arg.decode('utf8','ignore').encode('ascii','ignore').strip()
                    arg = arg.replace('\n',' ')
                    arg = arg.replace('\r','')
                    arg = self.remove_junk(arg)
                    if last == 0:
                          out = out + arg + '\t'
                    else:
                          out = out + arg
             print out

####################### Airlines Chield #######################

class AirLineReviewCollector(SkyThoughtCollector):

      months = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December' ]

       def __init__(self, configpath):
             #print "In Config"
       super(AirLineReviewCollector,self).getConfig(configpath

)

       def parseSoupHeader(self, header):
             #print "parsing header"
             name = surname = year = month = date = country =''
             txt = header.find("h9")
             words = str(txt).strip().split(' ')
             for j in range(len(words)-1):
                    if words[j] in self.months:
                          date = words[j-1]
                          month= words[j]
                          year = words[j+1]
                          name = words[j+3]
                          surname = words[j+4]
             if ")" in words[-1]:
                    country = words[-1].split(')')[0]
             if "(" in country:
                    country = country.split('(')[1]
             else:
                    country = words[-2].split('(')[1] + country
             return (name, surname, year, month, date, country)

       def parseSoupTable(self, table):
             #print "parsing table"
             images = table.findAll("img")
             over_all = str(images[0]).split("grn_bar_")[1].split(".gif")[0]
             money_value = str(images[1]).split("SCORE_")[1].split(".gif")[0]
             seat_comfort = str(images[2]).split("SCORE_")[1].split(".gif")[0]
             staff_service = str(images[3]).split("SCORE_")[1].split(".gif")[0]
             catering = str(images[4]).split("SCORE_")[1].split(".gif")[0]
             entertainment = str(images[4]).split("SCORE_")[1].split(".gif")[0]
             if 'YES' in str(images[6]):
                    recommend = 'YES'
             else

:
                    recommend = 'NO'
             status = table.findAll("p", {"class":"text25"})
             stat = str(status[2]).split(">")[1].split("<")[0]
             return (stat, over_all, money_value, seat_comfort, staff_service, catering, entertainment, recomend)

       def collectThoughts(self):
             #print "Collecting Thoughts"
             for al in AirLineReviewCollector.airlines:
                    count = 0
                    while count < AirLineReviewCollector.limit:
                          count = count + 1
                          url = ''
                          if count == 1:
                                 url = AirLineReviewCollector.baseURl + al + ".htm"
                          else:
                                 url = AirLineReviewCollector.baseURl + al + "_"+str(count)+".htm"
                          soup = BeautifulSoup.BeautifulSoup(super(AirLineReviewCollector,self).downloadURL(url))
                          blogs = soup.findAll("p", {"class":"text2"})
                          tables = soup.findAll("table", {"width":"192"})
                          review_headers = soup.findAll("td", {"class":"airport"})
                          for i in range(len(tables)-1):
                                 (name, surname, year, month, date, country) = self.parseSoupHeader(review_headers[i])
                                 (stat, over_all, money_value, seat_comfort, staff_service, catering, entertainment, recomend
) = self.parseSoupTable(tables[i])
                                 blog = str(blogs[i]).split(">")[1].split("<")[0]
                                 args = [al, name, surname, year, month, date, country, stat, over_all, money_value, seat_comfort, staff_service, catering, entertainment, recomend

, blog]
                                        super(AirLineReviewCollector,self).print_args(args)

######################## Retail Chield ########################

class RetailReviewCollector(SkyThoughtCollector):
       def __init__(self, configpath):
             #print "In Config"
       super(RetailReviewCollector,self).getConfig(configpath)

       def collectThoughts(self):
             soup = BeautifulSoup.BeautifulSoup(super(RetailReviewCollector,self).downloadURL(RetailReviewCollector.baseURl))
             lines = soup.findAll("a",{"style":"font-size:15px;"})
             links = []
             for line in lines:
                    if ("review" in str(line)) & ("target" in str(line)):
                          ln = str(line)
                          link = ln.split("href=")[-1].split("target=")[0].replace("\"","").strip()
                          links.append(link)

             for link in links

:
                    soup = BeautifulSoup.BeautifulSoup(super(RetailReviewCollector,self).downloadURL(link))
                    comment = bleach.clean(str(soup.findAll("div",{"itemprop":"description"})[0]),tags=[], strip=True)
                    tables = soup.findAll("table", {"class":"smallfont space0 pad2"})
                    parking = ambience = range = economy = product = 0
                    for table in tables:
                          if "Parking:" in str(table):
                                 rows = table.findAll("tbody")[0].findAll("tr")
                                 for row in rows:
                                       if "Parking:" in str(row):
                                              parking = str(row).count("read-barfull")
                                       if "Ambience" in str(row):
                                              ambience = str(row).count("read-barfull")
                                       if "Store" in str(row):
                                              range = str(row).count("read-barfull")
                                       if "Value" in str(row):
                                              economy = str(row).count("read-barfull")
                                       if "Product" in str(row):
                                              product = str(row).count

("smallratefull")

                    author = bleach.clean(soup.findAll("span",{"itemprop":"author"})[0], tags=[], strip=True)
                    date = soup.findAll("meta",{"itemprop":"datePublished"})[0]["content"]
                    args = [date, author,str(parking), str(ambience),str(range), str(economy), str(product), comment]
                                        super(RetailReviewCollector,self).print_args(args)

######################## Main Function ########################
if __name__ == "__main__":
       if sys.argv[1] == 'airline':
             instance = AirLineReviewCollector(sys.argv[2])
             instance.collectThoughts()
       else:
             if sys.argv[1] == 'retail':
                    instance = RetailReviewCollector(sys.argv[2])
                    instance.collectThoughts()
             else:
                    print "Usage is"
                    print sys.argv[0], '<airline/retail>', "<Config File Path>"

```

上面代码的配置如下所示:

```py
base_url = http://www.airlinequality.com/Forum/
#base_url = http://www.mouthshut.com/product-reviews/Mega-Mart-Bangalore-reviews-925103466
#base_url = http://www.mouthshut.com/product-reviews/Megamart-Chennai-reviews-925104102
air_lines = emrts,brit_awys,ual,biman,flydubai
limits = 10

```

我现在将简要讨论前面的代码。它有一个抽象的根类。它包含基本属性，如基本 URL 和页面限制；这些对于所有子类都是必不可少的。它还包含类方法函数中的公共逻辑，如下载 URL、打印输出和读取配置。它还有一个抽象方法 collectThoughts，必须在子类中实现。这个抽象方法向每个子类传递了一个共同的行为，即所有子类都必须从网络上收集想法。这个思想集合的实现是特定于孩子的。

## 用 Python 调用其他语言

现在我将描述如何在 Python 中使用其他语言的代码。这里有两个例子:一个是从 Python 调用 R 代码。有些用例需要 r 代码。例如，如果您想要一个现成的函数用于时间序列中的 Holt-Winter 方法，这在 Python 中很难做到。但是它在 R 中是可用的。因此，您可以使用`rpy2`模块从 Python 中调用 R 代码，如下所示:

```py
import rpy2.robjects as ro
ro.r('data(input)')
ro.r('x <-HoltWinters(input)')

```

有时候需要从 Python 调用 Java 代码。比如说你正在研究自然语言处理(NLP)领域的一个命名实体识别问题；一些文本作为输入给出，你必须识别文本中的名字。Python 的 NLTK 包确实有名称实体识别功能，但是准确率不好。斯坦福 NLP 在这里是比较好的选择，是用 Java 写的。你可以用两种方法解决这个问题。

*   您可以使用 Python 代码在命令行调用 Java。

    ```py
    import subprocess

    subprocess.call(['java','-cp','*','edu.stanford.nlp.sentiment.SentimentPipeline','-file','foo.txt'])

    ```

*   你可以将斯坦福 NLP 公开为 web 服务，并将其称为服务。

    ```py
    nlp = StanfordCoreNLP('http://127.0.0.1:9000')
    output = nlp.annotate(sentence, properties={
     "annotators": "tokenize,ssplit,parse,sentiment",
     "outputFormat": "json",
     # Only split the sentence at End Of Line. We assume that this method only takes in one single sentence.
     "ssplit.eolonly": "true",
     # Setting enforceRequirements to skip some annotators and make the process faster
     "enforceRequirements

    ": "false"
     })

    ```

## 将 Python 模型公开为微服务

您可以将 Python 模型公开为微服务，就像其他人可以使用您的 Python 模型来编写自己的代码一样。最好的方法是将您的模型公开为 web 服务。例如，以下代码使用 Flask 公开了深度学习模型:

```py
from flask import Flask, request, g
from flask_cors import CORS
import tensorflow as tf
from sqlalchemy import *
from sqlalchemy.orm import sessionmaker
import pygeoip
from pymongo import MongoClient
import json
import datetime as dt
import ipaddress
import math

app = Flask(__name__)
CORS(app)

@app.before_request
def before():
       db = create_engine('sqlite:///score.db')
       metadata = MetaData(db)
       g.scores = Table('scores', metadata, autoload=True)
       Session = sessionmaker(bind=db)
       g.session = Session()

       client = MongoClient()
       g.db = client.frequency

       g.gi = pygeoip.GeoIP('GeoIP.dat')

       sess = tf.Session()
       new_saver = tf.train.import_meta_graph('model.obj.meta')
       new_saver.restore(sess, tf.train.latest_checkpoint('./'))
       all_vars = tf.get_collection('vars')

       g.dropped_features = str(sess.run(all_vars[0]))
       g.b = sess.run(all_vars[1])[0]
       return

def get_hour(timestamp

):
       return dt.datetime.utcfromtimestamp(timestamp / 1e3).hour

def get_value(session, scores, feature_name, feature_value):
       s = scores.select((scores.c.feature_name == feature_name) & (scores.c.feature_value == feature_value))
       rs = s.execute()
       row = rs.fetchone()
       if row is not None:
             return float(row['score'])
       else:
             return 0.0

@app.route('/predict', methods=['POST'])
def predict():
       input_json = request.get_json(force=True)

       features = ['size','domain','client_time','device','ad_position','client_size', 'ip','root']
       predicted = 0
       feature_value = ''
       for f in features:
             if f not in g.dropped_features:
                    if f == 'ip':
                          feature_value = str(ipaddress.IPv4Address(ipaddress.ip_address(unicode(request.remote_addr

))))
                    else:
                          feature_value = input_json.get(f)
                    if f == 'ip':
                          if 'geo' not in g.dropped_features:
                                 geo = g.gi.country_name_by_addr(feature_value)
                                 predicted = predicted + get_value(g.session, g.scores, 'geo', geo)
                          if 'frequency' not in g.dropped_features:
                                 res = g.db.frequency.find_one({"ip" : feature_value})
                                 freq = 1
                                 if res is not None:
                                       freq = res['frequency']
                                 predicted = predicted + get_value(g.session, g.scores, 'frequency', str(freq))
                    if f == 'client_time':
                           feature_value = get_hour(int(feature_value))
                    predicted = predicted + get_value(g.session, g.scores, f, feature_value)
       return str(math.exp(predicted + g.b)-1)
app.run(debug = True, host ='0.0.0.0')

```

这段代码将深度学习模型公开为 Flask web 服务。JavaScript 客户机将发送带有 web 用户参数(如 IP 地址、广告大小、广告位置等)的请求，并返回广告价格作为响应。这些特征是绝对的。你将在第 [3](3.html) 章学习如何将它们转换成数字分数。这些分数存储在内存数据库中。该服务从数据库中获取分数，对结果求和，然后回复给客户端。该分数将在深度学习模型的每次迭代训练中实时更新。它使用 MongoDB 来存储该站点中该 IP 地址的频率。这是一个重要的参数，因为第一次访问网站的用户实际上是在搜索一些东西，而对于频率大于 5 的用户来说却不是这样。IP 地址的数量是巨大的，所以它们被存储在一个分布式的 MongoDB 数据库中。

## 高性能 API 和并发编程

当您正在构建一个通用的解决方案，同时也是一个图形用户界面(GUI)时，Flask 是一个不错的选择。但是如果高性能是你的应用最关键的需求，那么 Falcon 就是最好的选择。下面的代码是前面 Falcon 框架公开的相同模型的示例。我在这段代码中做的另一个改进是我实现了多线程，所以代码将被并行执行。除了 Falcon 特有的变化，您应该注意到使用线程池类并行化调用`get_score`函数的主要变化。

```py
import falcon
from falcon_cors import CORS
import json
from sqlalchemy import *
from sqlalchemy.orm import sessionmaker
import pygeoip
from pymongo import MongoClient
import json
import datetime as dt
import ipaddress
import math
from concurrent.futures import *
from sqlalchemy.engine import Engine
from sqlalchemy import event

import sqlite3

@event.listens_for(Engine, "connect")
def set_sqlite_pragma(dbapi_connection, connection_record):
 cursor = dbapi_connection.cursor()
 cursor.execute("PRAGMA cache_size=100000")
 cursor.close()

class Predictor(object):
       def __init__(self,domain):
             db1 = create_engine('sqlite:///score_' + domain + '0test.db')
             db2 = create_engine('sqlite:///probability_' + domain +'0test.db')
             db3 = create_engine('sqlite:///ctr_'+ domain + 'test.db')
        metadata1 = MetaData(db1)
        metadata2 = MetaData(db2)
        metadata3 = MetaData(db3)
        self.scores = Table('scores', metadata1, autoload=True)
        self.probabilities = Table('probabilities', metadata2, autoload=True)
        self.ctr = Table('ctr', metadata3, autoload=True)

       client = MongoClient(connect=False,maxPoolSize=1)
       self.db = client.frequency

       self.gi = pygeoip.GeoIP('GeoIP.dat')

             self.high = 1.2
             self.low = .8

       def get_hour(self,timestamp):
       return dt.datetime.utcfromtimestamp(timestamp / 1e3).hour

       def get_score(self, featurename, featurevalue):
             prob = 0
             pred = 0
             s = self.scores.select((self.scores.c.feature_name == featurename) & (self.scores.c.feature_value == featurevalue))
 rs = s.execute()
 row = rs.fetchone()
 if row is not None:
      pred = pred + float(row['score'])
 s = self.probabilities.select((self.probabilities.c.feature_name == featurename) & (self.probabilities.c.feature_value == featurevalue))
 rs = s.execute()
 row = rs.fetchone()
 if row is not None:
      prob = prob + float(row['Probability'])
             return pred, prob

      def get_value(self, f, value):
             if f == 'ip':
                    ip = str(ipaddress.IPv4Address(ipaddress.ip_address(value)))
 geo = self.gi.country_name_by_addr(ip)
                    pred1, prob1 = self.get_score('geo', geo)
       res = self.db.frequency.find_one({"ip" : ip})
       freq = 1
 if res is not None:
       freq = res['frequency']
                    pred2, prob2 = self.get_score('frequency', str(freq))
                    return (pred1 + pred2), (prob1 + prob2

)
             if f == 'root':
                    s = self.ctr.select(self.ctr.c.root == value)
                    rs = s.execute()
       row = rs.fetchone()
                    if row is not None:
                          ctr = row['ctr']
                          avv = row['avt']
                          avt = row['avv']
                          (pred1,prob1) = self.get_score('ctr', ctr)
                          (pred2,prob2) = self.get_score('avt', avt)
                          (pred3,prob3) = self.get_score('avv', avv)
                          (pred4,prob4) = self.get_score(f, value)
                          return (pred1 + pred2 + pred3 + pred4), (prob1 + prob2 + prob3 + prob4)
       if f == 'client_time':
             value = str(self.get_hour(int(value)))
             if f == 'domain':
                    conn = sqlite3.connect('multiplier.db')
                    cursor = conn.execute("SELECT high,low from multiplier where domain='" + value + "'")
                    row = cursor.fetchone()
                    if row is not None:
                          self.high = row[0]
                          self.low = row[1]
             return self.get_score(f, value)

       def on_post(self, req, resp

):
             input_json = json.loads(req.stream.read(),encoding='utf-8')
             input_json['ip'] = unicode(req.remote_addr)
       pred = 1
             prob = 1

             with ThreadPoolExecutor(max_workers=8) as pool:
                    future_array = { pool.submit(self.get_value,f,input_json[f]) : f for f in input_json}
                    for future in as_completed(future_array):
       pred1, prob1 = future.result()
                          pred = pred + pred1
                          prob = prob - prob1

             resp.status = falcon.HTTP_200

       res = math.exp(pred)-1
             if res < 0:
                   res = 0
             prob = math.exp(prob)
             if(prob <= .1):
                   prob = .1
             if(prob >= .9):
                   prob = .9
             multiplier = self.low + (self.high -self.low)*prob
             pred = multiplier*pred
             resp.body = str(pred)

cors = CORS(allow_all_origins=True,allow_all_methods=True,allow_all_headers=True

)
wsgi_app = api = falcon.API(middleware=[cors.middleware])

f = open('publishers1.list')
for domain in f:
       domain = domain.strip()
       p = Predictor(domain)
       url = '/predict/' + domain
       api.add_route(url, p)

```