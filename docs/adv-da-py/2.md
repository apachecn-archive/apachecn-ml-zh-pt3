# 二、使用 Python 的 ETL

每个数据科学专业人员都必须从不同的数据源提取、转换和加载(ETL)数据。在这一章中，我将讨论如何用 Python 对一些流行的数据库进行 ETL。对于关系数据库，我将介绍 MySQL。作为一个文档数据库的例子，我将介绍 Elasticsearch。对于图形数据库，我将介绍 Neo4j，对于 NoSQL，我将介绍 MongoDB。我还将讨论 Pandas 框架，它的灵感来自 R 的数据框架概念。

## 关系型数据库

MySQLdb 是 Python 中的一个 API，开发于 MySQL C 接口的顶层。

### 如何安装 MySQLdb？

首先，您需要在机器上安装 Python MySQLdb 模块。然后运行以下脚本:

```py
#!/usr/bin/python
import MySQLdb

```

如果您得到一个导入错误异常，这意味着模块没有正确安装。

以下是安装 MySQL Python 模块的说明:

```py
$ gunzip MySQL-python-1.2.2.tar.gz
$ tar -xvf MySQL-python-1.2.2.tar
$ cd MySQL-python-1.2.2
$ python setup.py build
$ python setup.py install

```

### 数据库连接

在连接到 MySQL 数据库之前，请确保您具备以下条件:

*   你需要一个名为`TEST`的数据库。
*   在`TEST`中你需要一张桌子`STUDENT`。
*   `STUDENT`需要三个字段:`NAME`、`SUR_NAME`和`ROLL_NO`。
*   在`TEST`中需要有一个对数据库有完全访问权的用户。

### 插入操作

以下代码执行 SQL `INSERT`语句，以便在`STUDENT`表中创建记录:

```py
#!/usr/bin/python

import MySQLdb

# Open database connection
db = MySQLdb.connect("localhost","user","passwd","TEST" )

# prepare a cursor object using cursor() method
cursor = db.cursor()

# Prepare SQL query to INSERT a record into the database.
sql = """INSERT INTO STUDENT(NAME,
         SUR_NAME, ROLL_NO)
         VALUES ('Sayan', 'Mukhopadhyay', 1)"""
try:
   # Execute the SQL command

cursor.execute(sql)
   # Commit your changes in the database
   db.commit()
except:
   # Rollback in case there is any error
   db.rollback()

# disconnect from server
db.close()

```

### 读取操作

以下代码从`STUDENT`表中获取数据并打印出来:

```py
#!/usr/bin/python

import MySQLdb

# Open database connection
db = MySQLdb.connect("localhost","user","passwd","TEST" )

# prepare a cursor object using cursor() method
cursor = db.cursor()

# Prepare SQL query to INSERT a record into the database.
sql = "SELECT * FROM STUDENT "
try:
   # Execute the SQL command

cursor.execute(sql)
   # Fetch all the rows in a list of lists.
results = cursor.fetchall()
for row in results:
fname = row[0]
lname = row[1]
id = row[2]
      # Now print fetched result
print "name=%s,surname=%s,id=%d" % \
             (fname, lname, id )
except:
print "Error: unable to fecth data"

# disconnect from server
db.close()

```

### 删除操作

以下代码从`TEST`中删除带有`id=1`的一行:

```py
#!/usr/bin/python

import MySQLdb

# Open database connection
db = MySQLdb.connect("localhost","test","passwd","TEST" )

# prepare a cursor object using cursor() method
cursor = db.cursor()

# Prepare SQL query to DELETE required records
sql = "DELETE FROM STUDENT WHERE ROLL_NO =1"
try:
   # Execute the SQL command

cursor.execute(sql)
   # Commit your changes in the database
db.commit()
except:
   # Rollback in case there is any error
db.rollback()

# disconnect from server
db.close()

```

### 更新操作

以下代码将变量`lastname`从 Mukhopadhyay 更改为 Mukherjee:

```py
#!/usr/bin/python

import MySQLdb

# Open database connection
db = MySQLdb.connect("localhost","user","passwd","TEST" )

# prepare a cursor object using cursor() method
cursor = db.cursor()

# Prepare SQL query to UPDATE required records
sql = "UPDATE STUDENT SET SUR_NAME="Mukherjee"
                          WHERE SUR_NAME="Mukhopadhyay"
try:
   # Execute the SQL command
cursor.execute(sql)
   # Commit your changes in the database
db.commit()
except:
   # Rollback in case there is any error
db.rollback()

# disconnect from server

db.close()

```

### 提交操作

提交操作向数据库提供其同意以完成修改，并且在该操作之后，这是无法恢复的。

### 回滚操作

如果你对任何修改都没有完全信服，并且想要撤销它们，那么你需要应用`roll-back()`方法。

下面是一个通过 Python 访问 MySQL 数据的完整例子。它将给出 CSV 文件或 MySQL 数据库中存储的数据的完整描述。

```py
import MySQLdb
import sys

out = open('Config1.txt','w')
print "Enter the Data Source Type:"
print "1\. MySql"
print "2\. Text"
print "3\. Exit"

while(1):
       data1 = sys.stdin.readline().strip()
       if(int(data1) == 1):
             out.write("source begin"+"\n"+"type=mysql\n")
             print "Enter the ip:"
             ip = sys.stdin.readline().strip()
             out.write("host=" + ip + "\n")
             print "Enter the database name:"
             db = sys.stdin.readline().strip()
             out.write("database=" + db + "\n")
             print "Enter the user name:"
             usr = sys.stdin.readline().strip()
             out.write("user=" + usr + "\n")
             print "Enter the password:"
             passwd = sys.stdin.readline().strip()
             out.write("password=" + passwd + "\n")
             connection = MySQLdb.connect(ip, usr, passwd, db)
             cursor = connection.cursor()
             query = "show tables"
             cursor.execute(query

)
             data = cursor.fetchall()
             tables = []
             for row in data:
                    for field in row:
                           tables.append(field.strip())
             for i in range(len(tables)):
                    print i, tables[i]
             tb = tables[int(sys.stdin.readline().strip())]
             out.write("table=" + tb + "\n")
             query = "describe " + tb
             cursor.execute(query)
             data = cursor.fetchall()
             columns = []
             for row in data:
                    columns.append(row[0].strip())

             for i in range(len(columns)):
                    print columns[i]
             print "Not index choose the exact column names seperated by coma"
             cols = sys.stdin.readline().strip()
             out.write("columns=" + cols + "\n")

             cursor.close()

             connection.close()
             out.write("source end"+"\n")

             print "Enter the Data Source Type:"
             print "1\. MySql"
             print "2\. Text"
             print "3\. Exit"

       if(int(data1) == 2):
             print "path of text file:"
             path = sys.stdin.readline().strip()
             file = open(path)
             count = 0
             for line in file:
                    print line
                    count = count + 1
                    if count > 3:
                          break
             file.close()
             out.write("source begin"+"\n"+"type=text\n")
             out.write("path=" + path + "\n")
             print "enter delimeter:"
             dlm = sys.stdin.readline().strip()
             out.write("dlm=" + dlm + "\n")
             print "enter column indexes seperated by comma:"
             cols = sys.stdin.readline().strip()
             out.write("columns=" + cols + "\n")
             out.write("source end"+"\n")

             print "Enter the Data Source Type:"
             print "1\. MySql"
             print "2\. Text"
             print "3\. Exit"

       if(int(data1) == 3):
             out.close()
             sys.exit()

```

## 弹性搜索

Elasticsearch (ES)底层客户端给出了从 Python 到 ES REST 端点的直接映射。Elasticsearch 的一大优势是它在一个地方为数据分析提供了一个完整的解决方案。Elasticsearch 就是数据库。它有一个名为 Kibana 的可配置前端，一个名为 Logstash 的数据收集工具，以及一个名为 Shield 的企业安全特性。

这个示例具有称为 cat、cluster、indices、ingest、nodes、snapshot 和 tasks 的特性，这些特性分别转换为实例`CatClient`、`ClusterClient`、`IndicesClient`、`CatClient`、`ClusterClient`、`IndicesClient`、`IngestClient`、`NodesClient`、`SnapshotClient`、`NodesClient`、`SnapshotClient`和`TasksClient`。这些实例是访问这些类及其方法的唯一受支持的方式。

您可以指定自己的连接类，可以通过提供`connection_class`参数来使用。

```py
# create connection to local host using the ThriftConnection
Es1=Elasticsearch(connection_class=ThriftConnection)

```

如果你想打开[嗅探](https://elasticsearch-py.readthedocs.io/en/master/index.html%23sniffing)，那么你有几个选择(在本章后面描述)。

```py
# create connection that will automatically inspect the cluster to get
# the list of active nodes. Start with nodes running on 'esnode1' and
# 'esnode2'
Es1=Elasticsearch(
    ['esnode1', 'esnode2'],
# sniff before doing anything
sniff_on_start=True,
# refresh nodes after a node fails to respond
sniff_on_connection_fail=True,
# and also every 30 seconds
sniffer_timeout=30
)

```

不同的主机可以有不同的参数；您可以为每个节点使用一个字典来指定它们。

```py
# connect to localhost directly and
another node using SSL on port 443
# and an url_prefix. Note that ``port`` needs to be an int.
Es1=Elasticsearch([
{'host':'localhost'},
{'host':'othernode','port':443,'url_prefix':'es','use_ssl':True},
])

```

还支持 SSL 客户端认证(参见`Urllib3HttpConnection`了解选项的详细描述)。

```py
Es1=Elasticsearch(
['localhost:443','other_host:443'],
# turn on SSL
use_ssl=True,
# make sure we verify SSL certificates (off by default)
verify_certs=True,
# provide a path to CA certs on disk
ca_certs='path to CA_certs',
# PEM formatted SSL client certificate
client_cert='path to clientcert.pem',
# PEM formatted SSL client key
client_key='path to clientkey.pem'
)

```

### 连接层 API

许多类负责处理 Elasticsearch 集群。这里，通过将参数传递给`Elasticsearch`类，可以忽略正在使用的默认子类。属于客户端的每一个参数都会被添加到[`Transport`](https://elasticsearch-py.readthedocs.io/en/master/connection.html%23elasticsearch.Transport%23elasticsearch.Transport)`ConnectionPool`和`Connection`上。

举个例子，如果你想使用你个人对`ConnectionSelector`类的使用，你只需要传入`selector_class`参数。

整个 API 以高精度包装了原始的 REST API，其中包括调用的必需参数和可选参数之间的区别。这意味着代码区分了位置参数和关键字参数；我建议你在所有调用中使用关键字参数，以保持一致性和安全性。如果 Elasticsearch 返回 2XX 响应，则 API 调用成功(并将返回响应)。否则会引发一个`TransportError`的实例(或者更具体的子类)。您可以在异常中看到其他异常和错误状态。如果您不希望引发异常，您总是可以传入一个带有应该被忽略的单个状态代码或它们的列表的`ignore`参数。

```py
from elasticsearch import Elasticsearch
es=Elasticsearch()
# ignore 400 cause by IndexAlreadyExistsException when creating an index
es.indices.create(index='test-index',ignore=400)

# ignore 404 and 400
es.indices.delete(index='test-index',ignore=[400,404])

```

## Neo4j Python 驱动程序

Neo4j Python 驱动由 Neo4j 支持，通过二进制协议与数据库连接。它试图保持简约，但同时又是 Python 的惯用语言。

```py
pip install neo4j-driver

from neo4j.v1 import GraphDatabase, basic_auth

driver11 = GraphDatabase.driver("bolt://localhost", auth=basic_auth("neo4j", "neo4j"))
session11 = driver11.session()

session11.run("CREATE (a:Person {name:'Sayan', title:'Mukhopadhyay'})")

result 11= session11.run("MATCH (a:Person) WHERE a.name = 'Sayan' RETURN a.name AS name, a.title AS title")
for recordi n resul11t:
print("%s %s"% (record["title"], record["name"]))

session.close()

```

## neo4j-rest-客户端

neo4j-rest-client 的主要目标是确保已经通过 python-embedded 在本地使用 neo4j 的 Python 程序员也能够访问 Neo4j REST 服务器。所以，neo4j-rest-client API 的结构与 python-embedded 完全同步。但是，引入了一个新的结构，以达到更 Pythonic 化的风格，并用 Neo4j 团队引入的新特性来扩充 API。

## 内存数据库

另一类重要的数据库是内存数据库。它在 RAM 中存储和处理数据。所以，对数据库的操作非常快，而数据是易变的。SQLite 是内存数据库的一个流行例子。在 Python 中，你需要使用 sqlalchemy 库来操作 SQLite。在第 [1](1.html) 章的 Flask 和 Falcon 示例中，我向您展示了如何从 SQLite 中选择数据。在这里，我将展示如何在 SQLite 中存储熊猫数据框:

```py
from sqlalchemy import create_engine
import sqlite3
conn = sqlite3.connect('multiplier.db')
conn.execute('''CREATE TABLE if not exists multiplier
       (domain        CHAR(50),
        low        REAL,
        high        REAL);''')
conn.close()

db_name = "sqlite:///" + prop + "_" + domain + str(i) + ".db"
disk_engine = create_engine(db_name)
df.to_sql('scores', disk_engine, if_exists="replace")

```

## MongoDB (Python 版本)

MongoDB 是一个开源文档数据库，旨在提供卓越的性能、易用性和自动伸缩性。MongoDB 确保不需要对象关系映射(ORM)来促进开发。包含由字段和值对组成的数据结构的文档在 MongoDB 中被称为记录。这些记录类似于 JSON 对象。字段的值可以由其他文档、数组和文档数组组成。

```py
{
"_id":ObjectId("01"),
"address": {
"street":"Siraj Mondal Lane",
"pincode":"743145",
"building":"129",
"coord": [ -24.97, 48.68 ]
   },
"borough":"Manhattan",

```

### 将数据导入集合

`mongoimport`可用于将文档放入数据库的集合中，在系统外壳或命令提示符内。如果集合预先存在于数据库中，操作将首先丢弃原始集合。

```py
mongoimport --DB test --collection restaurants --drop --file ~/downloads/primer-dataset.json

```

`mongoimport`命令被加入到本地主机上运行的 MongoDB 实例中，端口号为 27017。`--file`选项提供了一种导入数据的方法；这里是`~/downloads/primer-dataset.json`。

要将数据导入到运行在不同主机或端口上的 MongoDB 实例中，需要在`mongoimport`命令中通过包含`--host`或`--port`选项来特别提到主机名或端口。

MySQL 中也有类似的`load`命令。

### 使用 pymongo 创建连接

要创建连接，请执行以下操作:

```py
import MongoClient from pymongo.
Client11 = MongoClient()

```

如果没有对 [`MongoClient`](http://api.mongodb.com/python/current/api/pymongo/mongo_client.html%23pymongo.mongo_client.MongoClient%23(in%20PyMongo%20v3.4rc0)) 提到参数，那么它将默认为在端口 27017 上的本地主机接口上运行的 MongoDB 实例。

可以指定一个完整的 [MongoDB URL](http://docs.mongodb.com/manual/reference/connection-string) 来定义连接，包括主机和端口号。例如，下面的代码连接到运行在`mongodb0.example.net`和 27017 端口上的 MongoDB 实例:

```py
Client11 = MongoClient("mongodb://myhostname:27017")

```

### 访问数据库对象

要将名为`primer`的数据库分配给局部变量`DB`，您可以使用以下任意一行:

```py
Db11 = client11.primer
db11 = client11['primer']

```

通过使用字典样式或数据库对象的属性访问，可以直接访问集合对象，如以下两个示例所示:

```py
Coll11 = db11.dataset
coll = db11['dataset']

```

### 插入日期

您可以将文档放入不存在的集合中，以下操作将创建该集合:

```py
result=db.addrss.insert_one({<<your json >>)

```

### 更新数据

以下是更新数据的方法:

```py
result=db.address.update_one(
 {"building": "129",
 {"$set": {"address.street": "MG Road"}}
)

```

### 删除数据

要从集合中删除所有文档，请使用以下命令:

```py
result=db.restaurants.delete_many({})

```

## 熊猫

本节的目标是展示一些例子，让你能够开始使用熊猫。这些插图取自真实世界的数据，以及任何固有的缺陷和怪异之处。Pandas 是一个受 R 数据框架概念启发的框架。

要从 CSV 文件中读取数据，请使用以下命令:

```py
import pandas as pd
broken_df=pd.read_csv('data.csv')

```

要查看前三行，请使用以下命令:

```py
broken_df[:3]

```

要选择列，请使用以下命令:

```py
fixed_df['Column Header']

```

要绘制一个列，请使用以下命令:

```py
fixed_df['Column Header'].plot()

```

要获得数据集中的最大值，请使用:

```py
MaxValue=df['Births'].max() where Births is the column header

```

让我们假设数据集中还有一个名为`Name`的列。`Name`的命令与最大值相关。

```py
MaxName=df['Names'][df['Births']==df['Births'].max()].values

```

在 Pandas 中还有许多其他方法，比如`sort`、`groupby`和`orderby`，这些方法对于处理结构化数据非常有用。此外，Pandas 有一个现成的适配器，可用于 MongoDB、Google Big Query 等流行数据库。

接下来是一个关于熊猫的复杂例子。在每个不同列值的`X`数据框中，通过`root`列找到楼层分组的平均值。

```py
for col in X.columns:
                        if col != 'root':
                                avgs = df.groupby([col,'root'],as_index=False)['floor'].aggregate(np.mean)
                                for i,row in avgs.iterrows():
                                        k = row[col]
                                        v = row['floor']
                                        r = row['root']
                                        X.loc[(X[col] == k) & (X['root'] == r), col] = v
2.

```

## 使用 Python 进行 ETL(非结构化数据)

处理非结构化数据是现代数据分析的一项重要任务。在这一节中，我将介绍如何解析电子邮件，并介绍一个叫做主题爬行的高级研究主题。

### 电子邮件解析

参见第 [1](1.html) 章，了解使用 Python 进行网络抓取的完整示例。

和 BeautifulSoup 一样，Python 也有一个用于电子邮件解析的库。下面是解析存储在邮件服务器上的电子邮件数据的示例代码。配置中的输入是用户名和要为用户解析的邮件数量。

```py
from email.parser import Parser
import os
import sys

conf = open(sys.argv[1])
config={}
users={}
for line in conf:
       if ("," in line):
             fields = line.split(",")
             key = fields[0].strip().split("=")[1].strip()
             val = fields[1].strip().split("=")[1].strip()
             users[key] = val
       else:
             if ("=" in line):
                    words = line.strip().split('=')
                    config[words[0].strip()] = words[1].strip()
conf.close()

for usr in users.keys():
       path = config["path"]+"/"+usr+"/"+config["folder"]
       files = os.listdir(path)
       for f in sorted(files)

:
             if(int(f) > int(users[usr])):
                    users[usr] = f
                    path1 = path + "/" + f
                    data = ""
                    with open (path1) as myfile:
                          data=myfile.read()
                    if data != "" :
                          parser = Parser()
                    email = parser.parsestr(data)
                    out = ""
                    out = out + str(email.get('From')) + "," + str(email.get('To')) + "," + str(email.get('Subject')) + "," + str(email.get('Date')).replace(","," ")

                    if email.is_multipart():
                          for part in email.get_payload():
                                 out = out + "," + str(part.get_payload()).replace("\n"," ").replace(","," ")
                    else:
                          out = out + "," + str(email.get_payload()).replace("\n"," ").replace(","," ")

                    print out,"\n"

conf = open(sys.argv[1],'w')
conf.write("path=" + config["path"] + "\n")
conf.write("folder=" + config["folder"] + "\n")

for usr in users.keys():
       conf.write("name="+ usr +",value=" + users[usr] + "\n")

conf.close()
Sample config file for above code

.
path=/cygdrive/c/share/enron_mail_20110402/enron_mail_20110402/maildir
folder=Inbox
name=storey-g,value=142
name=ybarbo-p,value=775
name=tycholiz-b,value=602

```

### 主题爬行

主题爬虫是智能爬虫，从网络上的任何地方检索信息。他们从一个 URL 开始，然后在它下面的页面中找到链接；然后他们寻找新的网址，绕过通用搜索引擎的可扩展性限制。这是通过将爬行过程分布在用户、查询甚至客户端计算机上来实现的。爬行器可以使用可用的上下文来无限循环地浏览链接，目的是系统地定位高度相关的聚焦页面。

网络搜索是一项复杂的任务。大量的机器学习工作被应用于寻找页面之间的相似性，例如获取或访问的 URL 的最大数量。

#### 爬行算法

下图描述了主题爬网算法如何与其主要组件一起工作。

![A458833_1_En_2_Figa_HTML.jpg](img/A458833_1_En_2_Figa_HTML.jpg)

主题爬虫的起始 URL 被称为种子 URL。还有另一组称为目标 URL 的 URL，它们是期望输出的例子。

主题爬行的一个有趣的应用是，一个人力资源组织在网上任何地方搜索拥有特定技能的候选人。一个简单的替代解决方案是使用搜索引擎 API。下面的代码是一个使用 Google Search API、BeautifulSoup 和正则表达式的示例，这些表达式从 Web 上搜索具有特定技能的潜在候选人的电子邮件 ID 和电话号码。

```py
#!/usr/bin/env python
# -*- coding: utf-8 -*-

import pprint, json, urllib2
import nltk, sys, urllib
from bs4 import BeautifulSoup
import csv

from googleapiclient.discovery import build

def link_score(link):
 if ('cv' in link or 'resume' in link) and 'job' not in link:
 return True

def process_file():
 try:

 with open('data1.json','r') as fl:
 data = json.load(fl)
 all_links = []
 # pprint.pprint(len(data['items']))
 for item in data['items']:
 # print item['formattedUrl']
 all_links.append(item['formattedUrl'])
 return all_links
 except:
 return []

def main(istart, search_query):
 service = build("customsearch", "v1",
 developerKey="abcd")

 res = service.cse().list(
 q= search_query,
 cx='1234',
 num=10,
 gl='in', #in for india comment this for whole web
 start = istart,
 ).execute()
 import json
 with open('data1.json', 'w') as fp:
 json.dump(res, fp)
# pprint.pprint(type(res))
# pprint.pprint(res)

def get_email_ph(link_text, pdf=None):
 if pdf==True:

 from textract import process
 text = process(link_text)
 else:
 text = link_text
 # print text
 import re
 email = []
 ph = []
 valid_ph = re.compile("[789][0-9]{9}$")
 valid = re.compile("[A-Za-z]+[@]{1}[A-Za-z]+\.[a-z]+")
 for token in re.split(r'[,\s]',text):
# for token in nltk.tokenize(text):
 # print token

 a = valid.match(token)
 b = valid_ph.match(token)
 if a != None:
 print a.group()
 email.append(a.group())
 if b != None:
 print b.group()
 ph.append(b.group())
 return email, ph

def process_pdf_link(link):
 html = urllib2.urlopen(link)
 file = open("document.pdf", 'w')
 file.write(html.read())
 file.close()
 return get_email_ph("document.pdf", pdf=True)

def process_doc_link(link):
 testfile = urllib.URLopener()
 testfile.retrieve(link, "document.doc")
 return get_email_ph("document.doc", pdf=False)

def process_docx_link(link):
 testfile = urllib.URLopener()
 testfile.retrieve(link, "document.docx")
 return get_email_ph("document.docx", pdf=False)

def process_links(all_links):
 with open('email_ph.csv', 'wb') as csvfile:
 spamwriter = csv.writer(csvfile, delimiter=',')

 for link in all_links

:
 if link[:4] !='http':
 link = "http://"+link
 print link
 try:
 if link[-3:] == 'pdf':
 try:
 email, ph = process_pdf_link(link)
 spamwriter.writerow([link, ' '.join(email), ' '.join(ph)])
 except:
 print "error",link
 print sys.exc_info()
 elif link[-4:] == 'docx':
 try:
 email, ph = process_docx_link(link)
 spamwriter.writerow([link, ' '.join(email), ' '.join(ph)])
 except:
 print "error",link
 print sys.exc_info()
 spamwriter.writerow([link, ' '.join(email), ' '.join(ph)])
 elif link[-3:] == 'doc':
 try:
 email, ph = process_doc_link(link)
 spamwriter.writerow([link, ' '.join(email), ' '.join(ph)])
 except:
 print "error",link
 print sys.exc_info()
 spamwriter.writerow([link, ' '.join(email), ' '.join(ph)])
 else:
 try:
 html = urllib2.urlopen(link)
 email, ph = get_email_ph(BeautifulSoup(html.read()).get_text(), pdf=False)
 spamwriter.writerow([link, ' '.join(email), ' '.join(ph)])
 except:
 print "error",link

 print sys.exc_info()
 spamwriter.writerow([link, ' '.join(email), ' '.join(ph)])
 except:
 pass
 print "error",link
 print sys.exc_info()

if __name__ == '__main__':

#
 search_query = ' ASP .NET, C#, WebServices, HTML Chicago USA biodata cv'
#
#
 all_links = []
# all_links.extend(links)
 for i in range(1,90,10):
 main(i, search_query)
 all_links.extend(process_file())

 process_links(all_links)
#

```

这个代码用于从网络上为一组给定的求职关键词找到相关的联系人。它使用 Google Search API 获取链接，根据某个 URL 中是否存在某些关键字对它们进行过滤，然后解析链接内容并找到电子邮件 ID 和电话号码。内容可以是 PDF、Word 或 HTML 文档。