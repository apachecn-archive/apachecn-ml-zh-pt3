# 1.机器学习基础

制造智能的、有知觉的、有自我意识的机器的想法并不是在最近几年突然出现的。事实上，希腊神话中的许多传说都谈到智能机器和发明拥有自我意识和智能。几个世纪以来，计算机的起源和发展确实发生了革命性的变化，从 17 世纪的基本算盘及其后代计算尺，到 19 世纪查尔斯·巴贝奇设计的第一台通用计算机。事实上，一旦计算机随着巴贝奇发明的分析引擎和阿达·洛芙莱斯在 1842 年编写的第一个计算机程序而开始进化，人们就开始怀疑和思考是否有一天计算机或机器会真正变得智能并开始独立思考。事实上，著名的计算机科学家艾伦·图灵(Alan Turing)在理论计算机科学、算法和形式语言的发展方面具有很高的影响力，早在 20 世纪 50 年代就提出了人工智能和机器学习等概念。这个关于让机器学习的进化的简短见解只是为了让你对一些已经存在了几个世纪，但最近开始获得很多关注和焦点的东西有一个概念。

随着更快的计算机、更好的处理、更强的计算能力和更大的存储量，我们一直生活在我所称的“信息时代”或“数据时代”。日复一日，我们通过使用数据科学、人工智能、数据挖掘和机器学习的概念和方法来管理大数据和构建智能系统。当然，你们中的大多数人一定听说过我刚才提到的许多术语，也听说过“数据是新的石油”这样的说法。企业和组织在过去十年中面临的主要挑战是使用各种方法来尝试理解他们拥有的所有数据，并使用从中获得的有价值的信息和见解来做出更好的决策。事实上，随着技术的巨大进步，包括廉价和大规模计算、硬件(包括 GPU)和存储的可用性，我们已经看到围绕人工智能、机器学习和最近的深度学习等领域建立了一个蓬勃发展的生态系统。研究人员、开发人员、数据科学家和工程师日以继夜地研究和构建工具、框架、算法、技术和方法，以构建智能模型和系统，这些模型和系统可以预测事件、自动化任务、执行复杂分析、检测异常、自我修复故障，甚至理解和响应人类输入。

本章遵循结构化的方法，涵盖与机器学习相关的各种概念、方法和思想。核心思想是给你足够的背景，为什么我们需要机器学习，机器学习的基本构件，以及机器学习目前为我们提供了什么。这将使您能够了解如何最好地利用机器学习来最大限度地利用您的数据。由于这是一本关于实用机器学习的书，虽然我们将在后续章节中专注于特定的用例、问题和真实世界的案例研究，但理解关于学习算法、数据管理、模型构建、评估和部署的正式定义、概念和基础是极其重要的。因此，我们涵盖了所有这些方面，包括与数据挖掘和机器学习工作流相关的行业标准，从而为您提供了一个基础框架，可用于处理和解决我们在后续章节中解决的任何现实问题。除此之外，我们还涵盖了与机器学习相关的不同跨学科领域，这些领域实际上都是人工智能的相关领域。

这本书更侧重于应用或实用的机器学习，因此大部分章节的重点将是应用机器学习技术和算法来解决现实世界的问题。因此，对基础数学、统计学和机器学习有一定程度的精通是有益的。然而，由于这本书考虑到了不同读者的不同专业水平，这一基础章节以及第一部分和第二部分中的其他章节将让你快速了解机器学习和构建机器学习管道的关键方面。如果你已经熟悉了与机器学习相关的基本概念及其重要性，你可以快速浏览本章，进入第 [2](02.html) 章，“Python 机器学习生态系统”，在这里我们将讨论 Python 对于构建机器学习系统的好处，以及通常用于解决机器学习问题的主要工具和框架。

这本书非常强调通过大量代码片段、示例和多个案例研究进行学习。我们利用 Python 3，并用相关的代码文件(`.py`)和 jupyter 笔记本(`.ipynb`)来描述我们的所有示例，以获得更具交互性的体验。我们鼓励你在 [`https://github.com/dipanjanS/practical-machine-learning-with-python`](https://github.com/dipanjanS/practical-machine-learning-with-python) 查阅这本书的 GitHub 知识库，在那里我们将分享与每一章相关的必要代码和数据集。在阅读本书的过程中，您可以利用这个资源库自己尝试所有的示例，并在解决自己的实际问题时采用它们。与机器学习和深度学习相关的奖金内容也将在未来分享，所以继续观看那个空间！

## 对机器学习的需求

人类可能是目前这个星球上最先进和最聪明的生命形式。我们可以思考、推理、构建、评估和解决复杂的问题。人类的大脑仍然是我们自己还没有完全弄清楚的东西，因此人工智能仍然是在几个方面没有超越人类智能的东西。因此，你可能会想到一个紧迫的问题，为什么我们真的需要机器学习？我们有什么必要去花费时间和精力让机器学习和智能呢？答案可以用简单的一句话来概括，“规模化地做数据驱动的决策”。我们将在接下来的章节中深入解释这个句子的细节。

### 做出数据驱动的决策

从数据中获取关键信息或见解是企业和组织大力投资于优秀劳动力以及机器学习和人工智能等新范式和领域的关键原因。数据驱动决策的想法并不新鲜。运筹学、统计学和管理信息系统等领域已经存在了几十年，并试图通过使用数据和分析来做出数据驱动的决策，从而为任何企业或组织带来效率。利用您的数据来获得可操作的见解并做出更好决策的艺术和科学被称为做出数据驱动的决策。当然，这说起来容易做起来难，因为我们很少能直接使用原始数据来做出任何有见地的决策。这个问题的另一个重要方面是，我们经常使用推理或直觉的力量，试图根据我们在一段时间内和工作中所学到的东西来做出决定。我们的大脑是一个极其强大的装置，帮助我们做到这一点。考虑一些问题，比如理解你的同事或朋友在说什么，识别图像中的人，决定是批准还是拒绝一项商业交易，等等。虽然我们几乎可以不由自主地解决这些问题，但你能向别人解释一下你是如何解决这些问题的吗？某种程度上也许是，但过一会儿，就会像“嘿！我的大脑替我做了大部分的思考！”这正是为什么很难让机器像计算贷款利息或退税这样的常规计算程序一样学习解决这些问题。无法编程的问题的解决方案本质上需要一种不同的方法，我们使用数据本身来驱动决策，而不是使用可编程逻辑、规则或代码来做出这些决策。我们将在以后的章节中进一步讨论这一点。

### 效率和规模

虽然获得洞察力并根据数据做出决策至关重要，但这也需要高效、大规模地完成。使用机器学习或人工智能技术的关键思想是通过从数据中学习特定模式来自动化流程或任务。我们都希望计算机或机器能告诉我们股票何时上涨或下跌，图像是计算机还是电视，我们的产品定位和报价是否是最好的，确定购物价格趋势，在故障或断电发生前检测它们，等等。虽然人类的智慧和专业知识是我们绝对不能缺少的东西，但我们需要高效地解决大规模的现实世界问题。

A Real-World Problem at Scale

考虑以下现实世界的问题。您是 DSS 公司世界级基础架构团队的经理，该团队以基于云的基础架构和分析平台的形式为其他企业和消费者提供数据科学服务。作为服务和基础架构的提供商，您希望您的基础架构是一流的，并且能够抵御故障和停机。考虑到您是从圣路易斯的一个小办公室开始的，您已经很好地掌握了与您的 10 名经验丰富的员工团队一起定期监控所有网络设备，包括路由器、交换机、防火墙和负载平衡器。很快，你在提供基于云的深度学习服务和 GPU 开发方面取得了突破，并获得了巨额利润。然而，现在你的客户越来越多。是时候将你的基地扩展到旧金山、纽约和波士顿的办公室了。现在，您拥有一个庞大的互联基础设施，每栋建筑中都有数百台网络设备！现在，您将如何管理您的大规模基础架构？您是否为每个办公室雇佣更多人力，或者您是否尝试利用机器学习来处理断电预测、自动恢复和设备监控等任务？从工程师和经理的角度考虑一下这个问题。

### 传统编程范式

虽然电脑是极其精密复杂的设备，但它只是我们众所周知的电视机的另一个版本。“这怎么可能呢？”在这一点上是一个非常合理的问题。让我们考虑一台电视，甚至是一台所谓的智能电视，这种电视目前已经上市。无论在理论上还是在实践中，电视都会按照你的程序去做。它会显示您想要观看的频道，录制您稍后想要观看的节目，并播放您想要播放的应用程序！计算机一直在做完全相同的事情，但方式不同。传统的编程范例基本上涉及用户或程序员使用代码编写一组指令或操作，使计算机对数据执行特定的计算以给出期望的结果。图 [1-1](#Fig1) 描绘了传统编程范例的典型工作流程。

![A448827_1_En_1_Fig1_HTML.jpg](A448827_1_En_1_Fig1_HTML.jpg)

图 1-1。

Traditional programming paradigm

从图 [1-1](#Fig1) 中，你可以了解到，给计算机的核心输入是数据和一个或多个程序，这些程序基本上是在编程语言的帮助下编写的代码，如 Java、Python 等高级语言，或 C 甚至汇编等低级语言。程序使计算机能够处理数据、执行计算和生成输出。使用传统编程范例可以很好地完成的一项任务是计算您的年度税收。

现在，让我们考虑一下我们在上一节中讨论的 DSS 公司的实际基础设施问题。你认为传统的编程方法能够解决这个问题吗？嗯，在某种程度上可以。我们也许能够接入设备数据和事件流和日志，并访问各种设备属性，如使用级别、信号强度、传入和传出连接、内存和处理器使用级别、错误日志和事件，等等。然后，我们可以使用团队中网络和基础设施专家的领域知识，并根据基于这些数据属性的特定决策和规则来设置一些事件监控系统。这将为我们提供一种基于规则的反应式分析解决方案，我们可以监控设备，观察是否出现任何特定的异常或中断，然后采取必要的措施来快速解决任何潜在的问题。我们可能还需要雇佣一些支持和运营人员来持续监控和解决问题。然而，仍然存在一个紧迫的问题，即在停机或问题实际发生之前，尽可能多地防止它们的发生。机器学习能在某种程度上帮助我们吗？

### 为什么是机器学习？

我们现在将解决引发这场讨论的问题，即我们为什么需要机器学习。考虑到你到目前为止学到的东西，虽然传统的编程范式相当不错，人类的智能和领域专业知识肯定是做出数据驱动决策的重要因素，但我们需要机器学习来做出更快更好的决策。机器学习范式试图考虑数据和预期的输出或结果(如果有的话)，并使用计算机来构建程序，这也称为模型。该程序或模型可在未来用于制定必要的决策，并根据新的输入给出预期的输出。图 [1-2](#Fig2) 显示了机器学习范例与传统编程范例的相似之处和不同之处。

![A448827_1_En_1_Fig2_HTML.jpg](A448827_1_En_1_Fig2_HTML.jpg)

图 1-2。

Machine Learning paradigm

图 [1-2](#Fig2) 强调了一个事实，即在机器学习范例中，机器(在这种情况下是计算机)试图使用输入数据和预期输出来学习数据中的固有模式，这些模式最终将有助于建立一个类似于计算机程序的模型，这将有助于在未来通过使用从以前的数据点(其知识或经验)学习到的知识来为新的输入数据点做出数据驱动的决策(预测或告诉我们输出)。你可能会开始看到其中的好处。我们不需要手工编码的规则、复杂的流程图、case 和 if-then 条件，以及通常用于构建任何决策系统或决策支持系统的其他标准。基本思想是利用机器学习做出有见地的决策。

一旦我们讨论了为 DSS 公司管理基础设施的现实问题，这一点就会变得更加清楚。在传统的编程方法中，我们谈到了雇佣新员工、建立基于规则的监控系统等等。如果我们在这里使用机器学习范式的转变，我们可以通过以下步骤来解决这个问题。

*   利用设备数据和日志，并确保我们在一些数据存储(数据库、日志或平面文件)中有足够的历史数据
*   确定对构建模型有用的关键数据属性。这可能是设备使用、日志、内存、处理器、连接、线路强度、链接等等。
*   观察和捕获设备属性及其在不同时间段的行为，包括正常设备行为和异常设备行为或停机。这些结果就是你的输出，设备数据就是你的输入
*   将这些输入和输出对提供给计算机中任何特定的机器学习算法，并构建一个模型来学习固有的设备模式并观察相应的输出或结果
*   部署此模型，以便对于设备属性的较新值，它可以预测特定设备的行为是否正常，或者它是否可能导致潜在的中断

因此，一旦你能够建立一个机器学习模型，你就可以轻松地部署它，并围绕它建立一个智能系统，这样你不仅可以被动地监控设备，还可以主动识别潜在的问题，甚至在任何问题出现之前修复它们。想象一下，构建自我修复或自动修复系统，并对设备进行全天候监控。可能性确实是无穷无尽的，你不必每次扩大办公室或购买新的基础设施时都雇佣新的员工。

当然，前面讨论的建立机器学习模型所需的一系列步骤的工作流程要比它所描述的复杂得多，但这只是为了强调并让你更多地从概念上而不是技术上思考机器学习过程中的范式是如何转变的，你也需要改变你的思维，从传统的基于方法转向更多的数据驱动。机器学习的美妙之处在于，它永远不会受到领域的限制，你可以使用技术来解决跨多个领域、业务和行业的问题。同样，如图 [1-2](#Fig2) 所示，你总是不需要输出数据点来建立模型；对于更适合无监督学习的技术(我们将在本章后面深入讨论)，有时输入数据就足够了(或者说输出数据可能不存在)。一个简单的例子是根据过去的交易数据，通过查看他们通常在商店一起购买的食品来确定客户的购物模式。在下一节中，我们将深入理解机器学习。

## 理解机器学习

到目前为止，您已经看到了适合使用机器学习解决的典型现实世界问题的样子。除此之外，你还很好地掌握了传统编程和机器学习范式的基础。在本节中，我们将更详细地讨论机器学习。更具体地说，我们将从概念以及特定领域的角度来看待机器学习。机器学习可能在 20 世纪 90 年代开始崭露头角，当时研究人员和科学家开始将其作为人工智能(AI)的一个子领域给予更大的重视，以便技术从人工智能、概率和统计中借用概念，与使用需要大量手动时间和精力的固定的基于规则的模型相比，这些技术的性能要好得多。当然，正如我们之前指出的，机器学习并不是在 20 世纪 90 年代凭空出现的。这是一个多学科领域，随着时间的推移逐渐演变，并且在我们发言时仍在演变。

对进化历史的简要提及将非常有助于了解机器学习和人工智能发展中涉及的各种概念和技术。你可以说它开始于 18 世纪末和 19 世纪初，当第一批研究成果发表的时候，它们基本上都是关于贝叶斯定理的。事实上，托马斯·贝叶斯的主要著作《一篇解决机会主义问题的论文》出版于 1763 年。除此之外，在这段时间里，在概率和数学领域里还做了大量的研究和发现。这为 20 世纪更多的突破性研究和发明铺平了道路，其中包括 20 世纪初安德烈·马尔科夫(Andrey Markov)的马尔科夫链、艾伦·图灵(Alan Turing)的学习系统命题以及 20 世纪 50 年代弗兰克·罗森布拉特(Frank Rosenblatt)发明的非常著名的感知机。你们中的许多人可能知道，自 20 世纪 50 年代以来，神经网络有过几次高潮和低谷，它们最终在 20 世纪 80 年代随着反向传播的发现而重新成为人们关注的焦点(感谢 Rumelhart、Hinton 和 Williams！)和其他几项发明，包括 Hopfield 网络、新认知、卷积和递归神经网络以及 Q-learning。当然，自 20 世纪 90 年代以来，随着随机森林、支持向量机、长短期记忆网络(LSTMs)的发现，以及机器和深度学习框架的开发和发布，包括`torch`、`theano`、`tensorflow`、`scikit-learn`等等，机器学习也开始快速发展。我们也看到了智能系统的崛起，包括 IBM Watson、DeepFace 和 AlphaGo。事实上，这段旅程就像坐过山车一样，还有很长的路要走。花一点时间来思考这个进化的旅程，让我们谈谈这个旅程的目的。为什么以及什么时候要真正让机器学习？

### 为什么要让机器学习？

在前面的部分中，我们已经讨论了为什么我们需要机器学习，当我们解决试图利用数据来使用学习算法进行大规模数据驱动的决策，而不是过多关注手动工作和固定的基于规则的系统时。在本节中，我们将更详细地讨论为什么以及何时应该让机器学习。为了我们的利益，人类、企业和组织每天都在努力解决一些现实世界中的任务和问题。有几种情况下让机器学习可能是有益的，其中一些如下所述。

*   在某个领域缺乏足够的人类专业知识(例如，模拟在未知领域甚至太空行星中的导航)。
*   场景和行为会随着时间不断变化(例如，组织中基础设施的可用性、网络连接性等)。
*   人类在该领域中具有足够的专业知识，但是将这种专业知识正式解释或翻译成计算任务(例如，语音识别、翻译、场景识别、认知任务等)是极其困难的。
*   解决具有大量数据和太多复杂条件和约束的领域特定问题。

前面提到的场景只是几个例子，在这些例子中，让机器学习比投入时间、精力和金钱来试图构建可能在范围、覆盖范围、性能和智能方面受到限制的次等智能系统更有效。作为人类和领域专家，我们已经拥有足够的关于世界和我们各自领域的知识，这些知识可以是客观的、主观的，有时甚至是直觉的。随着大量历史数据的可用性，我们可以利用机器学习范式，通过在一段时间内观察数据模式来获得足够的经验，从而使机器执行特定的任务，然后在未来以最少的人工干预来解决任务时使用这种经验。核心思想仍然是让机器解决可以很容易直观地、几乎不由自主地定义但极难正式定义的任务。

### 形式定义

我们现在准备正式定义机器学习。到目前为止，你可能已经遇到了机器学习的多种定义，其中包括:使机器智能化的技术，类固醇上的自动化，自动化任务本身的自动化，21 世纪最性感的工作，让计算机自我学习和无数其他学习！虽然所有这些都是很好的引用，并且在一定程度上是正确的，但定义机器学习的最佳方式是从著名教授 Tom Mitchell 在 1997 年定义的机器学习的基础开始。

机器学习的想法是，将有一些学习算法来帮助机器从数据中学习。米切尔教授是这样定义的。

> "If the performance of a computer program measured by P on tasks in T improves with experience E, it is said that the computer program learns from experience E about certain tasks T and performance measurement P."

虽然这个定义乍一看似乎令人望而生畏，但我要求你慢慢通读几次，重点关注三个参数——T、P 和 E——它们是任何学习算法的主要组成部分，如图 [1-3](#Fig3) 所示。

![A448827_1_En_1_Fig3_HTML.jpg](A448827_1_En_1_Fig3_HTML.jpg)

图 1-3。

Defining the components of a learning algorithm

我们可以把定义简化如下。机器学习是一个由学习算法组成的领域，这些算法:

*   提高他们的绩效
*   在执行一些任务 T 时
*   随着时间的推移，经验 E

虽然我们在下面的章节中详细讨论了这些实体中的每一个，但我们不会花时间在形式上或数学上定义这些实体中的每一个，因为本书的范围更倾向于应用或实用的机器学习。如果您考虑我们之前的现实问题，任务之一可能是预测我们的基础设施的中断；经验 E 将是我们的机器学习模型通过观察来自各种设备数据属性的模式而随着时间推移将获得的东西；模型 P 的性能可以通过多种方式来衡量，例如模型预测停机的准确程度。

#### 定义任务

在前一节中，我们已经简要地讨论了任务 T，它可以用两种方法来定义。从问题的角度来看，任务 T 基本上是手头要解决的现实世界的问题，可以是从寻找最佳营销或产品组合到预测基础设施故障的任何事情。在机器学习的世界里，如果你能尽可能具体地定义任务，这样你就可以谈论你计划解决的确切问题是什么，以及你如何定义或制定一个特定的机器学习任务。

基于机器学习的任务很难通过常规和传统的编程方法来解决。任务 T 通常可以定义为基于过程或工作流的机器学习任务，系统应该遵循该过程或工作流来操作数据点或样本。通常，数据样本或数据点将由多个数据属性(在机器学习行话中也称为特征)组成，就像我们之前在 DSS Company 的问题中提到的各种设备参数一样。一个典型的数据点可以用一个向量(Python 列表)来表示，这样向量中的每个元素都代表一个特定的数据特征或属性。我们将在下一节以及第 [4](04.html) 章“特性工程和选择”中详细讨论更多特性和数据点。

回到可以归类为机器学习任务的典型任务，下面的列表描述了一些流行的任务。

*   分类或归类:这通常包括问题或任务的列表，其中机器必须接受数据点或样本，并为每个样本分配特定的类别或种类。一个简单的例子是将动物图像分为狗、猫和斑马。
*   回归:这些类型的任务通常涉及执行预测，以便输出真实的数值，而不是输入数据点的类或类别。理解回归任务的最佳方式是以预测房价的现实世界问题为例，将地块面积、楼层数、浴室、卧室和厨房作为每个数据点的输入属性。
*   异常检测:这些任务包括机器检查事件日志、事务日志和其他数据点，以便能够发现异常或不寻常的模式或不同于正常行为的事件。这方面的例子包括试图从日志中发现拒绝服务攻击、欺诈迹象等等。
*   结构化注释:这通常涉及对输入数据点执行一些分析，并将结构化元数据作为注释添加到原始数据中，描述数据元素之间的额外信息和关系。简单的例子是用词性、命名实体、语法和情感来注释文本。还可以对图像进行注释，比如为图像像素指定特定的类别，根据图像的类型、位置等对图像的特定区域进行注释。
*   翻译:自动机器翻译任务通常具有这样的性质，如果您有属于特定语言的输入数据样本，您可以将其翻译成具有另一种期望语言的输出。基于自然语言的翻译无疑是一个处理大量文本数据的巨大领域。
*   聚类或分组:通常通过使机器学习或观察输入数据点本身之间的内在潜在模式、关系和相似性，从输入数据样本中形成聚类或组。通常，这些任务缺乏预先标记或预先注释的数据，因此它们构成了无监督机器学习的一部分(我们将在后面讨论)。例如，将相似的产品、事件和实体分组。
*   转录:这些任务通常需要连续和非结构化的数据的各种表示，并将它们转换成更加结构化和离散的数据元素。示例包括语音到文本、光学字符识别、图像到文本等。

这应该会让您对通常使用机器学习解决的典型任务有一个很好的了解，但是这个列表肯定不是一个详尽的列表，因为任务的限制确实是无穷的，并且随着时间的推移，随着广泛的研究，会发现更多的任务。

#### 定义体验，E

在这一点上，您知道任何学习算法通常都需要数据来随着时间的推移进行学习并执行特定的任务，我们将其命名为 t。使用由数据样本或数据点组成的数据集以便学习算法或模型学习固有模式的过程被定义为通过学习算法获得的经验 E。算法获得的任何经验都来自数据样本或数据点，这可以是任何时间点。您可以使用历史数据一次性向它提供数据样本，甚至可以随时提供新的数据样本。

因此，模型或算法获得经验的想法通常作为迭代过程出现，也称为训练模型。您可以将模型视为一个实体，就像人类一样，通过观察和学习数据中出现的各种属性、关系和模式，通过数据点获取知识或经验。当然，学习和获取经验有各种形式和方法，包括监督学习、非监督学习和强化学习，但我们将在未来的章节中讨论学习方法。现在，后退一步，记住我们所做的类比，当机器真正学习时，它是基于不时提供给它的数据，从而允许它获得关于要解决的任务的经验和知识，这样它就可以使用这种经验，E，来预测或解决未来以前看不见的数据点的相同任务 T。

#### 定义性能，第页

假设我们有一个机器学习算法，它应该执行一项任务 T，并在一段时间内通过数据点获得经验 E。但是，我们如何知道它是否表现良好，或者是否按照它应该的方式运行呢？这就是模型的性能 P 发挥作用的地方。性能 P 通常是一种定量测量或指标，用于查看算法或模型根据经验执行任务 T 的情况，e。虽然性能指标通常是经过多年研究和开发后建立的标准指标，但每个指标通常都是针对我们在任何给定时间点试图解决的任务 T 进行计算的。

典型的性能测量包括准确度、精确度、召回率、F1 分数、灵敏度、特异性、错误率、错误分类率等等。性能测量通常在训练数据样本(算法用来获得经验，E)以及以前没有看到或学习的数据样本(通常称为验证和测试数据样本)上进行评估。这背后的想法是将算法一般化，以便它不会仅在训练数据点上变得太有偏差，并且将来在较新的数据点上表现良好。当我们讨论模型构建和验证时，将会讨论更多关于训练、验证和测试数据的内容。

在解决任何机器学习问题时，大多数情况下，性能度量 P 的选择要么是准确度，要么是 F1 分数，要么是精度，要么是召回率。虽然这在大多数情况下是正确的，但您应该始终记住，有时很难选择能够准确地让我们了解算法根据实际行为或预期结果执行得有多好的性能指标。一个简单的例子是，有时我们更希望惩罚错误分类或误报，而不是正确的命中或预测。在这种情况下，我们可能需要使用修改的成本函数或先验知识，以便我们可以牺牲命中率或整体准确性，以获得更准确的预测和更少的假阳性。一个真实世界的例子是一个智能系统，它预测我们是否应该向客户提供贷款。最好以这样一种方式来建立这个系统，它对贷款的反对要比拒绝贷款更加谨慎。原因很简单，因为与拒绝向潜在客户提供几笔小额贷款相比，向潜在违约者提供贷款的一个大错误可能会导致巨大损失。总之，您需要考虑 task T 中涉及的所有参数和属性，这样您就可以为您的系统决定正确的性能度量 P。

### 多学科领域

在上一节中，我们已经正式介绍并定义了机器学习，这应该会让您对任何学习算法所涉及的主要组件有一个很好的了解。现在让我们将视角转移到作为领域和领域的机器学习。你可能已经知道，从某些角度来看，机器学习大多被认为是人工智能甚至计算机科学的一个子领域。机器学习从诞生之日起，就有在一段时间内从多个领域衍生和借用的概念，使其成为真正的多学科或跨学科领域。图 [1-4](#Fig4) 应该让你对基于概念、方法、思想和技术的机器学习的主要重叠领域有一个好的想法。这里要记住的重要一点是，这绝对不是一个域或领域的详尽列表，而是描述了与机器学习相关的主要领域。

![A448827_1_En_1_Fig4_HTML.jpg](A448827_1_En_1_Fig4_HTML.jpg)

图 1-4。

Machine Learning: a true multi-disciplinary field

如图 [1-4](#Fig4) 所示，与机器学习相关的主要领域包括如下。我们将在接下来的章节中讨论这些字段。

*   人工智能
*   自然语言处理
*   数据挖掘技术
*   数学
*   统计数字
*   计算机科学
*   深度学习
*   数据科学

你可以说，数据科学就像一个广泛的跨学科领域，跨越所有其他领域，这些领域都是其中的子领域。当然，这只是一个简单的概括，并不严格地表明它包含所有其他领域作为一个超集，而是借用了它们的重要概念和方法。数据科学的基本思想也是从数据和领域知识中提取信息的过程、方法和技术。这是我们在下一节详细讨论数据科学时要讨论的一大部分。

回到机器学习，当关系数据库非常突出时，模式识别的思想和基本的数据挖掘方法如数据库的知识发现(KDD)就出现了。这些领域更侧重于从大型数据集中挖掘信息的能力和技术，这样您可以获得感兴趣的模式、知识和见解。当然，KDD 本身是一个完整的过程，包括数据采集、存储、入库、处理和分析。机器学习借用了与分析阶段更相关的概念，尽管你确实需要通过其他步骤才能到达最终阶段。数据挖掘也是一个跨学科或多学科领域，它借用了计算机科学、数学和统计学的概念。其结果是，计算统计学构成了大多数机器学习算法和技术的重要部分。

人工智能(AI)是由机器学习组成的超集，作为其专门领域之一。人工智能的基本思想是研究和开发机器所表现出的智能，这些智能基于它们对环境、输入参数和属性的感知以及它们的响应，以便它们能够根据预期执行所需的任务。人工智能本身是一个真正庞大的领域，它本身是跨学科的。它借鉴了数学、统计学、计算机科学、认知科学、语言学、神经科学以及更多的概念。机器学习更关注可用于理解数据、构建表示和执行预测等任务的算法和技术。人工智能下与机器学习相关的另一个主要子领域是自然语言处理(NLP)，它大量借鉴了计算语言学和计算机科学的概念。文本分析是当今分析师和数据科学家提取、处理和理解自然人类语言的重要领域。将自然语言处理与人工智能和机器学习结合起来，你会得到聊天机器人、机器翻译和虚拟个人助理，这些确实是创新和技术的未来！

说到深度学习，它是机器学习本身的一个子领域，它更多地处理与表征学习相关的技术，以便通过获得更多经验来改善越来越多的数据。它遵循分层和分级的方法，从而尝试使用概念表示的嵌套分层层次来表示给定的输入属性及其当前环境，这样，每个复杂层都是从另一个较简单的概念层构建的。神经网络是深度学习大量使用的东西，我们将在未来的章节中更详细地研究深度学习，并在本书的后面解决一些现实世界的问题。

计算机科学几乎是这些领域的基础，涉及计算机的研究、开发、工程和编程。因此，我们不会在这方面展开太多，但你一定要记住计算机科学对机器学习的重要性，机器学习可以很容易地应用于解决现实世界的问题。这应该让你对机器学习的多学科领域的广阔前景以及它如何跨越多个相关和重叠的领域有一个好的想法。我们将在接下来的章节中更详细地讨论其中的一些领域，并在必要时涵盖这些领域中的一些基本概念。

让我们在下一节看看计算机科学的一些核心基础知识。

## 计算机科学

计算机科学(CS)领域可以定义为理解计算机的科学研究。这包括学习、研究、开发、工程和实验，涉及理解、设计、建造和使用计算机。这也包括大量的算法和程序的设计和开发，这些算法和程序可以用来使计算机根据需要执行计算和任务。计算机科学主要有以下两个主要领域。

*   理论计算机科学
*   应用或实用计算机科学

计算机科学的两个主要领域跨越多个领域，其中每个领域都构成计算机科学的一部分或一个子领域。计算机科学的主要本质包括形式语言、自动机和计算理论、算法、数据结构、计算机设计和体系结构、编程语言和软件工程原理。

### 理论计算机科学

理论计算机科学是对理论和逻辑的研究，试图解释计算背后的原理和过程。这包括理解计算理论，该理论讨论如何有效地利用计算来解决问题。计算理论包括对形式语言、自动机以及理解计算和算法中的复杂性的研究。信息和编码理论是理论计算机科学的另一个主要领域，它为我们提供了信号处理、密码学和数据压缩等领域。编程语言原理及其分析是另一个重要的方面，它讲述了各种编程语言的特性、设计、分析和实现，以及编译器和解释器如何理解这些语言。最后但同样重要的是，数据结构和算法是广泛用于计算程序和函数的理论 CS 的两个基本支柱。

### 实用计算机科学

实用计算机科学(也称为应用计算机科学)更多地是关于工具、方法和过程，这些工具、方法和过程处理在现实世界中应用计算机科学的概念和原理来解决实际的日常问题。这包括新兴的子领域，如人工智能、机器学习、计算机视觉、深度学习、自然语言处理、数据挖掘和机器人，它们试图基于多种约束和参数来解决复杂的现实世界问题，并试图模拟需要大量人类智能和经验的任务。除此之外，我们还拥有成熟的领域，包括计算机体系结构、操作系统、数字逻辑和设计、分布式计算、计算机网络、安全、数据库和软件工程。

### 重要概念

这些是计算机科学中的几个概念，你应该知道并记住，因为它们是更好地理解其他章节、概念和例子的基础概念。这不是一个详尽的列表，但应该足够涵盖开始。

#### 算法

一个算法可以被描述为一系列的步骤、操作、计算或函数，它们可以被执行来完成一个特定的任务。它们基本上是通过一系列操作来正式描述和表示计算机程序的方法，通常使用简单的自然语言、数学符号和图表来描述。典型地，流程图、伪代码和自然语言被广泛用于表示算法。算法可以简单到将两个数相加，也可以复杂到计算矩阵的逆矩阵。

#### 编程语言

编程语言是一种有自己的一套符号、单词、标记和运算符的语言，这些符号、单词、标记和运算符有自己的意义。因此，句法和语义结合起来形成了一种形式语言。这种语言可以用来编写计算机程序，这些程序基本上是算法的现实世界实现，可以用来向计算机指定特定的指令，以便它进行我们必要的计算和操作。编程语言可以是低级语言，如 C 和汇编，也可以是高级语言，如 Java 和 Python。

#### 密码

这基本上是构成计算机程序基础的源代码。代码是使用编程语言编写的，由计算机语句和指令的集合组成，使计算机执行特定的预期任务。代码有助于使用编程语言将算法转换成程序。我们将使用 Python 来实现我们现实世界中的大多数机器学习解决方案。

#### 数据结构

数据结构是用于管理数据的专用结构。基本上，它们是抽象数据类型规范的真实实现，可用于高效地存储、检索、管理和操作数据。有一整套数据结构，如数组、列表、元组、记录、结构、联合、类等等。我们将广泛使用 Python 数据结构，如列表、数组、数据帧和字典来操作真实世界的数据！

## 数据科学

数据科学领域是一个非常多样化的跨学科领域，包括我们在图 [1-4](#Fig4) 中描述的多个领域。数据科学主要研究从数据(结构化和非结构化)中收集知识或信息的原理、方法、流程、工具和技术。数据科学更多的是流程、技术和方法的汇编，以培养基于数据驱动的决策文化。事实上，Drew Conway 的“数据科学维恩图”，如图 [1-5](#Fig5) 所示，展示了数据科学的核心组成部分和本质，事实上，它已经像病毒一样传播开来，变得非常流行！

![A448827_1_En_1_Fig5_HTML.jpg](A448827_1_En_1_Fig5_HTML.jpg)

图 1-5。

Drew Conway’s Data Science Venn diagram

图 [1-5](#Fig5) 相当直观，容易解读。基本上有三个主要组成部分，数据科学位于它们的交叉点。数学和统计知识是关于应用各种基于计算和量化数学和统计的技术从数据中提取洞察力。黑客技能基本上是指将数据处理、加工、操纵和争论成易于理解和分析的格式的能力。实质性专业知识基本上是实际的真实世界领域的专业知识，这在您解决问题时极其重要，因为除了您在数据和算法方面的专业知识之外，您还需要了解与该领域相关的各种因素、属性、约束和知识。

因此，德鲁正确地指出，机器学习是数据黑客技能、数学和统计学习方法方面的专业知识的结合。对于数据科学，除了机器学习之外，您还需要一定程度的领域专业知识和知识。您可以在德鲁的文章《在 T2》中了解到他的个人见解，文中谈到了数据科学维恩图。除此之外，我们还有布兰登·蒂尔尼，他用自己的方式讲述了数据科学是一个多学科领域的真实本质，如图 [1-6](#Fig6) 所示。

![A448827_1_En_1_Fig6_HTML.jpg](A448827_1_En_1_Fig6_HTML.jpg)

图 1-6。

Brendan Tierney's depiction of Data Science as a true multi-disciplinary field

如果你仔细观察他的描述，你会发现这里提到的许多领域就是我们在前面章节中刚刚谈到的，并且与图 [1-4](#Fig4) 的大部分相匹配。你可以清楚地看到数据科学是关注的中心，并从所有其他领域和机器学习中抽取部分作为子领域。

## 数学

数学领域涉及数字、逻辑和形式系统。亚里士多德对数学的最好定义是“数量的科学”。数学作为一个科学领域的范围是巨大的，跨越了包括代数、三角学、微积分、几何和数论在内的许多领域。线性代数和概率是机器学习中广泛使用的数学的两个主要子领域，我们将在这一节中涵盖其中的一些重要概念。我们的主要关注点将始终是实用的机器学习，而应用数学也是其中的一个重要方面。线性代数处理数学对象和结构，如向量、矩阵、直线、平面、超平面和向量空间。概率论是一个数学领域和框架，用于研究和量化偶然性和不确定性事件，并从中推导出定理和公理。这些定律和公理帮助我们推理、理解和量化不确定性及其在任何现实世界系统或场景中的影响，这有助于我们通过利用这一框架来建立我们的机器学习模型。

### 重要概念

在本节中，我们将讨论应用数学中的一些关键术语和概念，即线性代数和概率论。这些概念在机器学习中被广泛使用，并形成了机器学习算法、模型和过程的一些基本结构和原则。

#### 数量

标量通常表示一个数字，而不是一组数字。一个简单的例子可能是 x = 5 或 x ∈ R，其中 x 是指向单个数字或实数值单个数字的标量元素。

#### 矢量

向量被定义为一种保存有序排列的数字数组的结构。这基本上意味着集合中数字的顺序很重要。向量在数学上可以表示为 x = [x <sub>1</sub> ，x <sub>2</sub> ，…，x <sub>n</sub> ，基本上告诉我们 x 是一个一维向量，数组中有 n 个元素。可以使用数组索引来引用每个元素，确定它在向量中的位置。下面的代码片段向我们展示了如何用 Python 来表示简单的向量。

```py
In [1]: x = [1, 2, 3, 4, 5]
   ...: x
Out[1]: [1, 2, 3, 4, 5]
In [2]: import numpy as np
   ...: x = np.array([1, 2, 3, 4, 5])
   ...:
   ...: print(x)
   ...: print(type(x))
[1 2 3 4 5]
<class 'numpy.ndarray'>

```

因此，你可以看到 Python 列表以及基于`numpy`的数组可以用来表示向量。数据集中的每一行都可以作为 n 个属性的一维向量，作为学习算法的输入。

#### [数]矩阵

矩阵是一种二维结构，基本上包含数字。它也经常被称为 2D 阵列。与向量情况下的单个向量索引相比，可以使用行和列索引来引用每个元素。从数学上讲，您可以将矩阵描述为![$$ M=\left[\begin{array}{ccc}{m}_{11}& {m}_{12}& {m}_{13}\\ {}{m}_{21}& {m}_{22}& {m}_{23}\\ {}{m}_{31}& {m}_{32}& {m}_{33}\end{array}\right] $$](A448827_1_En_1_Chapter_IEq1.gif)，这样 M 是一个具有三行三列的 3×3 矩阵，每个元素由 m <sub>rc</sub> 表示，这样 r 表示行索引，c 表示列索引。在 Python 中，矩阵可以很容易地表示为列表的列表，我们可以利用下面代码片段中描述的`numpy`数组结构。

```py
In [3]: m = np.array([[1, 5, 2],
   ...:               [4, 7, 4],
   ...:               [2, 0, 9]])

In [4]: # view matrix
   ...: print(m)
[[1 5 2]
 [4 7 4]
 [2 0 9]]

In [5]: # view dimensions
   ...: print(m.shape)
(3, 3)

```

因此，你可以看到我们如何轻松地利用`numpy`数组来表示矩阵。您可以将包含行和列的数据集视为一个矩阵，这样数据特征或属性由列表示，每一行表示一个数据样本。我们将在后面的分析中使用同样的类比。当然，您可以执行矩阵运算，如加、减、积、求逆、转置、行列式等等。下面的代码片段展示了一些流行的矩阵运算。

```py
In [9]: # matrix transpose
   ...: print('Matrix Transpose:\n', m.transpose(), '\n')
   ...:
   ...: # matrix determinant
   ...: print ('Matrix Determinant:', np.linalg.det(m), '\n')
   ...:
   ...: # matrix inverse
   ...: m_inv = np.linalg.inv(m)
   ...: print ('Matrix inverse:\n', m_inv, '\n')
   ...:
   ...: # identity matrix (result of matrix x matrix_inverse)
   ...: iden_m =  np.dot(m, m_inv)
   ...: iden_m = np.round(np.abs(iden_m), 0)
   ...: print ('Product of matrix and its inverse:\n', iden_m)
   ...:
Matrix Transpose:
 [[1 4 2]
  [5 7 0]
  [2 4 9]]

Matrix Determinant: -105.0

Matrix inverse:
 [[-0.6         0.42857143 -0.05714286]
  [ 0.26666667 -0.04761905 -0.03809524]
  [ 0.13333333 -0.0952381   0.12380952]]

Product of matrix and its inverse:
 [[ 1\.  0\.  0.]
  [ 0\.  1\.  0.]
  [ 0\.  0\.  1.]]

```

这应该给你一个很好的开始矩阵及其基本操作的想法。这方面的更多内容将在第 2 章“Python 机器学习生态系统”中介绍。

#### 张量

你可以把张量想象成一个普通的数组。张量基本上是轴数可变的数组。三维张量 T 中的元素可以由 T <sub>x，y，z</sub> 表示，其中 x，y，z 表示用于指定元素 T 的三个轴

#### 标准

范数是用于计算向量大小的度量，通常也被定义为从原点到向量所表示的点的距离的度量。数学上，向量的 pth 范数表示如下。

![$$ {L}^p={\left\Vert x\right\Vert}_p={\left(\sum \limits_i{\left|{x}_i\right|}^p\right)}^{\frac{1}{p}} $$](A448827_1_En_1_Chapter_Equa.gif)

使得 p ≥ 1 且 p ∈ R .机器学习中的流行范数包括在 Lasso 回归模型中广泛使用的 L <sup>1</sup> 范数和在岭回归模型中使用的 L <sup>2</sup> 范数，也称为欧几里德范数。

#### 特征分解

这基本上是一个矩阵分解过程，我们将矩阵分解成一组特征向量和特征值。矩阵的特征分解在数学上可以表示为 M = V diag(λ) V <sup>-1</sup> ，使得矩阵 M 总共有 n 个线性独立的特征向量，表示为{v <sup>(1)</sup> ，v <sup>(2)</sup> ，…，v <sup>(n)</sup> }，并且它们对应的特征值可以表示为{λ <sub>1</sub> ，λ <sub>2</sub> ，…，λ <sub>n 【T13 矩阵 V 由矩阵每列的一个本征向量组成，即 V = [v <sup>(1)</sup> ，v <sup>(2)</sup> ，…，v <sup>(n)</sup> ，向量λ由所有本征值组成，即λ = [λ <sub>1</sub> ，λ <sub>2</sub> ，…，λ <sub>n</sub> 。</sub>

矩阵的特征向量被定义为非零向量，使得在矩阵乘以特征向量时，结果仅改变特征向量本身的比例，即，结果是标量乘以特征向量。这个标量被称为对应于本征矢量的本征值。数学上，这可以表示为 Mv = λv，其中 M 是我们的矩阵，v 是本征向量，λ是相应的本征值。以下 Python 片段描述了如何从矩阵中提取特征值和特征向量。

```py
In [4]: # eigendecomposition
   ...: m = np.array([[1, 5, 2],
   ...:               [4, 7, 4],
   ...:               [2, 0, 9]])
   ...:
   ...: eigen_vals, eigen_vecs = np.linalg.eig(m)
   ...:
   ...: print('Eigen Values:', eigen_vals, '\n')
   ...: print('Eigen Vectors:\n', eigen_vecs)
   ...:
Eigen Values: [ -1.32455532  11.32455532   7\.        ]

Eigen Vectors:
 [[-0.91761521  0.46120352 -0.46829291]
  [ 0.35550789  0.79362022 -0.74926865]
  [ 0.17775394  0.39681011  0.46829291]]

```

#### 奇异值分解

奇异值分解过程，也称为 SVD，是另一种矩阵分解或因式分解过程，这样我们能够分解矩阵以获得奇异向量和奇异值。任何实矩阵总是会被 SVD 分解，即使特征分解在某些情况下可能不适用。数学上，SVD 可以定义如下。考虑具有 M×n 维的矩阵 M，使得 M 表示全部行，n 表示全部列，矩阵的 SVD 可以用下面的等式表示。

![$$ {M}_{m\times n}={U}_{m\times m}\;{S}_{m\times n}\;{V^T}_{n\times n} $$](A448827_1_En_1_Chapter_Equb.gif)T2】

这为我们提供了分解方程的以下主要组成部分。

*   u<sub>m×m</sub>是一个 m×m 酉矩阵，其中每一列代表一个左奇异向量
*   s<sub>m×n</sub>是对角线上有正数的 m×n 矩阵，也可以表示为奇异值的向量
*   v<sup>T</sup><sub>n×n</sub>是一个 n×n 酉矩阵，其中每一行代表一个右奇异向量

在某些表示中，行和列可以互换，但最终结果应该是相同的，即 U 和 V 总是正交的。下面的代码片段展示了 Python 中一个简单的 SVD 分解。

```py
In [7]: # SVD
   ...: m = np.array([[1, 5, 2],
   ...:               [4, 7, 4],
   ...:               [2, 0, 9]])
   ...:
   ...: U, S, VT = np.linalg.svd(m)
   ...:
   ...: print('Getting SVD outputs:-\n')
   ...: print('U:\n', U, '\n')
   ...: print('S:\n', S, '\n')
   ...: print('VT:\n', VT, '\n')
   ...:
Getting SVD outputs:-

U:
 [[ 0.3831556  -0.39279153  0.83600634]
  [ 0.68811254 -0.48239977 -0.54202545]
  [ 0.61619228  0.78294653  0.0854506 ]]

S:
 [ 12.10668383   6.91783499   1.25370079]

VT:
 [[ 0.36079164  0.55610321  0.74871798]
  [-0.10935467 -0.7720271   0.62611158]
  [-0.92621323  0.30777163  0.21772844]]

```

SVD 作为一种技术，特别是奇异值，在基于摘要的算法和各种其他方法(如降维)中非常有用。

#### 随机变量

随机变量常用于概率和不确定性测量，它基本上是一个可以随机取各种值的变量。一般来说，这些变量可以是离散的，也可以是连续的。

#### 概率分布

概率分布是描述一个或多个随机变量呈现其每种可能状态的可能性的分布或排列。基于变量是离散的还是连续的，通常有两种主要的分布类型。

#### 概率质量函数

概率质量函数，也称为 PMF，是离散随机变量的概率分布。常见的例子包括泊松和二项式分布。

#### 概率密度函数

概率密度函数，也称为 PDF，是连续随机变量的概率分布。常见的例子包括正态分布、均匀分布和学生 t 分布。

#### 边缘概率

当我们已经有了一组随机变量的概率分布，并且想要计算这些随机变量的子集的概率分布时，就使用边际概率规则。对于离散随机变量，我们可以定义边际概率如下。

![$$ P(x)=\sum \limits_yP\left(x,y\right) $$](A448827_1_En_1_Chapter_Equc.gif)

对于连续的随机变量，我们可以用如下的积分运算来定义它。

![$$ p(x)=\int p\left(x,y\right) dy $$](A448827_1_En_1_Chapter_Equd.gif)

#### 条件概率

当我们想要确定一个事件将要发生的概率，例如另一个事件已经发生时，就使用条件概率规则。这在数学上表示如下。

![$$ P\left(x\;\left|\;y\right.\right)=\frac{P\left(x,y\right)}{P(y)} $$](A448827_1_En_1_Chapter_Eque.gif)

这告诉我们 x 的条件概率，假设 y 已经发生了。

#### 贝叶斯定理

这是另一个规则或定理，当我们知道感兴趣的事件的概率 P(A)，基于我们感兴趣的事件的另一个事件的条件概率 P(B | A)并且我们想要确定我们感兴趣的事件的条件概率 P(A | B)时，这个规则或定理是有用的。这可以使用下面的表达式进行数学定义。

![$$ P\left(A\;\left|\;B\right.\;\right)=\frac{P\left(B\;\left|\;A\right.\right)\;P(A)}{P(B)} $$](A448827_1_En_1_Chapter_Equf.gif)

使得 A 和 B 是事件而![$$ P(B)={\sum}_xP\left(B\;\left|\;A\right.\right)\;P(A) $$](A448827_1_En_1_Chapter_IEq2.gif)。

## 统计数字

统计学领域可以定义为数学的一个专门分支，由收集、组织、分析、解释和呈现数据的框架和方法组成。一般来说，这更多地属于应用数学，并从线性代数、分布、概率论和推理方法论中借用概念。统计中有两个主要领域，如下所述。

*   描述统计学
*   推论统计

任何统计过程的核心组成部分是数据。因此，通常首先进行数据收集，这可以是全局术语，通常称为总体或由于各种限制而更受限制的子集，通常称为样本。样本通常是从调查、实验、数据存储和观察研究中手工收集的。根据这些数据，使用统计方法进行各种分析。

描述性统计用于理解数据的基本特征，使用各种聚合和汇总方法来更好地描述和理解数据。这些可以是标准度量，如均值、中值、众数、偏度、峰度、标准差、方差等。如果你感兴趣，你可以参考任何关于统计的标准书籍来深入研究这些方法。下面的代码片段描述了如何计算一些基本的描述性统计指标。

```py
In [74]: # descriptive statistics
    ...: import scipy as sp
    ...: import numpy as np
    ...:
    ...: # get data
    ...: nums = np.random.randint(1,20, size=(1,15))[0]
    ...: print('Data: ', nums)
    ...:
    ...: # get descriptive stats
    ...: print ('Mean:', sp.mean(nums))
    ...: print ('Median:', sp.median(nums))
    ...: print ('Mode:', sp.stats.mode(nums))
    ...: print ('Standard Deviation:', sp.std(nums))
    ...: print ('Variance:', sp.var(nums))
    ...: print ('Skew:', sp.stats.skew(nums))
    ...: print ('Kurtosis:', sp.stats.kurtosis(nums))
    ...:
Data:  [ 2 19  8 10 17 13 18  9 19 16  4 14 16 15  5]
Mean: 12.3333333333
Median: 14.0
Mode: ModeResult(mode=array([16]), count=array([2]))
Standard Deviation: 5.44875113112
Variance: 29.6888888889
Skew: -0.49820055879944575
Kurtosis: -1.0714842769550714

```

像`pandas`、`scipy`和`numpy`这样的库和框架通常可以帮助我们在 Python 中轻松计算描述性统计数据和汇总数据。我们将在第 [2](02.html) 和 [3](03.html) 章中介绍这些框架以及基本的数据分析和可视化。

当我们想要检验假设、得出推论和关于我们的数据样本或总体的各种特征的结论时，使用推断统计学。假设检验、相关性、回归分析、预测等框架和技术通常用于任何形式的推断统计。在后续章节中，当我们讨论预测分析以及基于时间序列的预测时，我们会更详细地讨论这一点。

## 数据挖掘

数据挖掘领域涉及从非平凡数据集中发现和提取模式、知识、见解和有价值信息的过程、方法、工具和技术。当数据集相当大时，它们被定义为非平凡的，通常可从数据库和数据仓库中获得。同样，数据挖掘本身是一个多学科领域，融合了数学、统计学、计算机科学、数据库、机器学习和数据科学的概念和技术。这个术语通常是用词不当，因为“挖掘”指的是从数据中挖掘实际的见解或信息，而不是数据本身！在数据库的 KDD 或知识发现的整个过程中，数据挖掘是所有分析发生的步骤。

一般来说，KDD 和数据挖掘都与机器学习密切相关，因为它们都涉及分析数据以提取有用的模式和见解。因此，他们共享方法、概念、技术和过程。业内遵循的数据挖掘标准过程被称为 CRISP-DM 模型，我们将在本章的下一节详细讨论该模型。

## 人工智能

人工智能领域包含多个子领域，包括机器学习、自然语言处理、数据挖掘等等。它可以被定义为制造智能代理、机器和程序的艺术、科学和工程。该领域旨在为一个简单但极其困难的目标提供解决方案，“机器能像人类一样思考、推理和行动吗？”人工智能实际上早在 13 世纪就存在了，当时人们开始问这样的问题，并进行研究和开发建立工具，这些工具可以像计算器一样处理概念而不是数字。随着艾伦·图灵、麦卡鲁什和皮茨人工神经元的发现和发明，人工智能的进展稳步前进。人工智能在经历了一段低迷期后，随着专家系统的成功，神经网络的复兴，由于霍普菲尔德、鲁梅尔哈特、麦克莱兰、辛顿以及更多人的努力，终于在 20 世纪 80 年代再次复兴。由于摩尔定律，更快更好的计算导致了数据挖掘、机器学习甚至深度学习等领域的出现，以解决使用传统方法无法解决的复杂问题。图 [1-7](#Fig7) 显示了人工智能大伞下的一些主要方面。

![A448827_1_En_1_Fig7_HTML.jpg](A448827_1_En_1_Fig7_HTML.jpg)

图 1-7。

Diverse major facets under the AI umbrella

人工智能的一些主要目标包括认知功能的仿真，也称为认知学习、语义和知识表示、学习、推理、问题解决、规划和自然语言处理。AI 从统计学习、应用数学、优化方法、逻辑、概率论、机器学习、数据挖掘、模式识别和语言学中借用工具、概念和技术。随着时间的推移，人工智能仍在不断发展，该领域正在进行大量创新，包括一些最新的发现和发明，如自动驾驶汽车、聊天机器人、无人机和智能机器人。

## 自然语言处理

自然语言处理领域是一个结合了计算语言学、计算机科学和人工智能概念的多学科领域。NLP 包括让机器处理、理解自然人类语言并与之交互的能力。使用 NLP 构建的应用程序或系统的主要目标是实现机器和自然语言之间的交互，这种交互是随着时间的推移而发展的。这方面的主要挑战包括知识和语义表示、自然语言理解、生成和处理。NLP 的一些主要应用如下所述。

*   机器翻译
*   语音识别
*   问答系统
*   上下文识别和解析
*   文本摘要
*   文本分类
*   信息提取
*   情绪和情感分析
*   话题分割

使用 NLP 和文本分析的技术，您可以对文本数据进行处理、注释、分类、聚类、总结、提取语义、确定情感等等！下面的示例片段描述了对文本数据的一些基本 NLP 操作，其中我们根据文档的成分语法，用各种组件(如词性、短语级标签等)来注释文档(文本句子)。可以参考《用 Python 进行文本分析》159 页(ApressDipanjan Sarkar，2016)了解更多关于选区解析的详细信息。

![A448827_1_En_1_Fig8_HTML.jpg](A448827_1_En_1_Fig8_HTML.jpg)

图 1-8。

Constituency parse tree for our sample sentence

```py
from nltk.parse.stanford import StanfordParser

sentence = 'The quick brown fox jumps over the lazy dog'

# create parser object
scp = StanfordParser(path_to_jar='E:/stanford/stanford-parser-full-2015-04-20/stanford-parser.jar',
                   path_to_models_jar='E:/stanford/stanford-parser-full-2015-04-20/stanford-parser-3.5.2-models.jar')

# get parse tree
result = list(scp.raw_parse(sentence))
tree = result[0]

In [98]: # print the constituency parse tree
    ...: print(tree)
(ROOT
  (NP
    (NP (DT The) (JJ quick) (JJ brown) (NN fox))
    (NP
      (NP (NNS jumps))
      (PP (IN over) (NP (DT the) (JJ lazy) (NN dog))))))

In [99]: # visualize constituency parse tree
    ...: tree.draw()

```

因此，您可以清楚地看到，图 [1-8](#Fig8) 描绘了我们的示例句子的基于成分语法的解析树，它由多个名词短语(NP)组成。每个短语都有几个单词，这些单词也用它们自己的词性(POS)标签进行了注释。我们将在后续章节中更多地介绍机器学习流程中各个步骤的文本数据处理和分析，以及实际的使用案例。

## 深度学习

如前所述，深度学习领域是机器学习的一个子领域，最近变得非常突出。它的主要目标是让机器学习研究更接近其真正的目标“让机器智能化”。深度学习通常被称为神经网络的一个更名的花哨术语。这在某种程度上是正确的，但深度学习肯定不仅仅是基本的神经网络。基于深度学习的算法涉及使用来自表示学习的概念，其中在不同层中学习数据的各种表示，这也有助于从数据中自动提取特征。简单来说，基于深度学习的方法试图通过将数据表示为分层的概念层次来构建机器智能，其中每一层概念都是从其他更简单的层构建的。这种分层架构本身就是任何深度学习算法的核心组件之一。

在任何基本的监督机器学习技术中，我们基本上都试图学习数据样本和输出之间的映射，然后试图预测新数据样本的输出。除了学习从输入到输出的映射，表征学习还试图理解数据本身的表征。这使得深度学习算法与常规技术相比极其强大，常规技术需要在特征提取和工程等领域的大量专业知识。与旧的机器学习算法相比，深度学习在越来越多的数据下的性能和可扩展性方面也非常有效。图 [1-9](#Fig9) 根据吴恩达在 Extract Data 会议上的发言幻灯片对此进行了描述。

![A448827_1_En_1_Fig9_HTML.jpg](A448827_1_En_1_Fig9_HTML.jpg)

图 1-9。

Performance comparison of Deep Learning and traditional Machine Learning by Andrew Ng

事实上，正如吴恩达正确指出的那样，在过去十年中，我们已经注意到了与深度学习相关的几个值得注意的趋势和特征。它们总结如下。

*   深度学习算法基于分布式表示学习，随着时间的推移，随着数据的增加，它们开始表现得更好。
*   深度学习可以说是神经网络的重塑，但与传统的神经网络相比，它有很多东西。
*   更好的软件框架，如`tensorflow`、`theano`、`caffe`、`mxnet`和`keras`，再加上优越的硬件，使得构建极其复杂、多层、规模巨大的深度学习模型成为可能。
*   深度学习具有与自动特征提取以及执行监督学习操作相关的多种优势，这些优势已经帮助数据科学家和工程师解决了日益复杂的问题。

以下几点描述了大多数深度学习算法的显著特征，其中一些我们将在本书中使用。

*   概念的分级分层表示。这些概念在机器学习术语中也称为特征(数据属性)。
*   数据的分布式表示学习通过多层架构(无监督学习)进行。
*   更复杂和更高级的特性和概念是从更简单、更低级的特性中派生出来的。
*   除了输入和输出层之外,“深”神经网络通常被认为至少具有一个以上的隐藏层。通常它至少包含三到四个隐藏层。
*   深度架构具有多层架构，其中每层由多个非线性处理单元组成。每一层的输入都是体系结构中的前一层。第一层通常是输入，最后一层是输出。
*   可以执行自动特征提取、分类、异常检测和许多其他机器学习任务。

这应该会让你对深度学习的相关概念有一个很好的基础掌握。假设我们有一个从图像中识别物体的现实问题。图 [1-10](#Fig10) 将让我们很好地了解典型的机器学习和深度学习管道有什么不同(来源:Yann LeCun)。

![A448827_1_En_1_Fig10_HTML.jpg](A448827_1_En_1_Fig10_HTML.jpg)

图 1-10。

Comparing various learning pipelines by Yann LeCun

你可以清楚地看到，与其他机器学习方法相比，深度学习方法如何涉及来自原始数据的特征和概念的分层表示。我们以一些与深度学习相关的基本概念的简要覆盖来结束这一部分。

### 重要概念

在本节中，我们讨论深度学习算法和架构中的一些关键术语和概念。这在将来你构建自己的深度学习模型时应该会很有用。

#### 人工神经网络

人工神经网络(ANN)是一种模拟生物神经元及其在我们大脑中的功能的计算模型和架构。典型地，人工神经网络具有多层互连的节点。这些节点及其相互连接类似于我们大脑中的神经元网络。一个典型的人工神经网络有一个输入层，一个输出层，以及至少一个在输入和输出之间的隐藏层，并有相互连接，如图 [1-11](#Fig11) 所示

![A448827_1_En_1_Fig11_HTML.jpg](A448827_1_En_1_Fig11_HTML.jpg)

图 1-11。

A typical artificial neural network

任何基本的人工神经网络总是具有多层节点、各层之间的特定连接模式和链接、连接权重以及用于将加权输入转换为输出的节点/神经元的激活函数。网络的学习过程通常涉及成本函数，目标是优化成本函数(通常最小化成本)。权重在学习过程中不断更新。

#### 反向传播

反向传播算法是一种训练人工神经网络的流行技术，它导致了 20 世纪 80 年代神经网络的重新流行。该算法通常有两个主要阶段——传播和权重更新。它们简述如下。

1.  传播
    1.  输入数据样本向量通过神经网络向前传播，以从输出层生成输出值。
    2.  将生成的输出向量与该输入数据向量的实际/期望输出向量进行比较。
    3.  计算输出单元的误差差。
    4.  反向传播误差值以在每个节点/神经元产生增量。 
2.  重量更新
    1.  通过将输出增量(误差)和输入激活相乘来计算重量梯度。
    2.  使用学习率来确定要从原始权重中减去的梯度百分比，并更新节点的权重。 

这两个阶段通过多次迭代/历元重复多次，直到我们获得满意的结果。典型地，反向传播与优化算法或类似随机梯度下降的函数一起使用。

#### 多层感知器

多层感知器，也称为 MLP，是一个完全连接的前馈人工神经网络，至少有三层(输入层、输出层和至少一个隐藏层)，其中每层都与相邻层完全连接。每个神经元通常是一个非线性功能处理单元。反向传播通常用于训练 MLP，甚至深度神经网络在具有多个隐藏层时也是 MLP。通常用于监督机器学习任务，如分类。

#### 卷积神经网络

卷积神经网络，也称为 convnet 或 CNN，是人工神经网络的一种变体，专门模拟我们视觉皮层的功能和行为。CNN 通常由以下三部分组成。

*   多个卷积层，由多个滤波器组成，这些滤波器通过基本上计算点积来给出二维激活图，从而在输入数据(例如，图像原始像素)的高度和宽度上进行卷积。将所有的映射叠加到所有的滤波器上，我们最终得到了卷积层的最终输出。
*   池层，基本上是执行非线性下采样的层，以减少输入大小和来自卷积层输出的参数数量，从而进一步概化模型，防止过拟合并减少计算时间。过滤器检查输入的高度和宽度，并通过求和、求平均值或最大值等聚合来减少输入。典型的池组件是平均池或最大池。
*   完全连接的 MLPs，可执行图像分类和对象识别等任务。

具有所有组件的典型 CNN 架构如图 [1-12](#Fig12) 所示，这是一个 LeNet CNN 模型(来源:deeplearning.net)

![A448827_1_En_1_Fig12_HTML.jpg](A448827_1_En_1_Fig12_HTML.jpg)

图 1-12。

LeNet CNN model (Source: deeplearning.net)

#### 递归神经网络

递归神经网络，也称为 RNN，是一种特殊类型的人工神经网络，它允许通过使用一种特殊类型的循环结构来保存基于过去知识的信息。它们被大量用于与序列数据相关的领域，如预测句子的下一个单词。这些环形网络被称为循环网络，因为它们对输入数据序列中的每个元素执行相同的操作和计算。rnn 具有帮助从过去的序列中捕获信息的记忆。图 [1-13](#Fig13) (来源:Colah 在 [`http://colah.github.io/posts/2015-08-Understanding-LSTMs/`](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) 的博客)显示了一个 RNN 的典型结构，以及它是如何根据输入序列长度在任意时间点展开网络的。

![A448827_1_En_1_Fig13_HTML.jpg](A448827_1_En_1_Fig13_HTML.jpg)

图 1-13。

A recurrent neural network (Source: Colah's Blog)

图 [1-13](#Fig13) 清楚地描述了展开的网络如何在输入数据的每次传递中接受长度为 t 的序列，并对其进行操作。

#### 长短期记忆网络

rnn 擅长处理基于序列的数据，但是随着序列开始增加，它们开始随着时间在序列中失去历史背景，因此输出并不总是所期望的。这就是长短期记忆网络，通常被称为 LSTMs，出现的原因！由 Hochreiter & Schmidhuber 在 1997 年提出，LSTMs 可以记住来自真正基于长序列的数据的信息，并防止类似消失梯度问题的问题，这种问题通常发生在用反向传播训练的神经网络中。LSTMs 通常由三个或四个门组成，包括输入、输出和一个特殊的遗忘门。图 [1-14](#Fig14) 显示了单个 LSTM 细胞的高级图示。

![A448827_1_En_1_Fig14_HTML.jpg](A448827_1_En_1_Fig14_HTML.jpg)

图 1-14。

An LSTM cell (Source: deeplearning.net)

输入门通常可以允许或拒绝输入信号或输入来改变存储单元状态。输出门通常会根据需要将值传播给其他神经元。遗忘门控制存储单元的自循环连接，以根据需要记住或忘记先前的状态。多个 LSTM 细胞通常堆叠在任何深度学习网络中，以解决序列预测等现实世界的问题。

#### 自动编码器

自动编码器是一种专门的人工神经网络，主要用于执行无监督的机器学习任务。它的主要目标是学习数据表示、近似和编码。自动编码器可用于构建生成模型、执行维度缩减和检测异常。

## 机器学习方法

机器学习有多种算法、技术和方法，可用于建立模型，使用数据解决现实世界的问题。本节尝试将这些机器学习方法分为几大类，以对机器学习方法的整体前景有所了解，这些方法最终用于执行我们在上一节中讨论的特定机器学习任务。通常，相同的机器学习方法可以在多个保护伞下以多种方式分类。以下是机器学习方法的一些主要领域。

1.  基于学习过程中人为监督量的方法
    1.  监督学习
    2.  无监督学习
    3.  半监督学习
    4.  强化学习 
2.  基于从增量数据样本中学习能力的方法
    1.  批量学习
    2.  在线学习 
3.  基于数据样本归纳的方法
    1.  基于实例的学习
    2.  基于模型的学习 

我们将在下面的章节中简要介绍各种类型的学习方法，以便为机器学习方法及其通常解决的任务类型打下良好的基础。当我们在本书的后续章节中处理各种真实世界的用例和问题时，这应该给你足够的知识来开始理解在什么场景中应该应用什么方法。

讨论每一个机器学习算法的数学细节和内部原理超出了本书当前的范围和意图，因为重点更多的是通过应用机器学习来解决现实世界的问题，而不是理论上的机器学习。因此，我们鼓励您参考标准的机器学习参考资料，如 Christopher Bishop，2006 年出版的《模式识别和机器学习》,以及 Robert Tibshirani 等人，2001 年出版的《统计学习的要素》,以了解更多关于机器学习算法和方法内部的理论和数学细节。

## 监督学习

监督学习方法或算法包括在模型训练过程中接收数据样本(称为训练数据)和每个数据样本的相关输出(称为标签或响应)的学习算法。主要目的是基于多个训练数据实例来学习输入数据样本 x 和它们相应的输出 y 之间的映射或关联。然后，这种学习到的知识可以在将来用于预测任何新的输入数据样本 x’的输出 y’，该输入数据样本 x’在模型训练过程中以前是未知的或看不见的。这些方法被称为受监督的，因为模型在数据样本上学习，其中期望的输出响应/标签在训练阶段已经预先知道。

监督学习基本上试图从训练数据中对输入和它们相应的输出之间的关系进行建模，以便我们能够基于它先前获得的关于输入和它们的目标输出之间的关系和映射的知识来预测新数据输入的输出响应。这正是监督学习方法被广泛用于预测分析的原因，预测分析的主要目标是预测一些输入数据的一些响应，这些输入数据通常被输入到经过训练的监督 ML 模型中。基于它们旨在解决的 ML 任务的类型，监督学习方法有两大类。

*   分类
*   回归

让我们看看这两个机器学习任务，并观察最适合处理这些任务的监督学习方法的子集。

### 分类

基于分类的任务是监督机器学习下的一个子领域，其关键目标是基于模型在训练阶段所学习的内容来预测输入数据的本质上分类的输出标签或响应。这里的输出标签也称为类或类标签，它们本质上是分类的，意味着它们是无序的离散值。因此，每个输出响应属于特定的离散类或类别。

假设我们举一个预测天气的真实例子。简单来说，我们试图根据由湿度、温度、压力和降水等属性或特征组成的多个输入数据样本来预测天气是晴天还是雨天。由于预测可以是晴天也可以是雨天，所以总共有两个截然不同的类别；因此，这个问题也可以称为二元分类问题。图 [1-15](#Fig15) 描述了基于对输入数据样本训练监督模型来预测天气是晴天还是雨天的二元天气分类任务，该输入数据样本具有每个数据样本/观测的特征向量(降水、湿度、压力和温度)以及它们对应的分类标签是晴天还是雨天。

![A448827_1_En_1_Fig15_HTML.jpg](A448827_1_En_1_Fig15_HTML.jpg)

图 1-15。

Supervised learning: binary classification for weather prediction

不同类别的总数超过两个的任务变成多类别分类问题，其中每个预测响应可以是该集合中的任何一个可能类别。一个简单的例子是试图从扫描的手写图像中预测数字。在这种情况下，它变成了一个 10 类分类问题，因为任何图像的输出类标签可以是从 0 - 9 的任何数字。在这两种情况下，输出类都是指向一个特定类的标量值。多标签分类任务基于任何输入数据样本，输出响应通常是具有一个或多个输出类别标签的向量。一个简单的现实问题是试图预测一篇新闻文章的类别，该文章可能有多个输出类，如新闻、金融、政治等。

流行的分类算法包括逻辑回归、支持向量机、神经网络、像随机森林和梯度推进这样的集成、K-最近邻、决策树等等。

### 回归

以价值评估为主要目标的机器学习任务可以称为回归任务。基于回归的方法是在输入数据样本上训练的，输入数据样本具有连续数值的输出响应，这与分类不同，在分类中，我们具有离散的类别或类。回归模型利用输入数据属性或特征(也称为解释变量或自变量)及其相应的连续数字输出值(也称为响应变量、因变量或结果变量)来了解输入与其相应输出之间的特定关系和关联。有了这些知识，它可以预测新的、看不见的数据实例的输出响应，类似于分类，但具有连续的数字输出。

现实世界中最常见的回归例子之一是房价预测。您可以建立一个简单的回归模型，根据与以平方英尺为单位的地块面积相关的数据来预测房价。图 [1-16](#Fig16) 显示了基于不同方法的两种可能的回归模型，以地块面积预测房价。

![A448827_1_En_1_Fig16_HTML.jpg](A448827_1_En_1_Fig16_HTML.jpg)

图 1-16。

Supervised learning: regression models for house price prediction

这里的基本思想是，我们试图确定数据特征地块面积和结果变量(即房价，也是我们要预测的)之间是否存在任何关系或关联。因此，一旦我们了解了图 [1-16](#Fig16) 中描绘的这种趋势或关系，我们就可以预测任何给定地块的未来房价。如果您已经密切注意到该图，我们故意描绘了两种类型的模型，以表明可以有多种方法来基于您的训练数据构建模型。主要目标是在训练和验证模型的过程中最大限度地减少误差，使其能够很好地泛化，不会过度拟合或只偏向于训练数据，并且在未来的预测中表现良好。

简单线性回归模型尝试使用一个特征或解释变量 x 和一个响应变量 y 对数据关系进行建模，其目标是预测 y。普通最小二乘法(OLS)等方法通常用于在模型训练期间获得最佳线性拟合。

多元回归也称为多变量回归。这些方法试图对数据进行建模，其中我们在每个观察值中有一个响应输出变量 y，但有多个向量 X 形式的解释变量，而不是单个解释变量。我们的想法是基于 x 中的不同特征来预测 y。现实世界的一个例子是扩展我们的房屋预测模型，以构建一个更复杂的模型，其中我们基于多个特征来预测房价，而不仅仅是每个数据样本中的地块面积。这些特征可以在向量中表示为地块面积、卧室数量、浴室数量、总楼层、带家具或不带家具。基于所有这些属性，该模型试图学习每个特征向量与其对应的房价之间的关系，以便在未来预测它们。

多项式回归是多元回归的一种特殊情况，其中响应变量 y 被建模为输入要素 x 的 n 次多项式。基本上它是多元回归，其中输入要素向量中的每个要素都是 x 的倍数。图 [1-16](#Fig16) 中右侧预测房价的模型是 2 次多项式模型。

非线性回归方法试图基于应用于输入要素的非线性函数和必要的模型参数的组合来对输入要素和输出之间的关系进行建模。

Lasso 回归是一种特殊形式的回归，它通过执行正则化以及要素或变量选择来执行正态回归和概化模型。Lasso 代表最小绝对收缩和选择运算符。L1 范数通常用作 lasso 回归中的正则化项。

岭回归是另一种特殊形式的回归，它执行正态回归，并通过执行正则化来概化模型，以防止模型过度拟合。通常，L2 范数被用作岭回归中的正则项。

广义线性模型是通用框架，可用于对预测不同类型输出响应的数据进行建模，包括连续、离散和有序数据。逻辑回归等算法用于分类数据，有序概率单位回归用于有序数据。

## 无监督学习

监督学习方法通常需要一些训练数据，其中我们试图预测的结果已经以离散标签或连续值的形式可用。然而，我们通常没有自由或优势拥有预先标记的训练数据，我们仍然希望从我们的数据中提取有用的见解或模式。在这种情况下，无监督学习方法是非常强大的。这些方法被称为无监督的，因为模型或算法试图从给定的数据中学习内在的潜在结构、模式和关系，而无需任何帮助或监督，如以标记的输出或结果的形式提供注释。

无监督学习更关心的是试图从数据中提取有意义的见解或信息，而不是试图根据以前可用的监督训练数据来预测一些结果。无监督学习的结果有更多的不确定性，但你也可以从这些模型中获得许多信息，这些信息以前无法通过查看原始数据来查看。通常，无监督学习可能是构建大型智能系统的任务之一。例如，我们可以通过使用英语词汇的知识，使用无监督学习来获得推特情绪的可能结果标签，然后在类似的数据点及其结果上训练一个监督模型，这些数据点和结果是我们之前通过无监督学习获得的。关于只使用一种特定的技术，没有硬性的规则。只要与解决问题相关，你总是可以结合多种方法。无监督学习方法可以分为以下几类:与无监督学习相关的 ML 任务。

*   使聚集
*   降维
*   异常检测
*   关联规则挖掘

我们将在下面的章节中简要探讨这些任务，以便更好地了解无监督学习方法在现实世界中是如何使用的。

### 使聚集

聚类方法是机器学习方法，它试图在我们的数据集中找到数据样本之间的相似性和关系的模式，然后将这些样本聚类成不同的组，使得每个数据样本组或聚类基于固有属性或特征具有一些相似性。这些方法是完全无人监督的，因为它们试图通过查看数据特征来聚类数据，而没有任何事先训练、监督或关于数据属性、关联和关系的知识。

考虑一个现实世界中的问题，在一个数据中心运行多台服务器，并尝试分析日志中的典型问题或错误。我们的主要任务是确定通常每周频繁出现的各种日志消息。简而言之，我们希望根据一些固有的特征将日志消息分组到不同的集群中。一种简单的方法是从文本格式的日志消息中提取特征，并对其应用聚类，并基于内容的相似性将相似的日志消息分组在一起。图 [1-17](#Fig17) 显示了集群将如何解决这个问题。基本上，我们从原始日志消息开始。我们的聚类系统将使用特征提取来从文本中提取特征，如单词出现、短语出现等等。最后，像 K-means 或层次聚类这样的聚类算法将被用来基于消息的内在特征的相似性对消息进行分组或聚类。

![A448827_1_En_1_Fig17_HTML.jpg](A448827_1_En_1_Fig17_HTML.jpg)

图 1-17。

Unsupervised learning: clustering log messages

从图 [1-17](#Fig17) 中可以清楚地看到，我们的系统有三组不同的日志消息，第一组描述了磁盘问题，第二组是内存问题，第三组是处理器问题。图中还描述了有助于区分聚类和将相似的数据样本(日志)分组在一起的主要特征词。当然，有时一些特征可能出现在多个数据样本中，因此由于这是无监督学习，所以聚类也可能有轻微的重叠。然而，主要目标总是创建聚类，使得每个聚类的元素彼此靠近，而与其他聚类的元素远离。

有各种类型的聚类方法，可以根据以下主要方法进行分类。

*   基于质心的方法，例如 K-均值和 K-中值点
*   分层聚类方法，如凝聚和分裂(沃德的，密切传播)
*   基于分布的聚类方法，如高斯混合模型
*   基于密度的方法，如 dbscan 和光学。

除此之外，我们还有一些最近进入集群领域的方法，比如`birch`和`clarans`。

### 降维

一旦我们开始从原始数据样本中提取属性或特征，有时我们的特征空间会因数量庞大的特征而变得臃肿。这带来了多种挑战，包括分析和可视化具有数千或数百万个特征的数据，这使得特征空间极其复杂，带来了关于训练模型、内存和空间约束的问题。事实上，这被称为“维数灾难”。无监督方法也可以用于这些场景，我们减少每个数据样本的特征或属性的数量。这些方法通过提取或选择一组主要或代表性特征来减少特征变量的数量。有多种流行的算法可用于降维，如主成分分析(PCA)、最近邻和判别分析。图 [1-18](#Fig18) 显示了应用于具有三维的瑞士卷 3D 结构的典型特征缩减过程的输出，以使用 PCA 获得每个数据样本的二维特征空间。

![A448827_1_En_1_Fig18_HTML.jpg](A448827_1_En_1_Fig18_HTML.jpg)

图 1-18。

Unsupervised learning: dimensionality reduction

从图 [1-18](#Fig18) 中可以清楚地看到，每个数据样本最初都有三个特征或维度，即 D(x1，x2，x3)，在应用 PCA 后，我们将数据集中的每个数据样本缩减为两个维度，即 D’(Z1，z2)。降维技术可以分为以下两种主要方法。

*   特征选择方法:从原始特征列表中为每个数据样本选择特定的特征，而丢弃其他特征。在此过程中不会生成新特征。
*   特征提取方法:我们从数据中的原始特征列表中设计或提取新的特征。因此，缩减的特征子集将包含新生成的特征，这些特征不是原始特征集的一部分。五氯苯甲醚属于这一类别。

### 异常检测

异常检测的过程也称为异常值检测，我们感兴趣的是根据历史数据样本找出通常不会发生的罕见事件或观察结果。有时，异常很少发生，因此是罕见的事件，而在其他情况下，异常可能并不罕见，但可能随着时间的推移在非常短的时间内发生，因此具有特定的模式。无监督学习方法可以用于异常检测，使得我们在具有正常、非异常数据样本的训练数据集上训练算法。一旦它在正常样本中学习了必要的数据表示、模式和属性之间的关系，对于任何新的数据样本，它将能够通过使用它所学习的知识来识别它是异常的还是正常的数据点。图 [1-19](#Fig19) 描述了一些典型的基于异常检测的场景，在这些场景中，您可以应用监督方法(如单类 SVM)和非监督方法(如聚类、K-最近邻、自动编码器等)来基于数据及其特征检测异常。

![A448827_1_En_1_Fig19_HTML.jpg](A448827_1_En_1_Fig19_HTML.jpg)

图 1-19。

Unsupervised learning: anomaly detection

基于异常检测的方法在现实世界中非常流行，例如检测安全攻击或漏洞、信用卡欺诈、制造异常、网络问题等等。

### 关联规则挖掘

通常，关联规则挖掘是一种数据挖掘方法，用于检查和分析大型事务性数据集，以找到感兴趣的模式和规则。这些模式代表了跨事务的各种项目之间有趣的关系和关联。关联规则挖掘也常被称为购物篮分析，用于分析客户购物模式。关联规则有助于根据从训练事务中获得的知识来检测和预测事务模式。使用这种技术，我们可以回答像人们倾向于一起购买什么商品这样的问题，从而指出频繁的商品集。我们还可以关联或关联产品和物品，例如，购买啤酒的人也倾向于在酒吧购买鸡翅。图 [1-20](#Fig20) 显示了典型的关联规则挖掘方法应该如何在事务数据集上理想地工作。

![A448827_1_En_1_Fig20_HTML.jpg](A448827_1_En_1_Fig20_HTML.jpg)

图 1-20。

Unsupervised learning: association rule-mining

从图 [1-20](#Fig20) 中，你可以清楚地看到，基于一段时间内不同的客户交易，我们得到了密切关联的项目，客户倾向于一起购买。其中一些常见的项目集被描述为{肉、蛋}、{奶、蛋}等等。确定高质量关联规则或频繁项目集的标准通常是使用支持度、置信度和提升度等指标来完成的。

这是一种无监督的方法，因为我们事先不知道频繁项集是什么，也不知道哪些项与哪些项有更强的关联。只有在应用像 apriori 算法或 FP-growth 这样的算法之后，我们才能检测和预测彼此密切相关的产品或项目，并找到条件概率依赖性。我们将在第 8 章中详细介绍关联规则挖掘。

## 半监督学习

半监督学习方法通常介于监督和非监督学习方法之间。这些方法通常使用大量未标记的训练数据(形成非监督学习组件)和少量预先标记和注释的数据(形成监督学习组件)。多种技术以生成方法、基于图的方法和基于启发的方法的形式可用。

一种简单的方法是基于有限的标记数据建立一个监督模型，然后将其应用于大量的未标记数据，以获得更多的标记样本，在这些样本上训练模型，并重复这一过程。另一种方法是使用无监督算法来聚类相似的数据样本，使用人在回路中的努力来手动注释或标记这些组，然后在未来使用这些信息的组合。这种方法在许多图像标记系统中使用。涵盖半监督方法超出了本书的范围。

## 强化学习

强化学习方法与传统的监督或非监督方法略有不同。在这种情况下，我们有一个代理，我们希望在一段时间内训练它与特定环境进行交互，并在一段时间内提高它对环境执行的动作类型的性能。通常，代理从一组用于与环境交互的策略或策略开始。在观察环境时，它根据规则或策略并通过观察环境的当前状态来采取特定的行动。基于这一行动，代理人将获得奖励，奖励的形式可能是有益的，也可能是有害的。如果需要，它更新其当前的策略和策略，并且这个迭代过程继续，直到它对其环境有足够的了解以获得期望的回报。强化学习方法的主要步骤如下所述。

1.  为代理准备一套初始策略和策略
2.  观察环境和当前状态
3.  选择最佳策略并执行操作
4.  获得相应的奖励(或惩罚)
5.  如果需要，更新策略
6.  重复步骤 2 - 5，直到代理学习到最佳策略

考虑一个试图让机器人或机器学习下棋的现实问题。在这种情况下，代理将是机器人，环境和状态将是棋盘和棋子的位置。图 [1-21](#Fig21) 描述了一种合适的强化学习方法。

![A448827_1_En_1_Fig21_HTML.jpg](A448827_1_En_1_Fig21_HTML.jpg)

图 1-21。

Reinforcement learning: training a robot to play chess

让机器人学会下棋的主要步骤如图 [1-21](#Fig21) 所示。这是基于前面讨论的任何强化学习方法的步骤。事实上，谷歌的 DeepMind 用强化学习的组件构建了 AlphaGo AI，以训练系统玩围棋。

## 批量学习

批量学习方法也就是通常所说的离线学习方法。这些是在端到端机器学习系统中使用的机器学习方法，在这些系统中，一次性使用所有可用的训练数据来训练模型。一旦训练完成并且模型完成了学习过程，在获得令人满意的性能时，它被部署到生产中，在那里它预测新数据样本的输出。然而，该模型不会随着新数据在一段时间内不断学习。一旦训练完成，模型就停止学习。因此，由于模型在单个批次中使用数据进行训练，并且通常是一次性的过程，这被称为批次或离线学习。

我们总是可以根据新数据来训练模型，但随后我们必须添加新的数据样本以及旧的历史训练数据，并再次使用这批新数据来重建模型。如果大部分的模型构建工作流已经被实现了，那么重新训练一个模型就不会涉及到太多的工作；但是，随着每个新数据样本的数据量越来越大，在一段时间内，重新训练过程将开始消耗更多的处理器、内存和磁盘资源。当您构建将在容量有限的系统上运行的模型时，需要考虑以下几点。

## 在线学习

与批量学习方法相比，在线学习方法的工作方式有所不同。训练数据通常以多个增量批次的形式提供给算法。这些数据批次在 ML 术语中也称为小批次。然而，与批量学习方法不同，训练过程并没有就此结束。它根据发送给它进行预测的新数据样本，在一段时间内不断学习。基本上，它可以在运行过程中预测和学习新数据，而不必对以前的数据样本重新运行整个模型。

在线学习有几个优点-它适用于现实世界的情况，在这种情况下，模型可能需要在新数据样本到达时不断学习和重新训练。像设备故障或异常预测和股票市场预测这样的问题是两个相关的场景。除此之外，由于数据是以增量小批量的方式提供给模型的，因此您可以在商用硬件上构建这些模型，而不必担心内存或磁盘限制，因为与批量学习方法不同，您不需要在训练模型之前将完整的数据集加载到内存中。除此之外，一旦模型在数据集上进行训练，您可以删除它们，因为我们不再需要相同的数据，因为模型会增量学习并记住它在过去学习的内容。

在线学习方法的一个主要注意事项是，坏的数据样本会对模型性能产生不利影响。所有 ML 方法的工作原理都是“垃圾进垃圾出”。因此，如果您向一个训练有素的模型提供了错误的数据样本，它可能会开始学习没有实际意义的关系和模式，这最终会影响整个模型的性能。由于在线学习方法基于新的数据样本进行学习，因此您应该确保进行适当的检查，以便在模型性能突然下降时通知您。此外，应谨慎选择合适的模型参数，如学习率，以确保模型不会过度拟合或基于特定数据样本出现偏差。

## 基于实例的学习

有各种方法可以使用尝试基于输入数据进行归纳的方法来构建机器学习模型。基于实例的学习涉及 ML 系统和方法，它们使用原始数据点本身来计算更新的、以前未见过的数据样本的结果，而不是在训练数据上建立显式模型，然后对其进行测试。

一个简单的例子是 K-最近邻算法。假设`k = 3`，我们有了我们的初始训练数据。ML 方法通过特征了解数据的表示，包括其尺寸、每个数据点的位置等等。对于任何新的数据点，它将使用相似性度量(如余弦或欧几里德距离)并找到离该新数据点最近的三个输入数据点。一旦做出决定，我们只需获取这三个训练点的大部分结果，并将其预测或分配为该新数据点的结果标签/响应。因此，基于实例的学习通过查看输入数据点并使用相似性度量来概括和预测新的数据点来工作。

## 基于模型的学习

基于模型的学习方法是一种更传统的基于训练数据进行归纳的 ML 方法。通常会发生迭代过程，其中输入数据用于提取特征，模型基于各种模型参数(称为超参数)构建。这些超参数基于各种模型验证技术进行优化，以选择在训练数据和一定量的验证和测试数据(从初始数据集分离)上概括得最好的模型。最后，最好的模型用于在需要的时候做出预测或决策。

## CRISP-DM 过程模型

CRISP-DM 模型代表数据挖掘的跨行业标准过程。CRISP-DM 更广为人知的名字是其缩写，它是一个经过尝试、测试的健壮的行业标准流程模型，用于数据挖掘和分析项目。CRISP-DM 清晰地描述了执行任何项目的必要步骤、流程和工作流，从正式确定业务需求到测试和部署解决方案，以将数据转化为见解。数据科学、数据挖掘和机器学习都是试图运行多个迭代过程，以从数据中提取见解和信息。因此，我们可以说，分析数据确实既是一门艺术，也是一门科学，因为它并不总是毫无理由地运行算法；大量的主要工作包括理解业务、投入工作的实际价值，以及阐明最终结果和见解的适当方法。

CRISP-DM 模型告诉我们，为任何分析项目或系统构建端到端解决方案，总共有六个主要步骤或阶段，其中一些是迭代的。就像我们有一个软件开发生命周期，它包含软件开发项目的几个主要阶段或步骤，在这个场景中我们有一个数据挖掘或分析生命周期。图 [1-22](#Fig22) 描述了 CRISP-DM 模型的数据挖掘生命周期。

![A448827_1_En_1_Fig22_HTML.jpg](A448827_1_En_1_Fig22_HTML.jpg)

图 1-22。

The CRISP-DM model depicting the data mining lifecycle

图 [1-22](#Fig22) 清楚地显示了数据挖掘生命周期中共有六个主要阶段，前进的方向用箭头表示。这个模型不是一个僵硬的拼版，而是一个框架，确保您在经历任何分析项目的生命周期时都处于正确的轨道上。在异常检测或趋势分析等场景中，您可能对数据理解、探索和可视化更感兴趣，而不是密集建模。六个阶段中的每一个都详细描述如下。

### 商业理解

这是全面启动任何项目之前的初始阶段。然而，这是生命周期中最重要的阶段之一！这里的主要目标是从理解业务环境和手头要解决的问题的需求开始。业务需求的定义对于将业务问题转化为数据挖掘或分析问题，以及为客户和解决方案任务组设定期望和成功标准至关重要。此阶段的最终交付成果将是一份详细的计划，其中包含项目的主要里程碑和预期时间表，以及成功标准、假设、约束、警告和挑战。

#### 定义业务问题

这个阶段的第一个任务是从理解要解决的问题的业务目标开始，并建立问题的正式定义。以下几点对于清楚地阐明和定义业务问题至关重要。

*   获取要解决的问题的业务上下文，在领域和主题专家(SME)的帮助下评估问题。
*   描述要解决的业务目标的主要难点或目标领域。
*   了解当前已有的解决方案、缺少的内容以及需要改进的地方。
*   根据业务、数据科学家、分析师和 SME 的意见，定义业务目标以及适当的交付成果和成功标准。

#### 评估和分析场景

一旦明确定义了业务问题，所涉及的主要任务将是分析和评估与业务问题定义相关的当前场景。这包括查看当前可用的资源，并记下从资源、人员到数据等所需的各种项目。除此之外，需要讨论对风险和应急计划的正确评估。评估阶段涉及的主要步骤如下。

*   从数据、人员、资源时间和风险等各种角度评估和分析当前可用于解决问题的资源。
*   编写一份所需关键资源(硬件和软件)和相关人员的简要报告。如果有任何缺点，请务必在必要时指出来。
*   逐个讨论业务目标需求，然后在 SME 的帮助下，确定并记录每个需求的可能假设和约束。
*   基于可用数据验证假设和约束(许多假设和约束可能只有在详细分析后才能回答，因此它取决于要解决的问题和可用数据)。
*   记录并报告项目中可能涉及的风险，包括时间表、资源、人员、数据和财务问题。为每种可能的情况建立应急计划。
*   讨论成功的标准，如果需要，尝试记录投资回报或成本与估价的比较分析。这只是一个粗略的基准，以确保项目符合公司或业务愿景。

#### 定义数据挖掘问题

这可以被定义为预分析阶段，一旦成功标准和业务问题被定义，并且所有的风险、假设和约束被记录，该阶段就开始了。这个阶段包括与您的分析师、数据科学家和开发人员进行详细的技术讨论，并使业务利益相关者保持同步。以下是本阶段要完成的主要任务。

*   通过评估可能的工具、算法和技术，讨论并记录适用于解决方案的可能的机器学习和数据挖掘方法。
*   开发端到端解决方案架构的概要设计。
*   记录解决方案的最终输出是什么，以及它将如何与现有的业务组件集成。
*   从数据科学的角度记录成功评估标准。一个简单的例子是确保预测至少 80%准确。

#### 工程计划

这是业务理解阶段的最后一个阶段。项目计划通常由 CRISP-DM 模型中的全部六个主要阶段、预计时间表、分配的资源和人员以及可能的风险和应急计划组成。注意确保为每个阶段定义具体的高级别可交付成果和成功标准，并使用注释(如基于 SME 的反馈)突出建模等迭代阶段。在部署之前，可能需要重新构建和调整模型。

一旦你掌握了以下几点，你就应该为下一步做好准备。

*   问题的业务目标的定义
*   业务和数据挖掘工作的成功标准
*   预算分配和资源规划
*   要遵循的清晰、定义明确的机器学习和数据挖掘方法，包括从探索到部署的高级工作流程
*   详细的项目计划，定义了 CRISP-DM 模型的所有六个阶段，并估计了时间表和风险

### 数据理解

CRISP-DM 流程的第二阶段包括在开始分析流程之前，深入研究可用的数据并更详细地理解它。这包括收集数据、描述各种属性、对数据进行一些探索性分析，以及跟踪数据质量。这一阶段不应被忽视，因为坏数据或对可用数据的了解不足会在这一过程的后期产生连锁不利影响。

#### 数据收集

这项任务是为了提取、整理和收集您的业务目标所需的所有必要数据。通常这包括利用组织的历史数据仓库、数据集市、数据湖等等。根据组织中现有的可用数据以及是否需要额外的数据来进行评估。这可以从网络(即开放数据源)获得，或者可以从其他渠道(如调查、购买、实验和模拟)获得。详细的文档应记录所有用于分析的数据集，以及任何必要的额外数据源。本文件可与本阶段的后续阶段结合使用。

#### 数据描述

数据描述包括对数据进行初步分析，以了解有关数据、其来源、数量、属性和关系的更多信息。一旦记录了这些细节，如果发现任何缺点，应通知相关人员。以下因素对于构建适当的数据描述文档至关重要。

*   数据源(SQL、NoSQL、大数据)、原始记录(ROO)、参考记录(ROR)
*   数据量(大小、记录数量、数据库总数、表格)
*   数据属性及其描述(变量、数据类型)
*   关系和映射方案(理解属性表示)
*   基本描述性统计(平均值、中值、方差)
*   关注哪些属性对业务很重要

#### 探索性数据分析

探索性数据分析，也称为 EDA，是生命周期中第一个主要分析阶段之一。这里，主要目标是详细探索和理解数据。您可以利用描述性统计、绘图、图表和可视化来查看各种数据属性，找到关联和相关性，并记录数据质量问题(如果有的话)。以下是此阶段的一些主要任务。

*   探索、描述和可视化数据属性
*   选择对问题最重要的数据和属性子集
*   进行广泛的分析，找出相关性和关联性，并测试假设
*   记录丢失的数据点(如果有)

#### 数据质量分析

数据质量分析是数据理解阶段的最后一个阶段，在此阶段，我们分析数据集中的数据质量，并记录潜在的错误、缺点以及在进一步分析数据或开始建模工作之前需要解决的问题。数据质量分析的主要重点包括以下内容。

*   缺少值
*   不一致的值
*   由于数据错误导致的错误信息(手动/自动)
*   错误的元数据信息

### 数据准备

CRISP-DM 流程的第三阶段发生在获得足够的业务问题和相关数据集的知识之后。数据准备主要是一组任务，在运行任何分析或机器学习方法和构建模型之前，执行这些任务来清理、争论、整理和准备数据。在本节中，我们将简要讨论数据准备阶段的一些主要任务。这里要记住的重要一点是，数据准备通常是数据挖掘生命周期中最耗时的阶段，通常占整个项目的 60%到 70%。然而，这个阶段应该非常认真地对待，因为正如我们以前多次讨论过的，坏的数据将导致坏的模型和差的性能和结果。

#### 数据集成

数据集成的过程主要是在我们有可能想要集成或合并多个数据集时完成的。这可以通过两种方式实现。通过组合多个数据集来追加它们，这通常是针对具有相同属性的数据集进行的。通过使用公共字段(如键)将具有不同属性或列的几个数据集合并在一起。

#### 数据争论

数据争论或数据管理的过程包括数据处理、清理、规范化和格式化。原始形式的数据很少能被机器学习方法用来构建模型。因此，我们需要根据数据的形式处理数据，清除潜在的错误和不一致，并将其格式化为 ML 算法更易于使用的格式。以下是与数据争论相关的主要任务。

*   处理缺失值(删除行，估算缺失值)
*   处理数据不一致(删除行、属性、修复不一致)
*   修复不正确的元数据和注释
*   处理不明确的属性值
*   将数据管理和格式化为必要的格式(CSV、Json、关系型)

#### 属性生成和选择

数据由观察值或样本(行)和属性或特征(列)组成。在机器学习术语中，属性生成的过程也称为特征提取和工程。属性生成基本上是根据一些规则、逻辑或假设从现有属性创建新属性或变量。一个简单的例子是为一个组织中的雇员数据集创建一个名为`age`的新数值变量，该变量基于两个日期时间字段`current_date`和`birth_date`。有几种关于属性生成的技术，我们将在以后的章节中讨论。

属性选择基本上是根据属性重要性、质量、相关性、假设和约束等参数从数据集中选择要素或属性的子集。有时甚至使用机器学习方法来根据数据选择相关属性。在机器学习术语中，这通常被称为特征选择。

### 建模

CRISP-DM 流程的第四个阶段是该流程的核心阶段，在这个阶段中，大部分分析都与使用干净、格式化的数据及其属性来构建模型以解决业务问题有关。这是一个迭代过程，如前面图 [1-22](#Fig22) 所示，包括模型评估和建模前的所有步骤。基本思想是迭代地构建多个模型，试图得到满足我们的成功标准、数据挖掘目标和业务目标的最佳模型。在本节中，我们将简要讨论与建模相关的一些主要阶段。

#### 选择建模技术

在这一阶段，我们挑选了“业务理解”阶段列出的相关机器学习和数据挖掘工具、框架、技术和算法的列表。通常根据数据分析师和数据科学家的输入和见解来选择在解决问题时被证明是健壮和有用的技术。这些主要由当前可用的数据、业务目标、数据挖掘目标、算法要求和约束决定。

#### 模型结构

模型构建过程也称为使用数据集中的数据和要素训练模型。数据(特征)和机器学习算法的结合为我们提供了一个模型，该模型试图对训练数据进行归纳，并以洞察和/或预测的形式给出必要的结果。通常使用各种算法对相同的数据尝试多种建模方法来解决相同的问题，以获得最佳的模型，该模型执行并给出最接近商业成功标准的输出。这里要跟踪的关键是创建的模型、使用的模型参数以及它们的结果。

#### 模型评估和调整

在这一阶段，我们根据几个指标评估每个模型，如模型准确度、精确度、召回率、F1 分数等。我们还基于网格搜索和交叉验证等技术调整模型参数，以获得给我们最佳结果的模型。调整后的模型也与数据挖掘目标相匹配，以查看我们是否能够获得预期的结果和性能。在机器学习领域，模型调整也被称为超参数优化。

#### 模型评估

一旦我们有了提供期望的和相关的结果的模型，就基于以下参数对模型进行详细的评估。

*   模型性能符合定义的成功标准
*   来自模型的可重复且一致的结果
*   可扩展性、健壮性和易于部署
*   模型的未来可扩展性
*   模型评估给出了满意的结果

### 估价

CRISP-DM 流程的第五个阶段在我们从建模阶段获得最终模型后开始，这些模型满足与我们的数据挖掘目标相关的必要成功标准，并具有与模型评估指标(如准确性)相关的预期性能和结果。评估阶段包括对最终模型及其结果进行详细的评估和审查。本节评估的一些要点如下。

*   根据结果的质量对最终模型进行排序，并根据与业务目标的一致性对它们的相关性进行排序
*   被模型证明无效的任何假设或约束
*   从数据提取和处理到建模和预测的整个机器学习管道的部署成本
*   整个过程有什么痛点吗？应该推荐什么？应该避免什么？
*   基于结果的数据充分性报告
*   来自解决方案团队和 SME 的最终意见、反馈和建议

基于从这些点形成的报告，在讨论之后，团队可以决定他们是否想要进行模型部署的下一个阶段，或者是否需要从业务和数据理解开始到建模的全面重复。

### 部署

CRISP-DM 流程的最后一个阶段是将您选择的模型部署到生产中，并确保从开发到生产的过渡是无缝的。通常，大多数组织都遵循标准的生产路径方法。根据所需的资源、服务器、硬件、软件等，制定适当的部署计划。在必要的系统和服务器上验证、保存和部署模型。还制定了定期监控和维护模型的计划，以持续评估其性能，检查结果及其有效性，并在需要时淘汰、替换和更新模型。

## 构建机器智能

机器学习、数据挖掘或人工智能的目标是让我们的生活变得更容易，自动化任务，并做出更好的决策。构建机器智能涉及到我们迄今为止所学的一切，从机器学习概念到实际实现和构建模型并在现实世界中使用它们。机器智能可以使用像机器学习这样的非传统计算方法来构建。在本节中，我们基于 CRISP-DM 模型建立了成熟的端到端机器学习管道，这将通过使用结构化流程构建机器智能来帮助我们解决现实世界的问题。

### 机器学习管道

解决现实世界机器学习或分析问题的最佳方式是使用机器学习管道，从获取数据开始，到使用机器学习算法和技术将其转化为信息和见解。这更像是一个基于技术或解决方案的管道，它假设 CRISP-DM 模型的几个方面都已涵盖，包括以下几点。

*   业务和数据理解
*   ML/DM 技术选择
*   风险、假设和限制评估

机器学习管道将主要由与数据检索和提取、准备、建模、评估和部署相关的元素组成。图 [1-23](#Fig23) 显示了标准机器学习管道的高级概览，主要阶段在它们的块中突出显示。

![A448827_1_En_1_Fig23_HTML.jpg](A448827_1_En_1_Fig23_HTML.jpg)

图 1-23。

A standard Machine Learning pipeline

从图 [1-23](#Fig23) 可以明显看出，机器学习管道中有几个主要阶段，它们与 CRISP-DM 过程模型非常相似，这也是我们之前详细讨论它的原因。这里简单介绍一下管道中的主要步骤。

*   数据检索:这主要是从各种数据源和数据存储中收集、提取和获取数据。我们将在第 [3](03.html) 章“处理、争论和可视化数据”中详细介绍数据检索机制。
*   数据准备:在这一步中，我们对数据进行预处理、清理、争论，并根据需要对其进行操作。还进行了初步的探索性数据分析。接下来的步骤包括从数据中提取、设计和选择特征/属性。
    *   数据处理和争论:主要涉及数据处理、清理、整理、争论和进行初步的描述性和探索性数据分析。我们将在第 [3](03.html) 章“处理、争论和可视化数据”中通过实际操作的例子进一步详细介绍这一点。
    *   特征提取和工程:在这里，我们从原始数据中提取重要的特征或属性，甚至从现有的特征中创建或设计新的特征。第 [4](04.html) 章“特征工程和选择”涵盖了各种特征工程技术的细节。
    *   特征缩放和选择:数据特征通常需要归一化和缩放，以防止机器学习算法出现偏差。除此之外，我们经常需要根据特性的重要性和质量来选择所有可用特性的子集。这个过程被称为特征选择。第 4 章“特征工程和选择”涵盖了这些方面。
*   建模:在建模过程中，我们通常将数据特征提供给机器学习方法或算法，并训练模型，通常在大多数情况下优化特定的成本函数，目的是减少错误并概括从数据中学习到的表示。第 [5](05.html) 章，“构建、调优和部署模型”，涵盖了构建机器学习模型背后的艺术和科学。
*   模型评估和调整:在验证数据集上对构建的模型进行评估和测试，并根据准确性、F1 分数等指标评估模型性能。模型具有各种参数，这些参数在一个称为超参数优化的过程中进行调整，以获得具有最佳结果的模型。第 5 章“构建、调优和部署模型”涵盖了这些方面。
*   部署和监控:选定的模型被部署到生产环境中，并根据它们的预测和结果不断地被监控。关于模型部署的详细信息在第 [5](05.html) 章“构建、调整和部署模型”中介绍。

### 监督机器学习管道

到目前为止，我们知道有监督的机器学习方法都是关于使用有监督的标记数据来训练模型，然后预测新数据样本的结果。某些过程(如特征工程、缩放和选择)应始终保持不变，以便使用相同的特征来训练模型，并从新的数据样本中提取相同的特征以在预测阶段提供给模型。基于我们早期的通用机器学习管道，图 [1-24](#Fig24) 显示了一个标准的监督机器学习管道。

![A448827_1_En_1_Fig24_HTML.jpg](A448827_1_En_1_Fig24_HTML.jpg)

图 1-24。

Supervised Machine Learning pipeline

您可以清楚地看到图 [1-24](#Fig24) 中突出显示的模型训练和预测两个阶段。此外，根据我们之前提到的内容，数据处理、争论、特征工程、缩放和选择的相同顺序用于训练模型中使用的数据和模型预测结果的未来数据样本。这是非常重要的一点，无论您何时构建任何监督模型，都必须记住这一点。除此之外，如所描述的，该模型是机器学习(监督)算法和训练数据特征以及相应标签的组合。该模型将从新数据样本中提取特征，并在预测阶段输出预测标签。

### 无监督机器学习管道

无监督机器学习就是从数据中提取模式、关系、关联和聚类。与特征工程、缩放和选择相关的过程类似于监督学习。然而，这里没有预先标记数据的概念。因此，无监督的机器学习管道与有监督的管道相比会略有不同。图 [1-25](#Fig25) 描绘了一个标准的无监督机器学习流水线。

![A448827_1_En_1_Fig25_HTML.jpg](A448827_1_En_1_Fig25_HTML.jpg)

图 1-25。

Unsupervised Machine Learning pipeline

图 [1-25](#Fig25) 清楚地描述了没有监督标记数据用于训练模型。在没有标签的情况下，我们只有经过与监督学习管道中相同的数据准备阶段的训练数据，并且我们用无监督机器学习算法和训练特征来构建我们的无监督模型。在预测阶段，我们从新的数据样本中提取特征，并将其传递给模型，该模型根据我们尝试执行的机器学习任务的类型给出相关结果，这些任务可以是聚类、模式检测、关联规则或降维。

## 真实案例研究:预测学生资助建议

让我们从目前所学的退一步！这里的主要目标是牢固掌握整个机器学习领域，理解关键概念，建立基本基础，并理解如何在机器学习管道的帮助下执行机器学习项目，CRISP-DM 流程模型是所有灵感的来源。让我们将所有这些放在一起，通过在玩具数据集上建立监督机器学习管道，进行一个非常基本的真实世界案例研究。我们的主要目标如下。假设您有几个学生具有多种属性，如成绩、表现和分数，您能否基于过去的历史数据建立一个模型来预测该学生获得研究项目推荐资助的机会？

这将是一个快速演练，主要目的是描述如何构建和部署真实世界的机器学习管道并执行预测。这也会给你一个很好的动手体验来开始机器学习。如果你不理解每一行代码的细节，不要太担心；随后的章节详细介绍了这里使用的所有工具、技术和框架。我们将在本书中使用 Python 3.5 可以参考第 [2](02.html) 章“Python 机器学习生态系统”，了解更多关于 Python 以及机器学习中使用的各种工具和框架。您可以按照本节中的代码片段进行操作，或者通过在命令行/终端中运行与这个笔记本相同的目录中的`jupyter notebook`来打开`Predicting Student Recommendation Machine Learning Pipeline.ipynb` jupyter 笔记本。然后，您可以从浏览器运行笔记本中的相关代码片段。第 2 章[详细介绍了 jupyter 笔记本。](02.html)

### 目标

您有历史学生成绩数据和他们的资助建议结果，以逗号分隔值文件的形式命名为`student_records.csv`。每个数据样本由以下属性组成。

*   `Name`(学生姓名)
*   `OverallGrade`(获得总成绩)
*   (他们在逗留期间是否勤奋)
*   `ResearchScore`(在研究工作中获得的分数)
*   `ProjectScore`(项目中获得的分数)
*   `Recommend`(是否获得资助推荐)

你的主要目标是根据这些数据建立一个预测模型，这样你就可以根据他们的表现来预测未来的学生是否会被推荐获得助学金。

### 资料检索

这里，我们将利用`pandas`框架从 CSV 文件中检索数据。下面的代码片段向我们展示了如何检索和查看数据。

![A448827_1_En_1_Fig26_HTML.jpg](A448827_1_En_1_Fig26_HTML.jpg)

图 1-26。

Raw data depicting student records and their recommendations

```py
In [1]: import pandas as pd
   ...: # turn of warning messages
   ...: pd.options.mode.chained_assignment = None  # default='warn'
   ...:
   ...: # get data
   ...: df = pd.read_csv('student_records.csv')
   ...: df

```

现在我们可以在图 [1-26](#Fig26) 中看到显示每个学生的记录及其相应推荐结果的数据样本，我们将执行与数据准备相关的必要任务。

### 数据准备

根据我们之前看到的数据集，我们没有任何数据错误或缺失值，因此我们在这一部分将主要关注要素工程和缩放。

#### 特征提取和工程

让我们首先从数据集中提取现有要素，并在单独的变量中提取结果。下面的代码片段展示了这个过程。参见图 [1-27](#Fig27) 和 [1-28](#Fig28) 。

![A448827_1_En_1_Fig27_HTML.jpg](A448827_1_En_1_Fig27_HTML.jpg)

图 1-27。

Dataset features

```py
In [2]: # get features and corresponding outcomes
   ...: feature_names = ['OverallGrade', 'Obedient', 'ResearchScore',
                         'ProjectScore']
   ...: training_features = df[feature_names]
   ...:
   ...: outcome_name = ['Recommend']
   ...: outcome_labels = df[outcome_name]

In [3]: # view features
   ...: training_features

```

![A448827_1_En_1_Fig28_HTML.jpg](A448827_1_En_1_Fig28_HTML.jpg)

图 1-28。

Dataset recommendation outcome labels for each student

```py
In [4]: # view outcome labels
   ...: outcome_labels

```

既然我们已经从数据及其相应的结果标签中提取了初始可用特征，那么让我们根据它们的类型(数字和分类)来分离出可用特征。第 [3](03.html) 章“处理、争论和可视化数据”中详细介绍了特征变量的类型。

```py
In [5]: # list down features based on type
   ...: numeric_feature_names = ['ResearchScore', 'ProjectScore']
   ...: categoricial_feature_names = ['OverallGrade', 'Obedient']

```

我们现在将使用下面的代码，使用来自`scikit-learn`的标准标量来缩放或归一化我们的两个基于分数的数字属性。

![A448827_1_En_1_Fig29_HTML.jpg](A448827_1_En_1_Fig29_HTML.jpg)

图 1-29。

Feature set with scaled numeric attributes

```py
In [6]: from sklearn.preprocessing import StandardScaler
   ...: ss = StandardScaler()
   ...:
   ...: # fit scaler on numeric features
   ...: ss.fit(training_features[numeric_feature_names])
   ...:
   ...: # scale numeric features now
   ...: training_features[numeric_feature_names] =
                                    ss.transform(training_features[numeric_feature_names])
   ...:
   ...: # view updated featureset
   ...: training_features

```

既然我们已经成功地缩放了我们的数字特征(见图 [1-29](#Fig29) ，让我们处理我们的分类特征，并基于以下代码执行必要的特征工程。

![A448827_1_En_1_Fig30_HTML.jpg](A448827_1_En_1_Fig30_HTML.jpg)

图 1-30。

Feature set with engineered categorical variables

```py
In [7]: training_features = pd.get_dummies(training_features,
                                           columns=categoricial_feature_names)
   ...: # view newly engineering features
   ...: training_features

```

```py
In [8]: # get list of new categorical features
   ...: categorical_engineered_features = list(set(training_features.columns) -
                                                 set(numeric_feature_names))

```

图 [1-30](#Fig30) 向我们展示了带有新设计分类变量的更新特征集。这个过程也称为热编码。

### 建模

现在，我们将使用逻辑回归算法，基于我们的特征集构建一个简单的分类(监督)模型。以下代码描述了如何构建监督模型。

```py
In [9]: from sklearn.linear_model import LogisticRegression
   ...: import numpy as np
   ...:
   ...: # fit the model
   ...: lr = LogisticRegression()
   ...: model = lr.fit(training_features,
                       np.array(outcome_labels['Recommend']))
   ...: # view model parameters
   ...: model
Out[9]: LogisticRegression(C=1.0, class_weight=None, dual=False,
                        fit_intercept=True, intercept_scaling=1, max_iter=100,
                        multi_class='ovr', n_jobs=1, penalty='l2',
                        random_state=None, solver='liblinear', tol=0.0001,
                        verbose=0, warm_start=False)

```

因此，我们现在有了基于带 L2 正则化的逻辑回归模型的监督学习模型，正如您在前面的输出中看到的参数。

### 模型评估

通常，模型评估是基于不同于定型数据集的某个维持或验证数据集来完成的，以防止过度拟合或偏向模型。由于这是一个玩具数据集上的示例，让我们使用下面的代码片段来评估我们的模型在训练数据上的性能。

```py
In [10]: # simple evaluation on training data
    ...: pred_labels = model.predict(training_features)
    ...: actual_labels = np.array(outcome_labels['Recommend'])
    ...:
    ...: # evaluate model performance
    ...: from sklearn.metrics import accuracy_score
    ...: from sklearn.metrics import classification_report
    ...:
    ...: print('Accuracy:', float(accuracy_score(actual_labels,
                pred_labels))*100, '%')
    ...: print('Classification Stats:')
    ...: print(classification_report(actual_labels, pred_labels))

Accuracy: 100.0 %
Classification Stats:
             precision    recall  f1-score   support

         No       1.00      1.00      1.00         5
        Yes       1.00      1.00      1.00         3

avg / total       1.00      1.00      1.00         8

```

因此，您可以看到我们之前提到的各种指标，如准确度、精确度、召回率和描述模型性能的 F1 分数。我们将在第 [5](05.html) 章“构建、调整和部署模型”中详细讨论这些指标。

### 模型部署

我们建立了我们的第一个监督学习模型，并且为了在系统或服务器中部署这个模型，我们需要持久化这个模型。我们还需要保存用于缩放数字特征的标量对象，因为我们使用它来转换新数据样本的数字特征。下面的代码片段描述了存储模型和标量对象的方法。

```py
In [11]: from sklearn.externals import joblib
    ...: import os
    ...: # save models to be deployed on your server
    ...: if not os.path.exists('Model'):
    ...:     os.mkdir('Model')
    ...: if not os.path.exists('Scaler'):
    ...:     os.mkdir('Scaler')
    ...:
    ...: joblib.dump(model, r'Model/model.pickle')
    ...: joblib.dump(ss, r'Scaler/scaler.pickle')

```

这些文件可以很容易地部署在服务器上，并带有必要的代码来重新加载模型和预测新的数据样本，我们将在接下来的小节中看到这一点。

### 行动预测

我们现在已经准备好开始使用我们新构建和部署的模型进行预测了！要开始预测，我们需要将模型和标量对象加载到内存中。下面的代码可以帮助我们做到这一点。

```py
In [12]: # load model and scaler objects
    ...: model = joblib.load(r'Model/model.pickle')
    ...: scaler = joblib.load(r'Scaler/scaler.pickle')

```

我们有一些新的学生记录样本(两个学生的)，我们希望我们的模型能够预测他们是否会获得助学金推荐。让我们使用下面的代码来检索和查看这些数据。

![A448827_1_En_1_Fig31_HTML.jpg](A448827_1_En_1_Fig31_HTML.jpg)

图 1-31。

New student records

```py
In [13]: ## data retrieval
    ...: new_data = pd.DataFrame([{'Name': 'Nathan', 'OverallGrade': 'F',
                   'Obedient': 'N', 'ResearchScore': 30, 'ProjectScore': 20},
    ...:                          {'Name': 'Thomas', 'OverallGrade': 'A',
                   'Obedient': 'Y', 'ResearchScore': 78, 'ProjectScore': 80}])
    ...: new_data = new_data[['Name', 'OverallGrade', 'Obedient',
                              'ResearchScore', 'ProjectScore']]
    ...: new_data

```

我们现在将在下面的代码片段中执行与数据准备相关的任务—特征提取、工程和缩放。

![A448827_1_En_1_Fig32_HTML.jpg](A448827_1_En_1_Fig32_HTML.jpg)

图 1-32。

Updated feature set for new students

```py
In [14]: ## data preparation
    ...: prediction_features = new_data[feature_names]
    ...:
    ...: # scaling
    ...: prediction_features[numeric_feature_names] =
                scaler.transform(prediction_features[numeric_feature_names])
    ...:
    ...: # engineering categorical variables
    ...: prediction_features = pd.get_dummies(prediction_features,
                                     columns=categoricial_feature_names)
    ...:
    ...: # view feature set
    ...: prediction_features

```

我们现在有了新学生的相关功能！但是，您可以看到，根据某些等级(如 B、C 和 e ),缺少一些分类特征。这是因为这些学生中没有一个获得这些等级，但是我们仍然需要这些属性，因为模型是针对包括这些属性在内的所有属性进行训练的。下面的代码片段帮助我们识别和添加缺失的分类特征。我们将每个学生的这些特征的值加为 0，因为他们没有获得这些分数。

![A448827_1_En_1_Fig33_HTML.jpg](A448827_1_En_1_Fig33_HTML.jpg)

图 1-33。

Final feature set for new students

```py
In [15]: # add missing categorical feature columns
    ...: current_categorical_engineered_features =
             set(prediction_features.columns) - set(numeric_feature_names)
    ...: missing_features = set(categorical_engineered_features) -
                               current_categorical_engineered_features
    ...: for feature in missing_features:
    ...:     # add zeros since feature is absent in these data samples
    ...:     prediction_features[feature] = [0] * len(prediction_features)
    ...:
    ...: # view final feature set
    ...: prediction_features

```

我们已经为两位新生准备好了完整的功能集。让我们来测试一下我们的模型，并得到关于拨款建议的预测！

![A448827_1_En_1_Fig34_HTML.jpg](A448827_1_En_1_Fig34_HTML.jpg)

图 1-34。

New student records with model predictions for grant recommendations

```py
In [16]: ## predict using model
    ...: predictions = model.predict(prediction_features)
    ...:
    ...: ## display results
    ...: new_data['Recommend'] = predictions
    ...: new_data

```

从图 [1-34](#Fig34) 中我们可以清楚地看到，我们的模型已经预测了两个新生的助学金推荐标签。与内森相比，托马斯显然很勤奋，平均成绩优秀，最有可能得到助学金的推荐。因此，您可以看到，我们的模型已经学会了如何根据过去的历史学生数据来预测助学金推荐结果。这应该会激起你开始学习机器的兴趣。在接下来的章节中，我们将深入探讨更复杂的现实世界问题！

## 机器学习的挑战

机器学习是一个快速发展、快节奏、令人兴奋的领域，有很多前景、机会和范围。然而，由于机器学习方法的复杂性质，它对数据的依赖性，以及不是更传统的计算范式之一，它带来了自己的一系列挑战。以下几点涵盖了机器学习中的一些主要挑战。

*   数据质量问题会导致问题，尤其是在数据处理和特征提取方面。
*   数据获取、提取和检索是一个极其繁琐和耗时的过程。
*   很多场景下缺乏好的质量和足够的训练数据。
*   用定义明确的目标清晰地阐述业务问题。
*   特征提取和工程，尤其是手工制作特征，是机器学习中最困难也是最重要的任务之一。最近，深度学习似乎在这一领域取得了一些优势。
*   过拟合或欠拟合模型会导致模型从训练数据中学习到较差的表示和关系，从而导致有害的性能。
*   维数灾难:太多的特征可能是一个真正的障碍。
*   复杂的模型很难在现实世界中部署。

这不是当今机器学习面临的挑战的详尽列表，但它绝对是数据科学家或分析师在机器学习项目和任务中通常面临的首要问题的列表。当我们在后续章节中讨论机器学习管道中的各个阶段以及解决现实世界中的问题时，我们将详细讨论这些问题。

## 机器学习的现实应用

今天，机器学习在现实世界中被广泛应用和使用，以解决基于传统方法和基于规则的系统不可能解决的复杂问题。下面的列表描述了机器学习的一些现实应用。

*   在线购物平台中的产品推荐
*   情绪和情感分析
*   异常检测
*   欺诈检测和防范
*   内容推荐(新闻、音乐、电影等)
*   天气预报
*   股票市场预测
*   市场篮子分析
*   客户细分
*   图像和视频中的对象和场景识别
*   语音识别
*   流失分析
*   点击通过预测
*   故障/缺陷检测和预防
*   垃圾邮件过滤

## 摘要

本章的目的是在深入研究机器学习管道和解决现实世界的问题之前，让你熟悉机器学习的基础。本章介绍了当今世界对机器学习的需求，重点是大规模做出数据驱动的决策。我们还谈到了各种编程范式，以及机器学习如何颠覆了传统的编程范式。接下来，我们探索了机器学习的前景，从形式定义到与机器学习相关的各个领域。基本的基础概念涵盖了数学、统计学、计算机科学、数据科学、数据挖掘、人工智能、自然语言处理和深度学习等领域，因为所有这些都与机器学习有关，我们还将在未来的章节中使用这些领域的工具、技术、方法和流程。还涵盖了与各种机器学习方法相关的概念，包括监督、非监督、半监督和强化学习。还描述了机器学习方法的其他分类，如基于批处理和基于在线的学习方法，以及基于在线和基于实例的学习方法。解释了 CRISP-DM 过程模型的详细描述，以给出数据挖掘项目的行业标准过程的概述。从这个模型中得出类比来建立机器学习管道，其中我们关注监督和非监督学习管道。

我们将本章涵盖的所有内容结合在一起，解决了一个小的现实世界问题，即预测学生的资助建议，并从头开始构建一个样本机器学习管道。这肯定会让你为下一章做好准备，在下一章中，你将更详细地探索机器学习管道中的每个阶段，并覆盖 Python 机器学习生态系统的基础。最后但同样重要的是，机器学习的挑战和现实世界应用将让你对机器学习的广阔范围有一个好的想法，并让你意识到与机器学习问题相关的警告和陷阱。