# 3.处理、争论和可视化数据

自从计算机和互联网成为主流以来，我们周围的世界发生了巨大的变化。随着无处不在的手机和现在支持互联网的设备，数字世界和现实世界之间的界限比以往任何时候都更加模糊。这一切的核心是数据。数据是我们周围一切的中心，无论是金融、供应链、医学、太空探索、通信等等。不足为奇的是，仅在过去几年中，我们就生成了世界上 90%的数据，而这仅仅是个开始。数据被称为 21 世纪的石油，这是正确的。最后几章介绍了机器学习和 Python 生态系统的概念。本章介绍了机器学习世界赖以展示其魔力和奇迹的核心实体。

一切数字化事物的核心都有某种形式的数据。数据是由全球范围内的众多来源以不同的速率以多种格式生成的。在我们深入机器学习的细节之前，我们将花一些时间和精力来理解这个被称为数据的中心实体。重要的是，我们理解它的各个方面，并根据需求配备不同的技术来处理它。

在本章中，我们将介绍数据在一个典型的机器学习相关用例中的经历，在这个用例中，数据从其最初的原始形式转变为机器学习算法/模型可以使用的形式。我们涵盖了各种数据格式、处理和争论技术，以将数据转换为机器学习算法可以用来进行分析的形式。我们还学习了不同的可视化技术，以更好地理解手头的数据。这些技术一起将帮助我们为在接下来的章节和真实世界场景中要解决的问题做好准备。

第 [1](01.html) 章介绍了 CRISP-DM 方法。这是全球数据科学团队遵循的标准工作流之一。在本章的下一节中，我们将集中讨论该方法的以下几个小节:

*   数据收集:了解不同数据类型的不同数据检索机制
*   数据描述:了解所收集数据的各种属性和特性
*   数据争论:为建模步骤中的消费准备数据
*   数据可视化:可视化不同的属性，以便共享结果、更好地理解等等

本章的代码示例、jupyter 笔记本和示例数据集可在本书的 GitHub 资源库中获得，该资源库位于第三章的目录/文件夹下的 [`https://github.com/dipanjanS/practical-machine-learning-with-python`](https://github.com/dipanjanS/practical-machine-learning-with-python) 。

## 数据收集

数据收集是一切开始的地方。虽然被列为业务理解和问题定义之后的一个步骤，但是数据收集通常是并行进行的。这样做是为了在形成和处理完整的用例之前，用可用性、潜在价值等事实来帮助增强业务理解过程。当然，一旦问题陈述被定义并且项目开始进行，数据收集就采取一种正式的和更好的形式。

数据是我们周围一切的中心，这是一个巨大的机会。然而，这也提出了一个事实，即它必须以不同的格式、形状和大小呈现。它的无所不在也意味着它存在于遗留机器(比如大型机)、web(比如网站和 web 应用程序)、数据库、平面文件、传感器、移动设备等系统中。

让我们看看一些最常见的数据格式和收集这些数据的方法。

### 战斗支援车

CSV 数据文件是最广泛可用的数据格式之一。它也是跨领域的不同系统仍在使用和首选的最古老的格式之一。逗号分隔值(CSV)是包含数据的数据文件，其每个属性由“，”(逗号)分隔。图 [3-1](#Fig1) 描绘了典型 CSV 文件的快速快照。

![A448827_1_En_3_Fig1_HTML.jpg](A448827_1_En_3_Fig1_HTML.jpg)

图 3-1。

Sample CSV file

示例 CSV 显示了数据通常是如何排列的。它包含由逗号分隔的不同数据类型的属性。CSV 可能包含可选的标题行(如示例所示)。CSV 还可以选择将每个属性用单引号或双引号括起来，以便更好地区分。虽然 CSV 通常用于存储表格数据，即行和列形式的数据，但这不是唯一的方法。

CSV 有不同的变体，只需将分隔符更改为制表符，就可以生成一个 TSV(或制表符分隔值)文件。这里的基本思想是使用一个独特的符号来界定/分隔不同的属性。

现在我们知道了 CSV 是什么样子，让我们使用一些 Python 魔法来读取/提取这些数据以供使用。

使用 Python 这样的语言的优势之一是它能够抽象和处理大量的东西。与其他需要特定库或大量代码来完成基本工作的语言不同，Python 使用élan 来处理。同样的是读取 CSV 文件。读取 CSV 最简单的方法是通过 Python `csv`模块。这个模块提供了一个名为`reader()`的抽象函数。reader 函数将 file 对象作为输入，返回一个包含从 csv 文件中读取的信息的`iterator`。下面的代码片段使用`csv.reader()`函数读取给定的文件。

```py
csv_reader = csv.reader(open(file_name, 'rb'), delimiter=',')

```

一旦返回了`iterator`,我们就可以轻松地遍历内容，并获得所需形式/格式的数据。为了完整起见，我们来看一个例子，在这个例子中，我们使用`csv`模块读取图 [3-1](#Fig1) 中所示的 CSV 的内容。然后，我们将提取它的每个属性，并将数据转换成一个带有表示它们的键的`dict`。下面的代码片段构成了这些操作。

```py
csv_rows = list()
csv_attr_dict = dict()
csv_reader = None

# read csv
csv_reader = csv.reader(open(file_name, 'rb'), delimiter=delimiter)

# iterate and extract data
for row in csv_reader:
    print(row)
    csv_rows.append(row)

# iterate and add data to attribute lists
for row in csv_rows[1:]:
    csv_attr_dict['sno'].append(row[0])
    csv_attr_dict['fruit'].append(row[1])
    csv_attr_dict['color'].append(row[2])
    csv_attr_dict['price'].append(row[3])

```

输出是一个包含每个属性的`dict`，作为一个带有值的`key`和一个从 CSV 文件中读取的值的有序`list`。

```py
CSV Attributes::
{'color': ['red', 'yellow', 'yellow', 'orange', 'green', 'yellow', 'green'],
 'fruit': ['apple', 'banana', 'mango', 'orange', 'kiwi', 'pineapple', 'guava'],
 'price': ['110.85', '50.12', '70.29', '80.00', '150.00', '90.00', '20.00'],
 'sno': ['1', '2', '3', '4', '5', '6', '7']}

```

从 CSV 中提取数据及其转换取决于用例需求。将我们的示例 CSV 转换成属性的`dict`是一种方法。我们可以根据数据和我们的要求选择不同的输出格式。

虽然处理和读取 CSV 文件的工作流程非常简单易用，但我们希望标准化并加快我们的流程。此外，通常情况下，更容易理解表格格式的数据。在前一章中，我们介绍了具有一些惊人功能的`pandas`库。现在让我们利用`pandas`来读取 CSV。

下面的代码片段展示了与`csv`模块相比，`pandas`如何使从 CSV 中读取和提取数据更加简单和一致。

```py
df = pd.read_csv(file_name,sep=delimiter)

```

通过一行和几个可选参数(根据需要)，`pandas`将数据从 CSV 文件提取到 dataframe 中，data frame 是相同数据的表格表示。使用`pandas`的一个主要优点是它可以处理 CSV 文件中的许多不同变化，比如有或没有标题的文件、用引号括起来的属性值、推断数据类型等等。此外，各种机器学习库能够直接处理`pandas`数据帧的事实，使其成为事实上处理 CSV 文件的标准包。

前面的代码片段生成以下输出数据帧:

```py
   sno      fruit   color   price
0    1      apple     red  110.85
1    2     banana  yellow   50.12
2    3      mango  yellow   70.29
3    4     orange  orange   80.00
4    5       kiwi   green  150.00
5    6  pineapple  yellow   90.00
6    7      guava   green   20.00

```

Note

`pandas`使读取 CSV 文件的过程变得轻而易举，然而当我们需要更多的灵活性时,`csv`模块就派上了用场。例如，并不是每个用例都需要表格形式的数据，或者数据的格式可能不一致，需要一个像`csv`这样的灵活库来支持定制逻辑处理这样的数据。

同样，来自包含除'，'(逗号)之外的分隔符(如制表符或分号)的平面文件的数据可以很容易地用这两个模块来处理。我们将在后续章节的具体用例中使用这些工具；在此之前，我们鼓励您探索和尝试这些方法，以便更好地理解。

### 数据

Java Script Object Notation (JSON)是数字领域中使用最广泛的数据交换格式之一。JSON 是 XML 等遗留格式的轻量级替代(我们接下来将讨论这种格式)。JSON 是一种文本格式，它独立于语言，有特定的约定。JSON 是一种人类可读的格式，在大多数编程/脚本语言中很容易解析。JSON 文件/对象只是名称(键)-值对的集合。这种键值对结构在编程语言中具有相应的数据结构，这些数据结构以字典(Python `dict`)、`struct`、`object`、`record`、键控列表等形式存在。更多详情请见 [`http://www.json.org/`](http://www.json.org/) 。

JSON 标准定义了结构，如图 [3-2](#Fig2) 所示。

![A448827_1_En_3_Fig2_HTML.jpg](A448827_1_En_3_Fig2_HTML.jpg)

图 3-2。

JSON object structure (reference: [`http://www.json.org/`](http://www.json.org/) )

图 [3-3](#Fig3) 是一个示例 JSON，描述了一个带有不同数据类型的各种属性的术语表记录。

![A448827_1_En_3_Fig3_HTML.jpg](A448827_1_En_3_Fig3_HTML.jpg)

图 3-3。

Sample JSON (reference: [`http://www.json.org/`](http://www.json.org/) )

JSONs 被广泛用于跨系统发送信息。JSON 对象的 Python 等价物是`dict`数据类型，它本身是一个键值对结构。Python 有各种与 JSON 相关的库，提供抽象和实用函数。`json`库就是这样一个选项，它允许我们处理 JSON 文件/对象。让我们首先来看看我们的样本 JSON 文件，然后使用这个库将这些数据引入 Python 以供使用。

![A448827_1_En_3_Fig4_HTML.jpg](A448827_1_En_3_Fig4_HTML.jpg)

图 3-4。

Sample JSON with nested attributes

图 [3-4](#Fig4) 中的 JSON 对象描述了一个相当嵌套的结构，包含字符串、数字和数组类型的值。JSON 也支持对象、布尔值和其他数据类型作为值。下面的代码片段读取文件的内容，然后利用`json.loads()`实用程序解析并将其转换成标准的 Python `dict`。

```py
json_filedata = open(file_name).read()
json_data = json.loads(json_filedata)

```

`json_data`是一个 Python `dict`，JSON 文件的键和值被解析和类型转换为 Python 数据类型。`json`库还提供了将 Python 字典写回 JSON 文件的工具，具有错误检查和类型转换的能力。前面操作的输出如下。

```py
outer_col_1 :
        nested_inner_col_1 : val_1
        nested_inner_col_2 : 2
        nested_inner_col_1 : val_2
        nested_inner_col_2 : 2
outer_col_2 :
        inner_col_1 : 3
outer_col_3 : 4

```

在我们进入下一种格式之前，值得注意的是`pandas`也提供了解析 JSONs 的工具。`pandas read_json()`是一个非常强大的工具，它提供了多个选项来处理以不同风格创建的 JSONs。图 [3-5](#Fig5) 描绘了一个代表多个数据点的样本 JSON，每个数据点都有两个属性，分别为`col_1`和`col_2`。

![A448827_1_En_3_Fig5_HTML.jpg](A448827_1_En_3_Fig5_HTML.jpg)

图 3-5。

Sample JSON depicting records with similar attributes

我们可以很容易地使用`pandas`解析这样一个 JSON，方法是将`orientation`参数设置为“`records`”，如下所示。

```py
df = pd.read_json(file_name,orient="records")

```

输出是一个表格数据帧，每个数据点由两个属性值表示，如下所示。

```py
  col_1 col_2
0     a     b
1     c     d
2     e     f
3     g     h
4     i     j
5     k     l

```

鼓励你在 [`https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_json.html`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_json.html) 阅读更多关于`pandas read_json()`的内容。

### 可扩展置标语言

已经介绍了两种最广泛使用的数据格式，现在让我们来看看 XML。XML 是一种相当过时的格式，但仍被许多系统使用。XML 或可扩展标记语言是一种标记语言，它定义了对要在互联网上共享的数据/文档进行编码的规则。像 JSON 一样，XML 也是一种人类可读的文本格式。它的设计目标包括对各种人类语言的强大支持(通过 Unicode)、平台独立性和简单性。XML 广泛用于表示各种形状和大小的数据。

XML 被不同的系统广泛用作配置格式、元数据以及 RSS、SOAP 等服务的数据表示格式。

XML 是一种具有语法规则和模式的语言，经过多年的定义和完善。XML 最重要的组成部分如下:

*   标签:一种标记结构，由用尖括号(“”)括起来的字符串表示。
*   内容:标记语法中没有标记的任何数据都是 XML 文件/对象的内容。
*   元素:XML 的逻辑结构。一个元素可以用一个带有或不带有属性的开始和结束标签来定义，或者它可以只是一个空标签。
*   Attribute:表示所考虑元素的属性的键值对。这些包含在一个开始或空标签中。

图 3-6 是一个描述可扩展标记语言各种组件的 XML 样本。更多关键概念和细节可在 [`https://www.w3schools.com/xml/`](https://www.w3schools.com/xml/) 浏览。

![A448827_1_En_3_Fig6_HTML.jpg](A448827_1_En_3_Fig6_HTML.jpg)

图 3-6。

Sample XML annotated with key components

XML 可以被看作是树结构，从一个根元素开始，分支成不同的元素，每个元素都有自己的属性和进一步的分支，内容位于叶节点。

大多数 XML 解析器使用这种树状结构来读取 XML 内容。以下是两种主要类型的 XML 解析器:

*   DOM 解析器:文档对象模型解析器是最接近 XML 的树表示形式。它解析 XML 并生成树结构。DOM 解析器的一个很大的缺点是对于大型 XML 文件的不稳定性。
*   SAX 解析器:XML 的简单 API(简称 SAX)是一种在 web 上广泛使用的变体。这是一个基于事件的解析器，它一个元素一个元素地解析 XML，并提供钩子来根据标签触发事件。这克服了 DOM 基于内存的限制，但缺乏整体表示能力。

这两种类型衍生出多种变体。首先，让我们看看 Python 的`xml`库中可用的`ElementTree`解析器。`ElementTree`解析器是对 DOM 解析器的优化，它利用像`lists`和`dicts`这样的 Python 数据结构以简洁的方式处理数据。

下面的代码片段使用`ElementTree`解析器来加载和解析我们之前看到的示例 XML 文件。`parse()`函数返回一个树对象，这个树对象有各种属性、迭代器和工具，用于提取解析后的 XML 的根和其他组件。

```py
tree = ET.parse(file_name)
root = tree.getroot()

print("Root tag:{0}".format(root.tag))
print("Attributes of Root:: {0}".format(root.attrib))

```

两个`print`语句为我们提供了与根标签及其属性相关的值(如果有的话)。根对象还有一个迭代器，可以用来提取与所有子节点相关的信息。下面的代码片段迭代根对象以打印子节点的内容。

```py
for child in xml:root:
        print("{0}tag:{1}, attribute:{2}".format(
                                            "\t"*indent_level,
                                            child.tag,
                                            child.attrib))

        print("{0}tag data:{1}".format("\t"*indent_level,
                                        child.text))

```

使用`ElementTree`解析 XML 生成的最终输出如下。我们使用了一个定制的打印实用程序来提高输出的可读性，其代码可以在资源库中找到。

```py
Root tag:records
Attributes of Root:: {'attr': 'sample xml records'}
tag:record, attribute:{'name': 'rec_1'}
tag data:

        tag:sub_element, attribute:{}
        tag data:

                tag:detail1, attribute:{}
                tag data:Attribute 1
                tag:detail2, attribute:{}
                tag data:2
        tag:sub_element_with_attr, attribute:{'attr': 'complex'}
        tag data:
            Sub_Element_Text

        tag:sub_element_only_attr, attribute:{'attr_val': 'only_attr'}
        tag data:None
tag:record, attribute:{'name': 'rec_2'}
tag data:

        tag:sub_element, attribute:{}
        tag data:

                tag:detail1, attribute:{}
                tag data:Attribute 1
                tag:detail2, attribute:{}
                tag data:2
        tag:sub_element_with_attr, attribute:{'attr': 'complex'}
        tag data:
            Sub_Element_Text

        tag:sub_element_only_attr, attribute:{'attr_val': 'only_attr'}
        tag data:None

```

`xml`库提供了通过`ElementTree`解析器公开的非常有用的工具，但是它缺乏强大的火力。另一个 Python 库`xmltodict`提供了类似的功能，但是使用 Python 的原生数据结构，如`dicts`，提供了一种更 Python 化的方式来处理 XML。下面是解析同一个 XML 的快速代码片段。与`ElementTree`不同的是，`xmltodict`的`parse()`函数读取一个文件对象并将内容转换成嵌套的字典。

```py
xml_filedata = open(file_name).read()
ordered_dict = xmltodict.parse(xml_filedata)

```

生成的输出类似于使用`ElementTree`生成的输出，除了`xmltodict`使用`@`符号自动标记元素和属性。以下是示例输出。

```py
records :
        @attr : sample xml records
record :
                @name : rec_1
sub_element :
                        detail1 : Attribute 1
                        detail2 : 2
sub_element_with_attr :
                        @attr : complex
                        #text : Sub_Element_Text
sub_element_only_attr :
                        @attr_val : only_attr

```

### HTML 和抓取

我们在本章开始时谈到了以惊人的速度产生的大量信息/数据。互联网或网络是这场革命的驱动力之一，加上电脑、智能手机和平板电脑的巨大影响力。

互联网是一个巨大的互联网络，通过超链接连接信息。互联网上的大量数据是以网页的形式存在的。这些网页每天都被生成、更新和使用数百万次。随着信息驻留在这些网页中，我们必须学会如何交互和提取这些信息/数据。

到目前为止，我们已经处理了 CSV、JSON 和 XML 等格式，这些格式可以通过各种方法获得/提取，如手动下载、API 等。有了网页，方法就变了。在这一节中，我们将讨论 HTML 格式(最常见的网页相关格式)和网页抓取技术。

#### 超文本标记语言

超文本标记语言(HTML)是一种类似于 XML 的标记语言。HTML 主要由 web 浏览器和类似的应用程序用来呈现供消费的网页。

HTML 定义了使用标记描述网页的规则和结构。以下是 HTML 页面的标准组件:

*   元素:构成 HTML 页面基本构件的逻辑结构
*   标签:由尖括号(`<`和`>`)定义的标记结构。一些重要的标签是:
    *   `<html></html>`:这对标签包含了整个 HTML 文档。它标记了 HTML 页面的开始和结束。
    *   `<body></body>`:这对标签包含了浏览器呈现的 HTML 页面的主要内容。

HTML 标准中定义了更多的标准标签集；更多信息请访问 [`https://www.w3schools.com/html/html_intro.asp`](https://www.w3schools.com/html/html_intro.asp) 。

下面是一个用于生成由 web 浏览器呈现的 HTML 页面的代码片段，如图 [3-7](#Fig7) 中的截图所示。

![A448827_1_En_3_Fig7_HTML.jpg](A448827_1_En_3_Fig7_HTML.jpg)

图 3-7。

Sample HTML page as rendered in browser

```py
<!DOCTYPE html>
<html>
<head>
<title>Sample HTML Page</title>
</head>
<body>

<h1>Sample WebPage</h1>
<p>HTML has been rendered</p>

</body>
</html>

```

浏览器使用标记来理解特殊指令，如文本格式、定位、超链接等，但只呈现内容供最终用户查看。对于数据/信息驻留在 HTML 页面中的用例，我们需要特殊的技术来提取这些内容。

#### 网页抓取

Web 抓取是一种从 web 上，尤其是从网页上抓取或提取数据的技术。Web 抓取可能涉及手动复制数据或使用自动化从网页中抓取、解析和提取信息。在大多数情况下，web 抓取指的是自动抓取特定网站或 web 的一部分，以提取和解析稍后可用于分析或其他用例的信息。典型的网页抓取流程可以总结如下:

*   爬行:一个 bot 或 web crawler 被设计为使用所需的 URL 集来查询 web 服务器，以获取 web 页面。爬行器可以使用复杂的技术从链接到相关 URL 的页面中获取信息，甚至在一定程度上解析信息。网站维护一个名为`robots.txt`的文件，使用所谓的“机器人排除协议”来限制/提供对其内容的访问。更多详情可登陆 [`http://www.robotstxt.org/robotstxt.html`](http://www.robotstxt.org/robotstxt.html) 。
*   抓取:获取原始网页后，下一个任务是从中提取信息。抓取任务包括利用正则表达式、基于 XPath 的提取或特定标签等技术来缩小页面上所需信息的范围。

从缩小到所需信息的角度来看，网络搜集涉及到创造性。随着网站不断变化和网页变得动态(见`asp, jsp`等)。)、访问控制(用户名/密码、验证码等)的存在使任务更加复杂。Python 是一种非常强大的编程语言，这一点现在应该很明显了，抓取 web 是它提供多种实用工具的另一项任务。为了更好地理解网络抓取，让我们从从 Apress 博客中提取一篇博文的文本开始。

第一个任务是识别我们感兴趣的 URL。对于我们当前的例子，我们专注于在 [`http://www.apress.com/in/blog/all-blog-posts`](http://www.apress.com/in/blog/all-blog-posts) 的一个新闻网站的博客页面上当天的第一篇博客文章。点击最上面的博客文章会把我们带到考虑中的主要文章。文章如图 [3-8](#Fig8) 画面所示。

![A448827_1_En_3_Fig8_HTML.jpg](A448827_1_En_3_Fig8_HTML.jpg)

图 3-8。

A blog post on Apress.com

现在我们已经有了所需的页面及其 URL，我们将使用`requests`库来查询所需的 URL 并获得响应。下面的代码片段做了同样的事情。

```py
base_url = "http://www.apress.com/in/blog/all-blog-posts"
blog_suffix = "/wannacry-how-to-prepare/12302194"

response = requests.get(base_url+blog_suffix)

```

如果`get`请求成功，响应对象的`status_code`属性包含值 200(相当于 HTML 成功代码)。在得到成功的响应后，下一个任务是设计一种方法来提取所需的信息。

既然在这种情况下我们对博客文章的实际内容感兴趣，让我们分析页面背后的 HTML，看看我们是否能找到感兴趣的特定标签。

Note

大多数现代浏览器都内置了 HTML 检查工具。如果您使用的是 Google Chrome，请按 F12 或右键单击页面并选择 Inspect 或 View Source。这将打开 HTML 代码供您分析。

图 [3-9](#Fig9) 描绘了我们感兴趣的博文背后的 HTML 的快照。

![A448827_1_En_3_Fig9_HTML.jpg](A448827_1_En_3_Fig9_HTML.jpg)

图 3-9。

Inspecting the HTML content of a blog post on Apress.com

仔细观察，我们可以清楚地看到博文的文本包含在`div`标签`<div class="cms-richtext">`中。现在我们已经缩小到感兴趣的标签，我们使用 Python 的正则表达式库`re`来搜索和提取包含在这些标签中的数据。下面的代码片段利用`re.compile()`编译一个正则表达式，然后使用`re.findall()`从获取的响应中提取信息。

```py
content_pattern = re.compile(r'<div class="cms-richtext">(.*?)</div>')
result = re.findall(content_pattern, content)

```

`find`操作的输出是博文中所需的文本。当然，它仍然包含交错在实际文本之间的 HTML 标签。我们可以执行进一步的清理，以达到所需的水平，但这是一个良好的开端。以下是使用正则表达式提取的信息的快照。

```py
Out[59]: '<p class="intro--paragraph"><em>By Mike Halsey</em></p><p><br/></p><p>It was a perfectly ordinary Friday when the Wannacry ransomware struck in May 2017\. The malware spread around the world to more than 150 countries in just a matter of a few hours, affecting the National Health Service in the UK, telecoms provider Telefonica in Spain, and many other organisations and businesses in the USA, Canada, China, Japan, Russia, and right across Europe, the Middle-East, and Asia.</p><p>The malware was reported to have been stolen in an attack on the US National Security Agency (NSA), though the NSA denied this, and exploited vulnerabilities in the Microsoft Windows operating system. Microsoft had been aware of the vulnerabilities since early in the year, and had patched them back in March.

```

这是获取所需数据的一种简单且非常基本的方法。如果我们想更进一步，提取与页面上所有博客文章相关的信息，并执行更好的清理，该怎么办？

对于这样的任务，我们利用了`BeautifulSoup`库。`BeautifulSoup`是网页抓取和相关任务的标准库。它提供了一些惊人的功能来减轻刮擦过程。对于手头的任务，我们的过程将是首先抓取索引页面，并提取页面上列出的所有博客文章链接的 URL。为此，我们将使用`requests.get()`函数提取内容，然后利用`BeautifulSoup`的工具从 URL 获取内容。下面的代码片段展示了函数`get_post_mapping()`，它解析主页内容，将博文标题和相应的 URL 提取到一个字典中。该函数最终返回这样的字典列表。

```py
def get_post_mapping(content):
    """This function extracts blog post title and url from response object

    Args:
        content (request.content): String content returned from requests.get

    Returns:
        list: a list of dictionaries with keys title and url

    """
    post_detail_list = []
    post_soup = BeautifulSoup(content,"lxml")
    h3_content = post_soup.find_all("h3")

    for h3 in h3_content:
        post_detail_list.append(
            {'title':h3.a.get_text(),'url':h3.a.attrs.get('href')}
            )

    return post_detail_list

```

pervious 函数首先创建一个对象`BeautifulSoup`，指定`lxml`作为它的解析器。然后，它使用`h3`标签和基于正则表达式的搜索来提取所需的标签列表(我们通过前面使用的相同的 inspect 元素方法获得了`h3`标签)。下一个任务是简单地遍历`h3`标签列表，并利用来自`BeautifulSoup`的`get_text()`实用函数来获取博文标题及其对应的 URL。该函数返回的列表如下。

```py
[{'title': u"Wannacry: Why It's Only the Beginning, and How to Prepare for What Comes Next",
  'url': '/in/blog/all-blog-posts/wannacry-how-to-prepare/12302194'},
 {'title': u'Reusing ngrx/effects in Angular (communicating between reducers)',
  'url': '/in/blog/all-blog-posts/reusing-ngrx-effects-in-angular/12279358'},
 {'title': u'Interview with Tony Smith - Author and SharePoint Expert',
  'url': '/in/blog/all-blog-posts/interview-with-tony-smith-author-and-sharepoint-expert/12271238'},
 {'title': u'Making Sense of Sensors \u2013 Types and Levels of Recognition',
  'url': '/in/blog/all-blog-posts/making-sense-of-sensors/12253808'},
 {'title': u'VS 2017, .NET Core, and JavaScript Frameworks, Oh My!',
  'url': '/in/blog/all-blog-posts/vs-2017-net-core-and-javascript-frameworks-oh-my/12261706'}]

```

现在我们有了列表，最后一步是遍历这个 URL 列表并提取每篇博文的文本。下面的函数展示了与我们之前使用正则表达式的方法相比，`BeautifulSoup`是如何简化任务的。识别所需标签的方法保持不变，尽管我们利用这个库的能力来获得没有所有 HTML 标签的文本。

```py
def get_post_content(content):
    """This function extracts blog post content from response object

    Args:
        content (request.content): String content returned from requests.get

    Returns:
        str: blog's content in plain text

    """
    plain_text = ""
    text_soup = BeautifulSoup(content,"lxml")
    para_list = text_soup.find_all("div",
                                   {'class':'cms-richtext'})

    for p in para_list[0]:
        plain_text += p.getText()

    return plain_text

```

以下输出是其中一篇文章的内容。与我们之前的方法相比，注意这种情况下更清晰的文本。

```py
By Mike HalseyIt was a perfectly ordinary Friday when the Wannacry ransomware struck in May 2017\. The malware spread around the world to more than 150 countries in just a matter of a few hours, affecting the National Health Service in the UK, telecoms provider Telefonica in Spain, and many other organisations and businesses in the USA, Canada, China, Japan, Russia, and right across Europe, the Middle-East, and Asia.The malware was reported to have been stolen in an attack on the US National Security Agency (NSA), though the NSA denied this, and exploited vulnerabilities in the Microsoft Windows operating system. Microsoft had been aware of the vulnerabilities since early in the year, and had patched them back in March.

```

通过这两种方法，我们从感兴趣的网站中抓取并提取了与博客文章相关的信息。为了更好地理解，我们鼓励您尝试来自`BeautifulSoup`和其他网站的其他实用程序。当然，请务必阅读`robots.txt`并遵守网站管理员制定的规则。

### 结构化查询语言

数据库可以追溯到 20 世纪 70 年代，代表了以关系形式存储的大量数据。数据库中以表格形式可用的数据，或者更具体地说，关系数据库，由我们在处理不同用例时遇到的另一种格式的结构化数据组成。多年来，已经有了各种风格的数据库，其中大多数都符合 SQL 标准。

Python 生态系统以两种主要方式处理来自数据库的数据。在处理数据科学和相关用例时，第一种也是最常用的方法是直接使用 SQL 查询访问数据。为了使用 SQL 查询访问数据，像`sqlalchemy`和`pyodbc`这样强大的库提供了方便的接口来连接、提取和操作来自各种关系数据库的数据，如 MS SQL Server、MySQL、Oracle 等等。`sqlite3`库提供了一个轻量级的易于使用的接口来处理 SQLite 数据库，尽管其他两个库也可以处理同样的问题。

与数据库交互的第二种方式是 ORM 或对象关系映射方法。这种方法与面向对象的数据模型是同义的，也就是说，关系数据是按照对象和类来映射的。提供一个高级接口，以 ORM 方式与数据库交互。我们将在随后的章节中基于用例对这些进行更多的探索。

## 数据描述

在上一节中，我们讨论了各种数据格式和从中提取信息的方法。每种数据格式都由具有不同类型属性的数据点组成。这些原始数据形式的数据类型构成了机器学习算法和整个数据科学工作流中的其他任务所使用的输入特征的基础。在这一节中，我们将讨论在不同用例中处理的主要数据类型。

### 数字的

这是最简单的可用数据类型。它也是大多数算法可以直接使用和理解的类型(尽管这并不意味着我们使用原始形式的数字数据)。数字数据表示被观察实体的标量信息，例如，访问网站的次数、产品价格、人的体重等等。数值也是矢量特征的基础，其中每个维度都由一个标量值表示。数值数据的比例、范围和分布对算法和/或整个工作流有着隐含的影响。为了处理数字数据，我们使用诸如标准化、宁滨、量化等技术来根据我们的要求转换数字数据。

### 文本

由非结构化字母数字内容组成的数据是最常见的数据类型之一。当表示人类语言内容时，文本数据包含隐含的语法结构和意义。这种类型的数据需要额外的关注和努力来转换和理解。我们将在接下来的章节中讨论文本数据的转换和使用。

### 绝对的

这种数据类型介于数字和文本之间。分类变量是指被观察的实体的类别。例如，头发颜色是黑色、棕色、金色和红色，或者经济地位是低、中或高。这些值可以表示为数字或字母数字，它们描述了所考虑的项目的属性。基于某些特征，分类变量可以被视为:

*   名义上的:这些仅定义数据点的类别，没有任何可能的排序。例如，头发颜色可以是黑色、棕色、金色等。，但这些类别不能有任何顺序。
*   Ordinal:它们定义了类别，但也可以根据上下文规则进行排序。例如，按低、中或高经济地位分类的人可以按各自的顺序清楚地排序/分类。

值得注意的是，标准的数学运算，如加、减、乘等。不要携带分类变量的含义，即使这在语法上是允许的(分类变量用数字表示)。因此，小心处理分类变量是很重要的，我们将在下一节看到处理分类数据的几种方法。

不同的数据类型构成了算法为分析手头数据而摄取的特征的基础。在接下来的章节中，尤其是第 [4](04.html) 章:特征工程和选择，你将会学到更多关于如何处理特定数据类型的知识。

## 数据争论

到目前为止，我们已经讨论了数据格式和数据类型，并了解了从不同来源收集数据的方法。现在，我们已经了解了收集和理解数据的初始过程，下一个合乎逻辑的步骤是能够基于手头的用例使用各种机器学习算法来使用它进行分析。但是，在我们到达这个“原始”数据接近可用于算法或可视化的阶段之前，我们需要对它进行润色和塑造。

数据争论或数据管理是将数据从一种形式清理、转换和映射到另一种形式的过程，以利用它来完成诸如分析、汇总、报告、可视化等任务。

### 理解数据

数据争论是整个数据科学工作流程中最重要和最复杂的步骤之一。该过程的输出直接影响所有下游步骤，如探索、总结、可视化、分析，甚至最终结果。这清楚地表明了为什么数据科学家要花大量时间在数据收集和争论上。有很多调查有助于揭示这样一个事实:数据科学家往往会将 80%的时间花在数据处理和争论上！

因此，在我们开始下一章的实际用例及算法之前，我们必须了解并学习如何处理数据，并将其转化为可用的形式。首先，让我们先描述一下手头的数据集。为了简单起见，我们准备了一个描述某些用户的产品购买交易的样本数据集。因为我们已经讨论了收集/提取数据的方法，所以在本节中我们将跳过这一步。图 [3-10](#Fig10) 显示了数据集的快照。

![A448827_1_En_3_Fig10_HTML.jpg](A448827_1_En_3_Fig10_HTML.jpg)

图 3-10。

Sample dataset Note

所考虑的数据集是使用标准 Python 库生成的，如`random, datetime, numpy, pandas`等等。这个数据集是使用本书代码库中的一个名为`generate_sample_data()`的实用函数生成的。这些数据是随机生成的，仅供参考。

数据集描述了具有以下属性/特征/特性的交易:

*   日期:交易的日期
*   价格:购买产品的价格
*   产品 ID:产品识别号
*   采购数量:此事务处理中采购的产品数量
*   序列号:事务处理序列号
*   用户 ID:执行交易的用户的标识号
*   用户类型:用户的类型

现在，让我们开始我们的争论/管理过程，并了解各种方法/技巧来清理、转换和映射我们的数据集，使其成为可用的形式。第一步也是最重要的一步通常是快速了解记录/行的数量、列/属性的数量、列/属性名称以及它们的数据类型。

对于本节的大部分内容和后续内容，我们将依赖于`pandas`及其实用程序来执行所需的任务。下面的代码片段提供了关于行数、属性数和细节的详细信息。

```py
print("Number of rows::",df.shape[0])
print("Number of columns::",df.shape[1] )

print("Column Names::",df.columns.values.tolist())

print("Column Data Types::\n",df.dtypes)

```

所需信息可直接从`pandas`数据帧本身获得。shape 属性是一个双值元组，分别表示行数和列数。通过`columns`属性可以获得列名，而`dtypes`属性为我们提供了数据集中每一列的数据类型。以下是该代码片段生成的输出。

```py
Number of rows:: 1001
Number of columns:: 7

Column Names:: ['Date', 'Price', 'Product ID', 'Quantity Purchased', 'Serial No', 'User ID', 'User Type']

Column Data Types::
Date                   object
Price                 float64
Product ID              int32
Quantity Purchased      int32
Serial No               int32
User ID                 int32
User Type              object
dtype: object

```

列名已经清楚地列出，并且已经在前面解释过了。通过检查数据类型，我们可以清楚地看到,`Date`属性被表示为一个对象。在我们继续进行转换和清理之前，让我们深入研究并收集更多信息，以了解和准备数据集争论所需任务的策略。下面的代码片段有助于获取与包含缺失值的属性/列、行数以及其中包含缺失值的索引相关的信息。

```py
print("Columns with Missing Values::",df.columns[df.isnull().any()].tolist())

print("Number of rows with Missing Values::",len(pd.isnull(df).any(1).nonzero()[0].tolist()))

print("Sample Indices with missing data::",pd.isnull(df).any(1).nonzero()[0].tolist()[0:5] )

```

使用`pandas`，下标可用于行和列(详见第 [2](02.html) 章)。我们使用`isnull()`来标识包含缺失值的列。实用程序`any()`和`nonzero()`提供了很好的抽象来标识任何符合条件的行/列(在这种情况下指向缺少值的行/列)。输出如下。

```py
Columns with Missing Values:: ['Date', 'Price', 'User Type']
Number of rows with Missing Values:: 61
Sample Indices with missing data:: [0L, 1L, 6L, 7L, 10L]

```

让我们做一个快速的事实检查，以获得每一列的非空行的详细信息以及该数据帧消耗的内存量。我们还会得到一些基本的汇总统计数据，如最小值、最大值等等；这些在接下来的任务中会很有用。对于第一个任务，我们使用`info()`实用程序，而汇总统计数据由`describe()`函数提供。下面的代码片段可以做到这一点。

```py
print("General Stats::")
print(df.info())

print("Summary Stats::" )
print(df.describe())

```

以下是使用`info()`和`describe()`实用程序生成的输出。它显示日期和价格都有大约 970 个非空行，而数据集消耗了将近 40KB 的内存。摘要统计信息是不言自明的，并从输出中删除了日期和用户类型等非数字列。

```py
General Stats::
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 1001 entries, 0 to 1000
Data columns (total 7 columns):
Date                  970 non-null object
Price                 970 non-null float64
Product ID            1001 non-null int32
Quantity Purchased    1001 non-null int32
Serial No             1001 non-null int32
User ID               1001 non-null int32
User Type             1000 non-null object
dtypes: float64(1), int32(4), object(2)
memory usage: 39.2+ KB
None

Summary Stats::
             Price   Product ID  Quantity Purchased    Serial No      User ID
count   970.000000  1001.000000         1001.000000  1001.000000  1001.000000
mean   1523.906402   600.236763           20.020979  1452.528472  5335.669331
std    1130.331869   308.072110           11.911782   386.505376   994.777199
min       2.830000     0.000000            0.000000    -1.000000  -101.000000
25%     651.622500   342.000000           10.000000  1223.000000  5236.000000
50%    1330.925000   635.000000           20.000000  1480.000000  5496.000000
75%    2203.897500   875.000000           30.000000  1745.000000  5726.000000
max    5840.370000  1099.000000           41.000000  2000.000000  6001.000000

```

### 过滤数据

我们已经完成了手头数据集的第一遍，并了解了它有什么和缺少什么。下一阶段是清理。清理数据集包括移除/处理不正确或缺失的数据、处理异常值等任务。清理还包括标准化属性列名，使它们更易读、更直观，并符合特定的标准，以便每个相关人员都能理解。为了执行这个任务，我们编写一个小函数，并利用`pandas`的`rename()`实用程序来完成这个步骤。`rename()`函数接受一个`dict`，其中的键代表旧的列名，而值指向新的列名。我们也可以通过适当地设置`inplace`标志来决定修改现有的数据帧或生成一个新的数据帧。下面的代码片段展示了这个函数。

```py
def cleanup_column_names(df,rename_dict={},do_inplace=True):
    """This function renames columns of a pandas dataframe
       It converts column names to snake case if rename_dict is not passed.
    Args:
        rename_dict (dict): keys represent old column names and values point to
                            newer ones
        do_inplace (bool): flag to update existing dataframe or return a new one
    Returns:
        pandas dataframe if do_inplace is set to False, None otherwise

    """
    if not rename_dict:
        return df.rename(columns={col: col.lower().replace(' ','_')
                    for col in df.columns.values.tolist()},
                  inplace=do_inplace)
    else:
        return df.rename(columns=rename_dict,inplace=do_inplace)

```

在我们考虑的`dataframe`上使用该函数后，产生图 [3-11](#Fig11) 中的输出。由于我们没有传递任何带有新旧列名的`dict`，该函数将所有列更新为 snake case。

![A448827_1_En_3_Fig11_HTML.jpg](A448827_1_En_3_Fig11_HTML.jpg)

图 3-11。

Dataset with columns renamed

对于不同的算法、分析甚至可视化，我们通常只需要处理属性的子集。使用`pandas`，我们可以以多种方式垂直切片(选择列的子集)。`pandas`提供了不同的方法来适应不同的场景，我们将在下面的代码片段中看到。

```py
print("Using Column Index::" )
print(df[[3]].values[:, 0] )

print("Using Column Name::" )
print(df.quantity_purchased.values)

print(Using Column Data Type::" )
print(df.select_dtypes(include=['float64']).values[:,0] )

```

在这个代码片段中，我们以三种不同的方式执行了属性选择。第一种方法利用列索引号来获取所需的信息。在这种情况下，我们希望只处理字段`quantity_purchased`，因此索引号为 3 ( `pandas`列的索引为 0)。第二种方法也通过直接引用点标记中的列名来提取相同属性的数据。当在循环中工作时，第一种方法非常方便，而当我们在 Python 上利用面向对象的特性时，第二种方法可读性更好，也更好。然而，有时我们需要仅根据属性的数据类型来获取属性。第三种方法利用`select_dtypes()`工具来完成这项工作。它提供了仅基于数据类型包含和排除列的方法。在本例中，我们选择了数据类型为 float 的列(数据集中的 price 列)。这个片段的输出如下。

```py
Using Column Index::
[13  1  2 ...,  2 30 17]

Using Column Name::
[13  1  2 ...,  2 30 17]

Using Column Data Type::
[ 3021.06  1822.62   542.36 ...,  1768.66  1848.5   1712.22]

```

选择特定的属性/列是设置数据帧子集的方法之一。可能还需要水平分割数据帧。为了处理行的子集，`pandas`提供了一些方法，如下面的代码片段所示。

```py
print("Select Specific row indices::")
print(df.iloc[[10,501,20]] )

print(Excluding Specific Row indices::" )
print(df.drop([0,24,51], axis=0).head())

print("Subsetting based on logical condition(s)::" )
print(df[df.quantity_purchased>25].head())

print("Subsetting based on offset from top (bottom)::" )
print(df[100:].head() #df.tail(-100) )

```

第一种方法利用基于`iloc`(或整数索引/位置)的选择，我们需要从数据帧中指定我们需要的索引列表。第二种方法允许从数据帧本身移除/过滤掉特定的行索引。在需要过滤掉不满足特定标准的行的情况下，这很方便。第三种方法展示了基于条件逻辑的行过滤。最后一种方法基于从数据帧顶部的偏移进行过滤。称为`tail()`的类似方法也可用于从底部偏移。生成的输出如图 [3-12](#Fig12) 所示。

![A448827_1_En_3_Fig12_HTML.jpg](A448827_1_En_3_Fig12_HTML.jpg)

图 3-12。

Different ways of subsetting rows

### 铅字铸造

类型转换或者将数据转换成适当的数据类型，通常是清理和争论的重要部分。在从一种形式提取或转换为另一种形式时，数据经常被转换为错误的数据类型。此外，不同的平台和系统以不同的方式处理每种数据类型，因此获得正确的数据类型非常重要。在开始争论时，我们检查了数据集所有列的数据类型。如果您还记得，日期列被标记为对象。虽然如果我们不打算处理日期，这可能不是问题，但是在我们需要日期和相关属性的情况下，将它们作为对象/字符串可能会带来问题。此外，如果日期操作是以字符串的形式提供的，则很难处理日期操作。为了修复我们的数据帧，我们使用来自`pandas`的`to_datetime()`函数。这是一个非常灵活的工具，允许我们设置不同的属性，如日期时间格式、时区等等。因为在我们的例子中，值只是日期，所以我们使用下面的函数作为默认值。

```py
df['date'] = pd.to_datetime(df.date)
print(df.dtypes)

```

类似地，我们可以使用`to_numeric()`以及直接的 Python 风格类型转换来转换标记为字符串的数字列。现在检查数据类型，我们清楚地看到日期列在正确的数据类型`datetime64`中。

```py
date                  datetime64[ns]
price                        float64
product_id                     int32
quantity_purchased             int32
serial_no                      int32
user_id                        int32
user_type                     object
dtype: object

```

### 转换

数据争论的另一个常见任务是根据用例或数据本身的需求转换现有的列或派生新的属性。为了派生或转换列，`pandas`提供了三个不同的工具— `apply()`、`applymap()`和`map()`。`apply()`功能用于根据轴对整个对象执行操作(默认为所有行)。`applymap()`和`map()`函数与来自`pandas.Series`层次结构的`map()`一起工作。

作为理解这三个实用程序的例子，让我们导出一些新的属性。首先，让我们使用`map()`函数扩展`user_type`属性。我们编写一个小函数，将每个不同的`user_type`代码映射到它们对应的用户类，如下所示。

```py
def expand_user_type(u_type):
        if u_type in ['a','b']:
            return 'new'
        elif u_type == 'c':
            return 'existing'
        elif u_type == 'd':
            return 'loyal_existing'
        else:
            return 'error'

df['user_class'] = df['user_type'].map(expand_user_type)

```

同样，我们使用`applymap()`函数执行另一个基于元素的操作，从 date 属性中获取事务的星期。对于这种情况，我们使用`lambda`函数来快速完成工作。关于`lambda`功能的更多细节，请参考前面的章节。下面的代码片段为我们提供了每笔交易的周数。

```py
df['purchase_week'] = df[['date']].applymap(lambda dt:dt.week
                                                if not pd.isnull(dt.week)
                                                else 0)

```

图 [3-13](#Fig13) 用两个额外的属性描述了我们的数据帧— `user_class`和`purchase_week`。

![A448827_1_En_3_Fig13_HTML.jpg](A448827_1_En_3_Fig13_HTML.jpg)

图 3-13。

Dataframe with derived attributes using map and applymap

现在让我们使用`apply()`函数对整个 dataframe 对象本身执行操作。下面的代码片段使用`apply()`函数获取所有数值属性的范围(最大值到最小值)。我们使用前面讨论过的`select_dtypes`和`lambda`函数来完成任务。

```py
df.select_dtypes(include=[np.number]).apply(lambda x: x.max()- x.min())

```

输出是一个缩小的`pandas.Series`对象，显示每个数字列的范围值。

```py
price                 5837.54
product_id            1099.00
quantity_purchased      41.00
serial_no             2001.00
user_id               6102.00
purchase_week           53.00

```

### 输入缺失值

在处理机器学习和数据科学相关的用例时，缺少值会导致各种各样的问题。它们不仅会给算法带来问题，还会扰乱计算，甚至最终结果。缺少值还会带来以非标准方式解释的风险，并导致混乱和更多错误。因此，在整个数据争论过程中，输入缺失值具有很大的分量。

处理缺失值的最简单方法之一是忽略或从数据集中完全删除它们。当数据集相当大，并且我们需要足够多的各种类型的样本时，可以安全地使用这个选项。在下面的代码片段中，我们使用来自`pandas`的`dropna()`函数来删除缺少交易日期的数据行。

```py
print("Drop Rows with missing dates::" )
df_dropped = df.dropna(subset=['date'])
print("Shape::",df_dropped.shape)

```

结果是数据帧中的行没有任何遗漏的日期。输出数据帧如图 [3-14](#Fig14) 所示。

![A448827_1_En_3_Fig14_HTML.jpg](A448827_1_En_3_Fig14_HTML.jpg)

图 3-14。

Dataframe without any missing date information

删除行通常是一个非常昂贵且不可行的选择。在许多情况下，缺失值是借助数据框中的其他值来估算的。一个常用的技巧是用类似于`mean`或`median`的集中趋势度量来替换缺失值。人们也可以选择其他复杂的测量/统计。在我们的数据集中，价格列似乎有一些缺失的数据。我们利用 pandas 的`fillna()`方法，用数据框架中的平均价格值填充这些值。

在同样的行中，我们使用`ffill()`和`bfill()`函数来估算`user_type`属性的缺失值。因为，`user_type`是一个字符串类型属性，所以在这种情况下，我们使用一个基于近似的解决方案来处理缺失值。`ffill()`和`bfill()`函数向前复制前一行的数据(向前填充)或复制下一行的值(向后填充)。下面的代码片段展示了这三个函数。

```py
print("Fill Missing Price values with mean price::" )
df_dropped['price'].fillna(value=np.round(df.price.mean(),decimals=2),
                            inplace=True)

print("Fill Missing user_type values with value from \
         previous row (forward fill) ::" )
df_dropped['user_type'].fillna(method='ffill',inplace=True)

print("Fill Missing user_type values with value from \
        next row (backward fill) ::" )
df_dropped['user_type'].fillna(method='bfill',inplace=True)

```

除了这些方法之外，在某些情况下，如果缺少超过某个阈值的属性值，记录就没有多大用处。例如，如果在我们的数据集中，一个事务的非空属性少于三个，那么这个事务几乎是不可用的。在这种情况下，删除该数据点本身可能是明智的。我们可以使用函数`dropna()`将参数`thresh`设置为非空属性的阈值来过滤掉这样的数据点。更多细节可在官方文档页面上找到。

### 处理重复

许多数据集的另一个问题是存在重复。虽然数据很重要，而且越多越好，但副本本身并没有增加多少价值。此外，副本有助于我们识别记录/收集数据本身的潜在错误领域。为了识别重复项，我们有一个名为`duplicated()`的实用程序，它可以应用于整个数据帧以及数据帧的子集。我们可以通过修复错误和使用`duplicated()`函数来处理重复数据，尽管我们也可以选择完全删除重复数据点。为了删除重复项，我们使用了`drop_duplicates()`方法。下面的代码片段展示了这里讨论的两个函数。

```py
df_dropped[df_dropped.duplicated(subset=['serial_no'])]

df_dropped.drop_duplicates(subset=['serial_no'],inplace=True)

```

图 [3-15](#Fig15) 描述了识别字段`serial_no`具有重复值的数据帧子集的输出。前面代码片段中的第二行只是删除了这些重复的代码。

![A448827_1_En_3_Fig15_HTML.jpg](A448827_1_En_3_Fig15_HTML.jpg)

图 3-15。

Dataframe with duplicate serial_no values

### 处理分类数据

正如在“数据描述”一节中所讨论的，分类属性由取值数量有限的数据组成(但并不总是如此)。在我们的数据集中，属性 user_type 是一个分类变量，它只能从允许的集合{a，b，c，d}中获取有限数量的值。我们将在接下来的章节中学习和使用的算法主要处理数字数据，而分类变量可能会带来一些问题。使用`pandas`，我们可以用几种不同的方式处理分类变量。第一个是使用`map()`函数，我们简单地将允许集中的每个值映射为一个数值。虽然这可能是有用的，但这种方法应该小心谨慎。例如，像加法、均值等统计操作，虽然在语法上是有效的，但出于明显的原因应该避免(在接下来的章节中会有更多的介绍)。第二种方法是使用`get_dummies()`函数将分类变量转换成指示变量。该函数只是一个包装器，为所考虑的变量生成一个热编码。一种热编码和其他编码也可以使用像`sklearn`这样的库来处理(我们将在接下来的章节中看到更多的例子)。

下面的代码片段展示了前面讨论的使用`map()`和`get_dummies()`的两种方法。

```py
# using map to dummy encode
type_map={'a':0,'b':1,'c':2,'d':3,np.NAN:-1}
df['encoded_user_type'] = df.user_type.map(type_map)
print(df.head())

# using get_dummies to one hot encode
print(pd.get_dummies(df,columns=['user_type']).head())

```

输出如图 [3-16](#Fig16) 和图 [3-17](#Fig17) 所示。图 [3-16](#Fig16) 显示了虚拟编码的输出。使用`map()`方法，我们控制了特性的数量，但是必须小心本节提到的警告。

![A448827_1_En_3_Fig16_HTML.jpg](A448827_1_En_3_Fig16_HTML.jpg)

图 3-16。

Dataframe with user_type attribute dummy encoded

第二个图像，图 [3-17](#Fig17) ，展示了一个热编码`user_type`属性的输出。

![A448827_1_En_3_Fig17_HTML.jpg](A448827_1_En_3_Fig17_HTML.jpg)

图 3-17。

Dataframe with user_type attribute one hot encoded

当我们讨论特征工程时，我们将在第 [4](04.html) 章详细讨论这些方法。

### 标准化值

属性规范化是对属性值范围进行标准化的过程。在许多情况下，机器学习算法利用不同尺度/范围的距离度量、属性或特征，这可能不利地影响计算或使结果有偏差。归一化也称为特征缩放。有各种缩放/归一化要素的方法，其中一些是重缩放、标准化(或零均值单位方差)、单位缩放等等。我们可以根据手头的特性、算法和用例来选择一种规范化技术。当我们处理用例时，这一点会更清楚。我们还将在第 4 章[中详细介绍特征缩放策略:特征工程和选择。下面的代码片段展示了一个使用最小-最大缩放器的快速示例，可从`sklearn`的`preprocessing`模块获得，它将属性重新缩放到所需的给定范围。](04.html)

```py
df_normalized = df.dropna().copy()
min_max_scaler = preprocessing.MinMaxScaler()
np_scaled = min_max_scaler.fit_transform(df_normalized['price'].reshape(-1,1))
df_normalized['normalized_price'] = np_scaled.reshape(-1,1)

```

图 [3-18](#Fig18) 展示了未缩放的价格值和已缩放至`[0, 1]`范围的标准化价格值。

![A448827_1_En_3_Fig18_HTML.jpg](A448827_1_En_3_Fig18_HTML.jpg)

图 3-18。

Original and normalized values for price

### 字符串操作

原始数据在用于分析之前会出现各种各样的问题和复杂性。字符串是另一类原始数据，在我们的算法能够理解它们之前，需要特别注意和处理。正如在讨论分类数据的争论方法时提到的，在算法中直接使用字符串数据存在局限性和问题。

表示自然语言的字符串数据非常嘈杂，需要自己的一套步骤来解决。虽然这些步骤中的大部分都是依赖于用例的，但是在这里还是值得一提的(为了更加清晰，我们将在用例中详细介绍这些步骤)。字符串数据通常会经历一些复杂的步骤，例如:

*   标记化:将字符串数据分割成组成单元。比如把句子拆分成单词或者单词拆分成字符。
*   词干化和词汇化:这些是将单词转化为其词根或规范形式的规范化方法。词干化是一个获得词根形式的启发式过程，而词汇化利用语法和词汇规则来派生词根。
*   停用字词删除:文本包含出现频率高但没有传达太多信息的字词(标点符号、连词等)。这些单词/短语通常被删除以减少数据的维数和噪音。

除了前面提到的三个常见步骤，还有其他操作，如词性标注、哈希、索引等等。这些都是必需的，并根据手头的数据和问题陈述进行调整。请在接下来的章节中关注更多的细节。

## 数据汇总

数据汇总是指准备手头原始数据的压缩表示的过程。这个过程包括使用不同的统计、数学和其他方法对数据进行汇总。汇总有助于可视化、压缩原始数据以及更好地理解其属性。

库提供了各种强大的摘要技术来满足不同的需求。我们也将在这里讨论其中的一些。最广泛使用的汇总形式是根据某些条件或属性对值进行分组。下面的片段演示了这样一个总结。

```py
print(df['price'][df['user_type']=='a'].mean())
print(df['purchase_week'].value_counts())

```

第一个语句通过`user_type`计算所有交易的平均价格，而第二个语句计算每周的交易数量。尽管这些计算很有帮助，但是根据属性对数据进行分组有助于我们更好地理解它。`groupby()`函数帮助我们执行同样的操作，如下面的代码片段所示。

```py
print(df.groupby(['user_class'])['quantity_purchased'].sum())

```

该语句生成一个表格输出，表示每个`user_class`购买的数量总和。生成的输出如下。

```py
user_class
existing           4830
loyal_existing     5515
new               10100
Name: quantity_purchased, dtype: int32

```

`groupby()`函数是一个强大的接口，允许我们执行复杂的分组和聚合。在前面的示例中，我们仅根据单个属性进行分组，并执行单个聚合(即求和)。使用`groupby()`,我们可以执行多属性分组，并跨属性应用多个聚合。下面的代码片段展示了`groupby()`用法的三种变体及其相应的输出。

```py
# variant-1: multiple aggregations on single attribute
df.groupby(['user_class'])['quantity_purchased'].agg([np.sum, np.mean,
                                                              np.count_nonzero])

# variant-2: different aggregation functions for each attribute
df.groupby(['user_class','user_type']).agg({'price':np.mean,
                                                            'quantity_purchased':np.max})

# variant-3
df.groupby(['user_class','user_type']).agg({'price':{'total_price':np.sum,
                                                               'mean_price':np.mean,
                                                               'variance_price':np.std,
                                                               'count':np.count_nonzero},
                                                               'quantity_purchased':np.sum})

```

这三种不同的变体可以解释如下。

变体 1:这里我们对按`user_class`分组的采购数量应用三种不同的汇总(见图 [3-19](#Fig19) )。

![A448827_1_En_3_Fig19_HTML.jpg](A448827_1_En_3_Fig19_HTML.jpg)

图 3-19。

Groupby with multiple aggregations on single attribute

变体 2:这里，我们对两个不同的属性应用不同的聚合函数。`agg()`函数将字典作为输入，字典包含作为键的属性和作为值的聚合函数(见图 [3-20](#Fig20) )。

![A448827_1_En_3_Fig20_HTML.jpg](A448827_1_En_3_Fig20_HTML.jpg)

图 3-20。

Groupby with different aggregation functions for different attributes

变体 3:这里，我们结合了变体 1 和 2，也就是说，我们在 price 字段上应用多个聚合，而在`quantity_purchased`上只应用一个聚合。再次传递一个字典，如代码片段所示。输出如图 [3-21](#Fig21) 所示。

![A448827_1_En_3_Fig21_HTML.jpg](A448827_1_En_3_Fig21_HTML.jpg)

图 3-21。

Groupby with showcasing a complex operation

除了基于`groupby()`的汇总，其他函数如`pivot(), pivot_table(), stack(), unstack(), crosstab()`和`melt()`提供了根据需求重塑`pandas`数据帧的能力。在 [`https://pandas.pydata.org/pandas-docs/stable/reshaping.html`](https://pandas.pydata.org/pandas-docs/stable/reshaping.html) 的`pandas`文档中有这些方法的完整描述和示例。我们鼓励你去经历同样的事情。

## 数据可视化

数据科学是一种以数据为主角的叙事方式。作为数据科学从业者，我们处理大量数据，这些数据日复一日地经历着各种用例的处理、争论和分析。用图表、图形、地图等视觉方面来补充这个故事，不仅有助于提高对数据(以及用例/业务问题)的理解，还提供了发现隐藏模式和潜在洞察力的机会。

因此，数据可视化是以图表、图形、图片等形式直观地表示信息的过程，以便更好地、普遍一致地理解。

我们提到普遍一致的理解是为了指出人类语言的一个非常普遍的问题。人类语言天生复杂，根据作者的意图和技巧，读者可能会以不同的方式理解书面信息(导致各种问题)。因此，可视化地呈现数据为我们提供了一种一致的语言来呈现和理解信息(尽管这也不能避免误解，但它提供了一定的一致性)。

在本节中，我们首先利用`pandas`及其功能，通过不同的可视化来直观地理解数据。然后我们将从`matplotlib`的角度介绍可视化。

Note

数据可视化本身是一个跨领域的流行和深入的研究领域。这一章和这一节只介绍了几个主题来帮助我们开始。这绝不是关于数据可视化的全面而详细的指南。感兴趣的读者可以进一步探索，尽管本文和后续章节中涉及的主题对于大多数与可视化相关的常见任务来说应该足够了。

### 与熊猫一起想象

数据可视化是一个多样化的领域，本身就是一门科学。尽管可视化类型的选择在很大程度上取决于数据、受众等，但我们将继续使用上一节中的产品交易数据集来理解和可视化。

简单回顾一下，手头的数据集由表明某些用户购买产品的交易组成。每笔交易都有以下属性。

*   日期:交易的日期
*   价格:购买产品的价格
*   产品 ID:产品识别号
*   采购数量:此事务处理中采购的产品数量
*   序列号:事务处理序列号
*   用户 ID:执行交易的用户的标识号
*   用户类型:用户的类型

正如上一节所讨论的，我们让数据集清理列名，将属性转换为正确的数据类型，并派生出额外的属性`user_class`和`purchase_week`。

是一个非常流行和强大的库，我们在本章中已经看到了它的例子。可视化是`pandas`的另一个重要且广泛使用的特性。它通过绘图界面展示其可视化功能，并严格遵循`matplotlib`风格的可视化语法。

#### 折线图

我们首先来看一个拥有最大交易次数的用户的购买模式(我们将此作为一个练习，让您来识别这样的用户)。使用折线图可以最好地显示趋势。默认情况下，`plot()`界面只需在所需字段上设置数据框的子集，即可绘制出折线图。以下代码片段显示了给定用户的价格趋势。

```py
df[df.user_id == max_user_id][['price']].plot(style='blue')
plt.title('Price Trends for Particular User')

```

`plt`别名用于`matplotlib.pyplot`。我们将在接下来的章节中对此进行更多的讨论，现在假设我们需要对由`pandas`生成的图进行附加增强。在这种情况下，我们使用它来为我们的情节添加一个标题。生成的图如图 [3-22](#Fig22) 所示。

![A448827_1_En_3_Fig22_HTML.jpg](A448827_1_En_3_Fig22_HTML.jpg)

图 3-22。

Line chart showing price trend for a user

虽然我们可以看到这个用户的不同交易价格的可视化表示，但这对我们没有太大帮助。现在，让我们再次使用折线图来了解他/她的购买趋势(记住，我们在数据集中有可用的交易日期)。我们通过将数据帧子集化为两个必需的属性来使用相同的绘图界面。下面的代码片段概述了这个过程。

```py
df[df.user_id == max_user_id].plot(x='date',y='price',style='blue')   
plt.title('Price Trends for Particular User Over Time')

```

这一次，由于我们有两个属性，我们通知`pandas`使用日期作为我们的 x 轴，价格作为 y 轴。绘图接口使用élan 处理`datetime`数据类型，如图 [3-23](#Fig23) 所示的输出所示。

![A448827_1_En_3_Fig23_HTML.jpg](A448827_1_En_3_Fig23_HTML.jpg)

图 3-23。

Price trend over time for a given user

这一次，我们的可视化清楚地帮助我们看到这个用户的购买模式。虽然我们可以详细讨论这个视觉化的洞见，但是一个快速的推论是清晰可见的。如该图所示，用户似乎在年初购买了高价值的物品，随着一年的进行具有减少的趋势。此外，与一年中的其他时间相比，年初的交易数量更多且更接近。我们可以将这些细节与更多数据关联起来，以识别模式和行为。我们将在接下来的章节中涉及更多这样的方面。

#### 条形图

了解了特定用户的趋势后，让我们看看我们的数据集的汇总情况。因为我们已经有了一个名为`purchase_week`的派生属性，让我们用它来合计用户在一段时间内购买的数量。我们首先使用`groupby()`函数聚集一周级别的数据，然后聚集属性`quantity_purchased`。最后一步是在条形图上绘制聚合图。下面的片段有助于我们绘制这些信息。

```py
df[['purchase_week',
        'quantity_purchased']].groupby('purchase_week').sum().plot.barh(
                                                                color='orange')
plt.title('Quantities Purchased per Week')

```

我们使用`barh()`函数来准备一个水平条形图。就其表示信息的方式而言，它类似于标准的`bar()`图。区别在于图的方向。图 [3-24](#Fig24) 显示了生成的输出。

![A448827_1_En_3_Fig24_HTML.jpg](A448827_1_En_3_Fig24_HTML.jpg)

图 3-24。

Bar plot representing quantities purchased at a weekly level

#### 直方图

探索性数据分析(EDA)的一个最重要的方面是理解给定数据集的各种数值属性的分布。最简单也是最常见的可视化分布方式是通过直方图。我们绘制了在数据集中购买的产品的价格分布，如下面的代码片段所示。

```py
df.price.hist(color='green')
plt.title('Price Distribution')

```

我们使用`hist()`函数在一行代码中绘制价格分布。输出如图 [3-25](#Fig25) 所示。

![A448827_1_En_3_Fig25_HTML.jpg](A448827_1_En_3_Fig25_HTML.jpg)

图 3-25。

Histogram representing price distribution

图 [3-25](#Fig25) 中显示的输出清楚地显示了一个偏斜和拖尾分布。在我们的算法中使用这些属性时，这些信息将是有用的。当我们在实际用例中工作时，会更加清楚。

我们可以更进一步，尝试将每周的价格分布可视化。我们通过使用`hist()`函数中的参数`by`来实现。这个参数帮助我们根据提到的属性对数据进行分组，如`by`，然后为每个分组生成一个子图。在我们的例子中，我们按照购买周进行分组，如下面的代码片段所示。

```py
df[['price','purchase_week']].hist(by='purchase_week' ,sharex=True)

```

图 [3-26](#Fig26) 中描述的输出显示了每周的价格分布，最高的仓位用不同的颜色清楚地标出。

![A448827_1_En_3_Fig26_HTML.jpg](A448827_1_En_3_Fig26_HTML.jpg)

图 3-26。

Histograms on a weekly basis

#### 饼图

在理解数据或提取见解时，最常见的问题之一是了解哪种类型的贡献最大。为了直观显示百分比分布，最好使用饼图。对于我们的数据集，下面的代码片段帮助我们直观地看到哪种用户类型购买了多少。

```py
class_series = df.groupby('user_class').size()
class_series.name = 'User Class Distribution'
class_series.plot.pie(autopct='%.2f')
plt.title('User Class Share')
plt.show()

```

前面的代码片段使用了`groupby()`来提取代表每个`user_class`级别上的事务数量的序列。然后我们使用`pie()`函数来绘制百分比分布。我们使用`autopct`参数来标注每个`use_class`的实际百分比贡献。图 [3-27](#Fig27) 描绘了输出饼图。

![A448827_1_En_3_Fig27_HTML.jpg](A448827_1_En_3_Fig27_HTML.jpg)

图 3-27。

Pie chart representing user class transaction distribution

图 [3-27](#Fig27) 中的曲线图清楚地指出，新用户占据了总交易份额的 50%以上，而`existing`和`loyal_existing`用户完成了剩余的份额。我们不建议使用饼图，尤其是当你有超过三个或四个类别的时候。请改用条形图。

#### 箱线图

箱线图是帮助我们理解数字数据的四分位数分布的重要可视化工具。箱线图或盒须图是一种简明的表示法，有助于理解数据中不同的四分位数、偏斜度、离差和异常值。

我们将使用箱线图来查看属性`quantity_purchased`和`purchase_week`。下面的代码片段为我们生成了所需的情节。

```py
df[['quantity_purchased','purchase_week']].plot.box()
plt.title('Quantity and Week value distribution')

```

现在，我们来看看生成的图(见图 [3-28](#Fig28) )。盒状图中盒的底边标记第一个四分位数，而上边标记第三个四分位数。方框中间的线表示第二个四分位数或中间值。从框中延伸出来的顶部和底部胡须标记了值的范围。异常值被标记在胡须边界之外。在我们的例子中，对于购买的数量，中位数非常接近盒子的中间，而购买周则接近底部(清楚地指出了数据中的偏斜)。鼓励您在 [`http://www.physics.csbsju.edu/stats/box2.html`](http://www.physics.csbsju.edu/stats/box2.html) 、 [`http://www.stat.yale.edu/Courses/1997-98/101/boxplot.htm`](http://www.stat.yale.edu/Courses/1997-98/101/boxplot.htm) 阅读更多关于盒状图的深入了解。

![A448827_1_En_3_Fig28_HTML.jpg](A448827_1_En_3_Fig28_HTML.jpg)

图 3-28。

Box plots using pandas

#### 散点图

散点图是另一类可视化，通常用于确定属性之间的相关性或模式。像我们到目前为止看到的大多数可视化一样，散点图也可以通过`pandas`的`plot()`接口获得。

为了理解散点图，我们首先需要执行几个数据争论的步骤，以使我们的数据成为所需的形状。我们首先使用`map()`用虚拟编码对`user_class`进行编码(如前一节所述),然后使用`groupby()`获得每周每个`user_class`级别的平均价格和交易数量。下面的代码片段帮助我们获得数据帧。

![A448827_1_En_3_Fig29_HTML.jpg](A448827_1_En_3_Fig29_HTML.jpg)

图 3-29。

Dataframe aggregated on a per week per user_class level

```py
uclass_map = {'new': 1, 'existing': 2, 'loyal_existing': 3,'error':0}
df['enc_uclass'] = df.user_class.map(uclass_map)

bubble_df = df[['enc_uclass',
                'purchase_week',
                'price','product_id']].groupby(['purchase_week',
                                                enc_uclass']).agg(
                                                         {'price':'mean',
                                                          'product_id':'count'}
                                                         ).reset_index()

bubble_df.rename(columns={'product_id':'total_transactions'},inplace=True)

```

图 [3-29](#Fig29) 展示了生成的数据帧。现在，让我们使用散点图来可视化这些数据。下面的代码片段为我们完成了这项工作。

```py
bubble_df.plot.scatter(x='purchase_week',
                       y='price')
plt.title('Purchase Week Vs Price ')
plt.show()

```

这产生了图 [3-30](#Fig30) 中的图，展示了几乎随机分布的几周数据和平均价格，在图的左上方有一些轻微的集中。

![A448827_1_En_3_Fig30_HTML.jpg](A448827_1_En_3_Fig30_HTML.jpg)

图 3-30。

Scatter plot showing spread of data across purchase_week and price

散点图还为我们提供了比基本维度更直观的能力。我们可以用颜色和大小来绘制第三维和第四维。下面的代码片段有助于我们理解价差，颜色表示`user_class`，而气泡的大小表示交易的数量。

```py
bubble_df.plot.scatter(x='purchase_week',
                       y='price',
                       c=bubble_df['enc_uclass'],
                       s=bubble_df['total_transactions']*10)
plt.title('Purchase Week Vs Price Per User Class Based on Tx')  

```

参数是不言自明的——`c`代表颜色，而`s`代表气泡的大小。这种图也称为气泡图。生成的输出如图 [3-31](#Fig31) 所示。

![A448827_1_En_3_Fig31_HTML.jpg](A448827_1_En_3_Fig31_HTML.jpg)

图 3-31。

Scatter plot visualizing multi dimensional data

在这一节中，我们利用`pandas`来绘制各种可视化。这些是一些最广泛使用的可视化工具，`pandas`提供了很大的灵活性来做更多的事情。此外，还有一个扩展的图表列表，可以使用`pandas`可视化。完整信息可在`pandas`文档中找到。

### 使用 Matplotlib 可视化

`matplotlib`是一个流行的绘图库。它提供了生成出版物质量可视化的接口和实用程序。从 2003 年的第一个版本到今天，`matplotlib`正被其活跃的开发者社区不断改进。它也形成了许多其他绘图库的基础和灵感。如前一节所述，`pandas`和`SciPy`(另一个流行的科学计算 Python 库)为`matplotlib`实现提供了包装器，以便于可视化数据。

`matplotlib`提供了两个主要的工作模块，`pylab`和`pyplot`。在本节中，我们将只关注`pyplot`模块(不鼓励使用`pylab`)。`pyplot`接口是一个面向对象的接口，它支持显式实例化，而不是`pylab`的隐式实例化。

在上一节中，我们简要介绍了不同的可视化，并看到了一些调整它们的方法。由于`pandas`可视化源自`matplotlib`本身，我们将介绍`matplotlib`的其他概念和功能。这将使你不仅能轻松使用`matplotlib`，还能提供一些技巧来提高使用`pandas`生成的可视化效果。

#### 人物和支线剧情

重要的事情先来。任何`matplotlib`风格可视化的基础都是从图形和子图对象开始的。图形模块帮助`matplotlib`生成绘图窗口对象及其相关元素。简而言之，它是所有可视化组件的顶级容器。在`matplotlib`语法中，图形是最顶层的容器，在一个图形中，我们可以灵活地可视化多个图形。因此，支线剧情是高级人物容器中的情节。

让我们从一个简单的例子和所需的导入开始。然后我们将在这个例子的基础上更好地理解人物和支线剧情的概念。下面的代码片段导入了`matplotlib`的`pyplot`模块，并使用`numpy`绘制了一条简单的正弦曲线来生成 x 和 y 值。

```py
import numpy as np
import matplotlib.pyplot as plt

# sample plot
x = np.linspace(-10, 10, 50)
y=np.sin(x)

plt.plot(x,y)
plt.title('Sine Curve using matplotlib')
plt.xlabel('x-axis')
plt.ylabel('y-axis')

```

`pyplot`模块公开了像`plot()`这样的方法来生成可视化。本例中，`matplotlib`与`plt.plot(x, y)`一起在幕后生成`figure`和`axes`对象，输出图 [3-32](#Fig32) 中的情节。为了完整起见，语句`plt.title(), plt.xlabel()`等分别提供了设置图形标题和轴标签的方法。

![A448827_1_En_3_Fig32_HTML.jpg](A448827_1_En_3_Fig32_HTML.jpg)

图 3-32。

Sample plot

现在我们已经完成了一个样本图，让我们看看不同的物体在`matplotlib`宇宙中是如何相互作用的。如上所述，`figure`对象是所有元素的最顶层容器。在使事情复杂化之前，我们首先绘制不同的图形，即每个图形只包含一个单独的图形。下面的代码片段使用`numpy`和`matplotlib`绘制了两个不同图形中的正弦波和余弦波。

```py
# first figure
plt.figure(1)
plt.plot(x,y)
plt.title('Fig1: Sine Curve')
plt.xlabel('x-axis')
plt.ylabel('y-axis')

# second figure
plt.figure(2)
y=np.cos(x)
plt.plot(x,y)
plt.title('Fig2: Cosine Curve')
plt.xlabel('x-axis')
plt.ylabel('y-axis')

```

语句`plt.figure()`创建了一个类型为`Figure`的实例。作为参数传入的数字是图标识符，在存在多个图的情况下，这在引用同一个图时很有帮助。其余的语句类似于我们的示例图，其中`pyplot`总是指向当前要绘制的图形对象。注意，在实例化新图形的时刻，`pyplot`指的是新创建的对象，除非另有说明。产生的输出如图 [3-33](#Fig33) 所示。

![A448827_1_En_3_Fig33_HTML.jpg](A448827_1_En_3_Fig33_HTML.jpg)

图 3-33。

Multiple figures using matplotlib

我们绘制多个图表，同时讲述一个用例的数据故事。然而，在某些情况下，我们需要在同一个图形中绘制多个图。这就是支线剧情的概念出现的地方。子绘图将图形分成指定行和列的网格，并提供与绘图元素交互的界面。支线剧情可以用几种不同的方式生成，它们的使用取决于个人偏好和用例需求。我们从最直观的方法开始，即`add_subplot()`方法。该方法通过图形对象本身公开。它的参数有助于定义网格布局和其他属性。下面的代码片段在一个图形中生成了四个支线剧情。

```py
y = np.sin(x)
figure_obj = plt.figure(figsize=(8, 6))
ax1 = figure_obj.add_subplot(2,2,1)
ax1.plot(x,y)

ax2 = figure_obj.add_subplot(2,2,2)
ax3 = figure_obj.add_subplot(2,2,3)

ax4 = figure_obj.add_subplot(2,2,4)
ax4.plot(x+10,y)

```

这个代码片段首先使用`plt.figure()` `.`定义一个人物对象，然后我们得到一个 axes 对象，它指向使用语句`figure_obj.add_subplot(2,2,1)`生成的第一个子情节。这句话实际上是把图分成两行两列。最后一个参数(值 1)指向这个网格中的第一个子图。该代码片段简单地绘制了左上角子图中的正弦曲线(标识为 2，2，1)和第四个子图中在 x 轴上移动了 10 个单位的另一条正弦曲线(标识为 2，2，4)。产生的输出如图 [3-34](#Fig34) 所示。

![A448827_1_En_3_Fig34_HTML.jpg](A448827_1_En_3_Fig34_HTML.jpg)

图 3-34。

Subplots using add_subplot method

第二种生成支线剧情的方法是直接通过`pyplot`模块。`pyplot`模块公开了一个方法`subplots()`，该方法返回 figure 对象和一列`axes`对象，每个对象都指向`subplots()`参数中提到的布局中的一个子情节。当我们知道需要多少支线剧情时，这个方法很有用。下面的片段展示了同样的情况。

```py
fig, ax_list = plt.subplots(2,1,sharex=True, figsize=(8, 6))
y= np.sin(x)
ax_list[0].plot(x,y)

y= np.cos(x)
ax_list[1].plot(x,y)

```

语句`plt.subplots(2,1,sharex=True)`一口气做了三件事。它首先生成一个`figure`对象，然后该对象被分成 2 行 1 列(即总共两个支线剧情)。这两个支线剧情以`axes`对象列表的形式返回。最后也是第三件事是共享 x 轴，我们使用参数`sharex`来实现。x 轴的这种共享允许该图中的所有子情节具有相同的 x 轴。这使得我们可以在同样的尺度上观察数据，同时也提高了美感。输出如图 [3-35](#Fig35) 所示，显示了同一 x 轴上的正弦和余弦曲线。

![A448827_1_En_3_Fig35_HTML.jpg](A448827_1_En_3_Fig35_HTML.jpg)

图 3-35。

Subplots using subplots() method

另一个变体是`subplot()`函数，也是通过`pyplot`模块直接公开的。这非常类似于 figure 对象的`add_subplot()`方法。你可以在本章的代码中找到例子。在转移到其他概念之前，我们快速接触一下`subplot2grid()`函数，它也是通过`pyplot`模块公开的。该函数提供了类似于已经讨论过的功能，以及定义网格布局的更精细的控制，其中子情节可以跨越任意数量的列和行。下面的代码片段展示了一个包含不同大小的支线剧情的网格。

```py
y = np.abs(x)
z = x**2

plt.subplot2grid((4,3), (0, 0), rowspan=4, colspan=2)
plt.plot(x, y,'b',x,z,'r')

ax2 = plt.subplot2grid((4,3), (0, 2),rowspan=2)
plt.plot(x, y,'b')
plt.setp(ax2.get_xticklabels(), visible=False)

plt.subplot2grid((4,3), (2, 2), rowspan=2)
plt.plot(x, z,'r')

```

`subplot2grid()`函数有多个参数，解释如下:

*   `shape`:将网格中的行和列表示为(rows，columns)的元组。
*   `loc`:表示子情节位置的元组。该参数为 0 索引。
*   `rowspan`:该参数表示子情节覆盖的行数。
*   `colspan`:该参数表示子情节延伸到的列数。

图 [3-36](#Fig36) 中由代码片段生成的输出有一个包含两个函数的四行两列的子情节。另外两个支线剧情各覆盖两行一列。

![A448827_1_En_3_Fig36_HTML.jpg](A448827_1_En_3_Fig36_HTML.jpg)

图 3-36。

Subplots using subplot2grid()

#### 绘图格式

格式化情节是讲故事的另一个重要方面，在这里`matplotlib`为我们提供了大量的功能。从改变颜色到标记等等，`matplotlib`提供了易于使用的直观界面。

我们从颜色属性开始，它是作为`plot()`接口的一部分提供的。`color`属性遵循 RGBA 规范，允许我们以字符串(红色、绿色等)、单个字母(`r`、g 等)甚至十六进制值的形式提供 alpha 和颜色值。更多详情可在 [`https://matplotlib.org/api/colors_api.html`](https://matplotlib.org/api/colors_api.html) 的`matplotlib`文档中找到。

图 [3-37](#Fig37) 中描述的以下示例和输出说明了设置图的`color`和`alpha`属性是多么容易。

![A448827_1_En_3_Fig37_HTML.jpg](A448827_1_En_3_Fig37_HTML.jpg)

图 3-37。

Setting color and alpha properties of a plot

```py
y = x

# color
ax1 = plt.subplot(321)
plt.plot(x,y,color='green')
ax1.set_title('Line Color')

# alpha
ax2 = plt.subplot(322,sharex=ax1)
alpha = plt.plot(x,y)
alpha[0].set_alpha(0.3)
ax2.set_title('Line Alpha')
plt.setp(ax2.get_yticklabels(), visible=False)

```

同样，我们还可以选择使用不同的形状来标记数据点，以及使用不同的样式来绘制线。当在同一个图上表示不同的属性/类时，这些选项很方便。图 [3-38](#Fig38) 中描述的以下片段和输出展示了相同的内容。

![A448827_1_En_3_Fig38_HTML.jpg](A448827_1_En_3_Fig38_HTML.jpg)

图 3-38。

Setting marker and line style properties of a plot

```py
# marker
# markers -> '+', 'o', '*', 's', ',', '.', etc
ax3 = plt.subplot(323,sharex=ax1)
plt.plot(x,y,marker='*')
ax3.set_title('Point Marker')

# linestyle
# linestyles -> '-','--','-.', ':', 'steps'
ax4 = plt.subplot(324,sharex=ax1)
plt.plot(x,y,linestyle='--')
ax4.set_title('Line Style')
plt.setp(ax4.get_yticklabels(), visible=False)

```

虽然有更多的微调选项可用，但我们鼓励您通读文档以获取详细信息。我们以最后两个与线宽相关的技巧和一个快速完成这一切的简写符号来结束格式化部分，如下面的代码片段所示。

```py
# line width
ax5 = plt.subplot(325,sharex=ax1)
line = plt.plot(x,y)
line[0].set_linewidth(3.0)
ax5.set_title('Line Width')

# combine linestyle
ax6 = plt.subplot(326,sharex=ax1)
plt.plot(x,y,'b^')
ax6.set_title('Styling Shorthand')
plt.setp(ax6.get_yticklabels(), visible=False)

```

这个代码片段使用由`plot()`函数返回的 line 对象来设置线条宽度。代码片段的第二部分展示了一次设置线条颜色和数据点标记的简写符号，`b^`。图 [3-39](#Fig39) 所示的输出有助于显示这种效果。

![A448827_1_En_3_Fig39_HTML.jpg](A448827_1_En_3_Fig39_HTML.jpg)

图 3-39。

Example to show line_width and shorthand notation

#### 传说

图形图例是帮助我们将颜色/形状或其他属性映射到图中显示的不同属性的关键。虽然在大多数情况下`matplotlib`在准备和显示图例方面做得很好，但有时我们需要更精细的控制。

可通过`pyplot`模块直接使用`legend()`功能控制图的图例。我们可以通过这个函数设置位置、大小和其他格式属性。以下示例显示图例被放置在最佳位置。

![A448827_1_En_3_Fig40_HTML.jpg](A448827_1_En_3_Fig40_HTML.jpg)

图 3-40。

Sample plot with legend

```py
plt.plot(x,y,'g',label='y=x^2')
plt.plot(x,z,'b:',label='y=x')
plt.legend(loc="best")
plt.title('Legend Sample')

```

`matplotlib`的主要目标之一是提供出版质量可视化。`Matplotlib`支持`LaTEX`风格的图例格式，清晰地显示数学符号和方程式。`$`符号用于标记`LaTEX`样式格式化的开始和结束。下面的代码片段和输出图也显示了同样的情况(参见图 [3-41](#Fig41) )。

![A448827_1_En_3_Fig41_HTML.jpg](A448827_1_En_3_Fig41_HTML.jpg)

图 3-41。

Sample plot with LaTEX formatted legend

```py
# legend with latex formatting
plt.plot(x,y,'g',label='$y = x^2$')
plt.plot(x,z,'b:',linewidth=3,label='$y = x^2$')
plt.legend(loc="best",fontsize='x-large')
plt.title('Legend with $LaTEX$ formatting')

```

#### 轴控件

`matplotlib`的下一个特性是控制绘图的 x 轴和 y 轴的能力。除了像使用方法`set_xlabel()`和`set_ylabel()`设置轴标签和颜色这样的基本特性之外，还有更好的控件可用。我们先来看看如何添加一个辅助 y 轴。当我们在同一个地块上绘制与不同要素(具有不同比例的值)相关的数据时，有许多情况。为了获得正确的理解，通常有助于将两个特征放在不同的 y 轴上(每个都缩放到各自的范围)。为了获得额外的 y 轴，我们使用通过`axes`对象公开的函数`twinx()`。下面的代码片段概述了这个场景。

```py
## axis controls
# secondary y-axis
fig, ax1 = plt.subplots()
ax1.plot(x,y,'g')
ax1.set_ylabel(r"primary y-axis", color="green")

ax2 = ax1.twinx()

ax2.plot(x,z,'b:',linewidth=3)
ax2.set_ylabel(r"secondary y-axis", color="blue")

plt.title('Secondary Y Axis')

```

起初，用一个名为`twinx()`的函数来生成辅助 y 轴可能听起来很奇怪。很聪明的是，`matplotlib`有这样一个功能来指出一个事实，即额外的 y 轴将共享同一个 x 轴，因此得名`twinx()`。在同一行上，使用功能`twiny()`获得额外的 x 轴。输出曲线如图 [3-42](#Fig42) 所示。

![A448827_1_En_3_Fig42_HTML.jpg](A448827_1_En_3_Fig42_HTML.jpg)

图 3-42。

Sample plot with secondary y-axis

默认情况下，`matplotlib`标识正在绘制的数值范围，并调整 x 轴和 y 轴的刻度和范围。它还提供了通过`axis()`功能手动设置这些参数的能力。通过这个函数，我们可以使用预定义的关键字如`tight`、`scaled`和`equal`来设置轴的范围，同时传递一个列表，将值标记为【xmin，xmax，ymin，ymax】。以下代码片段显示了如何手动调整轴范围。

```py
# manual
y = np.log(x)
z = np.log2(x)
w = np.log10(x)

plt.plot(x,y,'r',x,z,'g',x,w,'b')
plt.axis([0,2,-1,2])
plt.title('Manual Axis Range')

```

图 [3-43](#Fig43) 中的输出在左边展示了在没有任何轴调整的情况下生成的图，而右边的显示了在前面的代码片段中完成的轴调整。

![A448827_1_En_3_Fig43_HTML.jpg](A448827_1_En_3_Fig43_HTML.jpg)

图 3-43。

Plots showcasing default axis and manually adjusted axis

现在，我们已经看到了如何设置轴范围，我们将快速接触手动设置刻度或轴标记。对于轴刻度，我们有两个独立的函数可用，一个用于设置刻度范围，另一个用于设置刻度标签。这些功能分别被直观地命名为`set_ticks()`和`set_ticklabels()`。在下面的示例中，我们为 x 轴设置了要标记的刻度，而对于 y 轴，我们使用适当的函数设置了刻度范围和标签。

```py
# Manual ticks
plt.plot(x, y)
ax = plt.gca()

ax.xaxis.set_ticks(np.arange(-2, 2, 1))

ax.yaxis.set_ticks(np.arange(0, 5))
ax.yaxis.set_ticklabels(["min", 2, 4, "max"])

plt.grid(True)
plt.title("Manual ticks on the x-axis")

```

输出是一个图，其中 x 轴的标签仅标记在-2 和 1 之间，而 y 轴的范围是 0 到 5，标签是手动更改的。输出曲线如图 [3-44](#Fig44) 所示。

![A448827_1_En_3_Fig44_HTML.jpg](A448827_1_En_3_Fig44_HTML.jpg)

图 3-44。

Plot showcasing axes with manual ticks

在我们进入下一组特性/功能之前，值得注意的是，使用`matplotlib`，除了手动设置之外，我们还可以根据数据范围以标准方式缩放轴(如前所述)。以下是在对数标度上缩放 y 轴的快速片段。输出如图 [3-45](#Fig45) 所示。

![A448827_1_En_3_Fig45_HTML.jpg](A448827_1_En_3_Fig45_HTML.jpg)

图 3-45。

Plot showcasing log scaled y-axis

```py
# scaling
plt.plot(x, y)
ax = plt.gca()
# values: log, logit, symlog
ax.set_yscale("log")
plt.grid(True)
plt.title("Log Scaled Axis")

```

#### 释文

来自`pyplot`模块的`text()`接口公开了`matplotlib`的注释能力。我们可以使用这个界面来注释图形/情节/子情节的任何部分。它将 x 和 y 坐标、要显示的文本、对齐和 fontsize 参数作为输入，将注释放置在绘图上的所需位置。以下代码片段注释了抛物线图的最小值。

```py
# annotations
y = x**2
min_x = 0
min_y = min_x**2

plt.plot(x, y, "b-", min_x, min_y, "ro")
plt.axis([-10,10,-25,100])

plt.text(0, 60, "Parabola\n$y = x^2$", fontsize=15, ha="center")
plt.text(min_x, min_y+2, "Minima", ha="center")
plt.text(min_x, min_y-6, "(%0.1f, %0.1f)"%(min_x, min_y), ha='center',color='gray')
plt.title("Annotated Plot")

```

`text()`接口提供了更多的功能和格式化特性。我们鼓励您仔细阅读官方文档和示例，了解这方面的详细信息。展示注释抛物线的输出图如图 [3-46](#Fig46) 所示。

![A448827_1_En_3_Fig46_HTML.jpg](A448827_1_En_3_Fig46_HTML.jpg)

图 3-46。

Plot showcasing annotations

#### 全局参数

为了保持一致性，我们通常试图在整个视觉故事中保持情节大小、字体和颜色的一致。设置这些属性会增加维护代码库的复杂性和难度。为了克服这些问题，我们可以全局设置格式设置，如下面的代码片段所示。

```py
# global formatting params
params = {'legend.fontsize': 'large',
          'figure.figsize': (10, 10),
         'axes.labelsize': 'large',
         'axes.titlesize':'large',
         'xtick.labelsize':'large',
         'ytick.labelsize':'large'}

plt.rcParams.update(params)

```

使用`rcParams.update()`设置后，`params`字典中提供的属性将应用于生成的每个图形。鼓励您再次应用这些设置并生成本节中讨论的图，以了解差异。

### Python 可视化生态系统

毫无疑问，`matplotlib`库是一个非常强大和流行的可视化/绘图库。它提供了绘制任何类型数据所需的大部分工具和技巧，能够控制甚至是最精细的元素。

然而，即使是亲用户也还有很多需要改进的地方。作为一个低级的 API，它需要大量的样板代码，交互性有限，样式和其他格式默认值似乎过时了。

为了解决这些问题并提供高级接口和与当前 Python 生态系统合作的能力，Python universe 有相当多的可视化库可供选择。一些最流行和最强大的是`bokeh`、`seaborn`、`ggplot`和`plotly`。每个库都建立在对`matplotlib`的理解和特性集的基础上，同时提供自己的特性集和易于使用的包装器来填补空白。

我们鼓励您探索这些库并理解它们的不同之处。如果需要的话，我们将在接下来的章节中介绍其中的一些。尽管有所不同，但大多数库都基于类似于`matplotlib`的概念，因此如果你精通`matplotlib`，学习曲线会更短。

## 摘要

就理解、处理和争论数据而言，这一章涵盖了相当多的内容。我们讨论了主要的数据格式，如平面文件(CSV、JSON、XML、HTML 等。)，并使用标准库来提取/收集数据。我们谈到了标准数据类型及其在整个数据科学过程中的重要性。本章的主要部分讲述了转换、清理和处理数据以使其成为可用形式的数据争论任务。虽然使用`pandas`库解释了这些技术，但是这些概念是通用的，并且应用于大多数数据科学相关的用例中。您可以使用这些技术作为指针，使用不同的库和编程/脚本语言可以很容易地应用它们。我们使用描述其用途的样本数据集涵盖了主要地块。我们也谈到了`matplotlib`的基础和强大的技巧。我们强烈建议您阅读引用的链接以深入了解。本章涵盖了 CRISP DM 模型中数据收集、处理和可视化的初始步骤。在接下来的章节中，我们将在这些概念的基础上，应用它们来解决具体的现实问题。敬请期待！