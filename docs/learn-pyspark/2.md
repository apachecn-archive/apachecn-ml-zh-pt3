# 2.数据处理

本章介绍了在 PySpark 中预处理和处理数据的不同步骤。预处理技术当然可以因情况而异，可以使用许多不同的方法将数据整理成所需的形式。本章的目的是揭示一些在 Spark 中处理大数据的常用技术。在这一章中，我们将讨论预处理数据的不同步骤，例如处理缺失值、合并数据集、应用函数、聚合和排序。数据预处理的一个主要部分是将数字列转换为分类列，反之亦然，我们将在接下来的几章中讨论这一点，它基于机器学习。我们将在本章中使用的数据集受主要研究数据集的启发，包含原始数据集的一些属性，以及包含虚构数据点的附加列。

### 注意

以下所有步骤都写在 Jupyter 笔记本上，在一个 Docker 镜像上运行 Spark(第 [1](1.html) 章提到)。所有后续代码也可以在数据块中运行。

## 创建 SparkSession 对象

第一步是创建一个`SparkSession`对象，以便使用 Spark。我们还从`spark.sql`导入所有需要的函数和数据类型:

```
[In]: from pyspark.sql import SparkSession
[In]: spark=SparkSession.builder.appName('data_processing').getOrCreate()
[In]: import pyspark.sql.functions as F
[In]: from pyspark.sql.types import *

```

现在，我们不是直接读取文件来创建数据帧，而是通过传递键值来回顾创建数据帧的过程。我们在 Spark 中创建 dataframe 的方法是声明它的模式并传递列值。

### 创建数据框架

在下面的示例中，我们创建了一个新的 dataframe，它有五列特定的数据类型(字符串和整数)。正如您所看到的，当我们在新的 dataframe 上调用 show 时，它被创建为三行五列，包含我们传递的值。

```
[In]:schema=StructType().add("user_id","string").add("country","string").add("browser", "string").add("OS",'string').add("age", "integer")

[In]: df=spark.createDataFrame([("A203",'India',"Chrome","WIN",33),("A201",'China',"Safari","MacOS",35),("A205",'UK',"Mozilla","Linux",25)],schema=schema)

[In]: df.printSchema()
[Out]:

```

![../images/478390_1_En_2_Chapter/478390_1_En_2_Figa_HTML.jpg](../images/478390_1_En_2_Chapter/478390_1_En_2_Figa_HTML.jpg)

```
[In]: df.show()

[Out]: 

```

![../images/478390_1_En_2_Chapter/478390_1_En_2_Figb_HTML.jpg](../images/478390_1_En_2_Chapter/478390_1_En_2_Figb_HTML.jpg)

### 空值

总体数据中包含空值是很常见的。因此，在数据处理管道中添加一个步骤来处理空值变得至关重要。在 Spark 中，我们可以通过用某个特定值替换空值或者删除包含空值的行/列来处理空值。

首先，我们创建一个新的数据帧(`df_na`)，它的两列包含空值(模式与前面的数据帧相同)。通过处理空值的第一种方法，我们用值 0 填充当前数据帧中的所有空值，这提供了一种快速修复方法。我们使用`fillna`函数将数据帧中的所有空值替换为 0。通过第二种方法，我们分别用`'USA'`和`'Safari'`替换特定列(国家、浏览器)中的空值。

```
[In]: df_na=spark.createDataFrame([("A203",None,"Chrome","WIN",33),("A201",'China',None,"MacOS",35),("A205",'UK',"Mozilla","Linux",25)],schema=schema)

[In]: df_na.show()
[Out]:

```

![../images/478390_1_En_2_Chapter/478390_1_En_2_Figc_HTML.jpg](../images/478390_1_En_2_Chapter/478390_1_En_2_Figc_HTML.jpg)

```
[In]: df_na.fillna('0').show()
[Out]:

```

![../images/478390_1_En_2_Chapter/478390_1_En_2_Figd_HTML.jpg](../images/478390_1_En_2_Chapter/478390_1_En_2_Figd_HTML.jpg)

```
[In]: df_na.fillna( { 'country':'USA', 'browser':'Safari' } ).show()
[Out]:

```

![../images/478390_1_En_2_Chapter/478390_1_En_2_Fige_HTML.jpg](../images/478390_1_En_2_Chapter/478390_1_En_2_Fige_HTML.jpg)

为了删除具有空值的行，我们可以简单地使用 PySpark 中的`na.drop`功能。然而，如果需要为特定的列执行此操作，我们也可以传递一组列名，如下例所示:

```
[In]: df_na.na.drop().show()
[Out]:

```

![../images/478390_1_En_2_Chapter/478390_1_En_2_Figf_HTML.jpg](../images/478390_1_En_2_Chapter/478390_1_En_2_Figf_HTML.jpg)

```
[In]: df_na.na.drop(subset='country').show()
[Out]:

```

![../images/478390_1_En_2_Chapter/478390_1_En_2_Figg_HTML.jpg](../images/478390_1_En_2_Chapter/478390_1_En_2_Figg_HTML.jpg)

数据处理中另一个非常常见的步骤是用特定值替换一些数据点。我们可以为此使用`replace`函数，如下例所示。要删除 dataframe 的列，我们可以使用 PySpark 的`drop`功能。

```
[In]: df_na.replace("Chrome","Google Chrome").show()
[Out]:

```

![../images/478390_1_En_2_Chapter/478390_1_En_2_Figh_HTML.jpg](../images/478390_1_En_2_Chapter/478390_1_En_2_Figh_HTML.jpg)

```
[In]: df_na.drop('user_id').show()
[Out]:

```

![../images/478390_1_En_2_Chapter/478390_1_En_2_Figi_HTML.jpg](../images/478390_1_En_2_Chapter/478390_1_En_2_Figi_HTML.jpg)

既然我们已经看到了如何通过传递值来创建数据帧，以及如何处理丢失的值，我们可以通过读取文件(`.csv`、`parquet`等)来创建 Spark 数据帧。).数据集总共包含 7 列和 2，000 行。summary 函数允许我们查看数据集的统计度量，例如数据帧中出现的数值数据的最小值、最大值和平均值。

```
[In]: df=spark.read.csv("customer_data.csv",header=True,inferSchema=True)
[In]: df.count()
[Out]: 2000

[In]: len(df.columns)
[Out]: 7

[In]: df.printSchema()
[Out]:

```

![../images/478390_1_En_2_Chapter/478390_1_En_2_Figj_HTML.jpg](../images/478390_1_En_2_Chapter/478390_1_En_2_Figj_HTML.jpg)

```
[In]: df.show(3)
[Out]: 

```

![../images/478390_1_En_2_Chapter/478390_1_En_2_Figk_HTML.jpg](../images/478390_1_En_2_Chapter/478390_1_En_2_Figk_HTML.jpg)

```
[In]: df.summary().show()
[Out]:

```

![../images/478390_1_En_2_Chapter/478390_1_En_2_Figl_HTML.jpg](../images/478390_1_En_2_Chapter/478390_1_En_2_Figl_HTML.jpg)

大多数情况下，我们不会使用数据帧中的所有列，因为有些列可能是多余的，在提供有用信息方面没有什么价值。因此，对数据帧进行子集化对于将适当的数据放置在适当的位置进行分析变得至关重要。我将在下一节介绍这一点。

## 数据帧的子集

可以基于多个条件创建数据帧的子集，在这些条件下，我们可以选择一些行、列或数据，并使用特定的过滤器。在下面的示例中，您将看到我们如何根据特定条件创建原始数据帧的子集，以演示筛选记录的过程。

*   挑选

*   过滤器

*   在哪里

### 挑选

在本例中，我们采用了一个数据帧列`'Avg_Salary'`，并使用`select`创建了原始数据帧的一个子集。我们可以传递子集中必须存在的任意数量的列。然后，我们在数据帧上应用过滤器，根据某个阈值(`Avg_Salary > 1000000`)提取记录。过滤后，我们可以获取子集中的记录总数，也可以进行进一步处理。

```
[In]: df.select(['Customer_subtype','Avg_Salary']).show()
[Out]:

```

![../images/478390_1_En_2_Chapter/478390_1_En_2_Figm_HTML.jpg](../images/478390_1_En_2_Chapter/478390_1_En_2_Figm_HTML.jpg)

```
[In]: df.filter(df['Avg_Salary'] > 1000000).count()

[In]: 128
[In]: df.filter(df['Avg_Salary'] > 1000000).show()

```

![../images/478390_1_En_2_Chapter/478390_1_En_2_Fign_HTML.jpg](../images/478390_1_En_2_Chapter/478390_1_En_2_Fign_HTML.jpg)

### 过滤器

我们还可以在数据帧上应用多个过滤器，包括更多的条件，如下所示。这可以通过两种方式实现:首先，通过应用连续过滤器，然后通过使用带有`where`语句的`(&`、`or)`操作数。

![../images/478390_1_En_2_Chapter/478390_1_En_2_Figo_HTML.jpg](../images/478390_1_En_2_Chapter/478390_1_En_2_Figo_HTML.jpg)

```
[In]: df.filter(df['Avg_Salary'] > 500000).filter(df['Number_of_houses'] > 2).show()
[Out]:

```

### 在哪里

```
[In]: df.where((df['Avg_Salary'] > 500000) & (df['Number_of_houses'] > 2)).show()
[Out]: 

```

![../images/478390_1_En_2_Chapter/478390_1_En_2_Figp_HTML.jpg](../images/478390_1_En_2_Chapter/478390_1_En_2_Figp_HTML.jpg)

现在我们已经看到了如何从数据帧创建子集，我们可以继续学习 PySpark 中的聚合。

## 聚集

任何一种聚合都可以简单地分为三个阶段，顺序如下:

*   裂开

*   应用

*   结合

第一步是根据一列或一组列拆分数据，然后对这些小的单独组(count、max、avg 等)执行操作。).一旦每组的结果都出来了，最后一步就是把所有这些结果结合起来。

在下面的例子中，我们基于`'Customer subtype'`聚集数据，并简单地计算每个类别中的记录数。我们使用 PySpark 中的`groupBy`函数。它的输出没有任何特定的顺序，因为我们没有对结果进行任何排序。因此，我们还将看到如何将任何类型的排序应用于最终结果。因为我们在 dataframe 中有七列——除了一个`(Avg_Salary)`,所有都是分类列，所以我们可以迭代每一列并应用聚合，如下例所示:

```
[In]: df.groupBy('Customer_subtype').count().show()
[Out]:

```

![../images/478390_1_En_2_Chapter/478390_1_En_2_Figq_HTML.jpg](../images/478390_1_En_2_Chapter/478390_1_En_2_Figq_HTML.jpg)

```
[In]:
for col in df.columns: 

    if col !='Avg_Salary':
        print(f" Aggregation for  {col}")
           df.groupBy(col).count().orderBy('count',ascending=False).show(truncate=False)
[Out]:

```

![../images/478390_1_En_2_Chapter/478390_1_En_2_Figr_HTML.jpg](../images/478390_1_En_2_Chapter/478390_1_En_2_Figr_HTML.jpg)

![../images/478390_1_En_2_Chapter/478390_1_En_2_Figs_HTML.jpg](../images/478390_1_En_2_Chapter/478390_1_En_2_Figs_HTML.jpg)

如上所述，我们可以对记录组进行不同种类的操作，例如

*   平均

*   最大

*   福建话

*   总额

以下示例根据不同的分组涵盖了其中的一些。`F`这里指的是火花`sql`功能。

```
[In]: df.groupBy('Customer_main_type').agg(F.mean('Avg_Salary')).show()
[Out]:

```

![../images/478390_1_En_2_Chapter/478390_1_En_2_Figt_HTML.jpg](../images/478390_1_En_2_Chapter/478390_1_En_2_Figt_HTML.jpg)

```
[In]: df.groupBy('Customer_main_type').agg(F.max('Avg_Salary')).show()
[Out]: 

```

![../images/478390_1_En_2_Chapter/478390_1_En_2_Figu_HTML.jpg](../images/478390_1_En_2_Chapter/478390_1_En_2_Figu_HTML.jpg)

```
[In]: df.groupBy('Customer_main_type').agg(F.min('Avg_Salary')).show()
[Out]:

```

![../images/478390_1_En_2_Chapter/478390_1_En_2_Figv_HTML.jpg](../images/478390_1_En_2_Chapter/478390_1_En_2_Figv_HTML.jpg)

```
[In]: df.groupBy('Customer_main_type').agg(F.sum('Avg_Salary')).show()
[Out]: 

```

![../images/478390_1_En_2_Chapter/478390_1_En_2_Figw_HTML.jpg](../images/478390_1_En_2_Chapter/478390_1_En_2_Figw_HTML.jpg)

有时，只需要用聚合或不用任何聚合对数据进行排序。在这里，我们可以利用 PySpark 的`'sort` `'`和`'orderBy'`功能，按照特定的顺序重新排列数据，如下例所示:

```
[In]: df.sort("Avg_Salary", ascending=False).show()
[Out]:

```

![../images/478390_1_En_2_Chapter/478390_1_En_2_Figx_HTML.jpg](../images/478390_1_En_2_Chapter/478390_1_En_2_Figx_HTML.jpg)

```
[In]: df.groupBy('Customer_subtype').agg(F.avg('Avg_Salary').alias('mean_salary')).orderBy('mean_salary',ascending=False).show(50,False)
[Out]: 

```

![../images/478390_1_En_2_Chapter/478390_1_En_2_Figy_HTML.jpg](../images/478390_1_En_2_Chapter/478390_1_En_2_Figy_HTML.jpg)

```
[In]: df.groupBy('Customer_subtype').agg(F.max('Avg_Salary').alias('max_salary')).orderBy('max_salary',ascending=False).show()
[Out]: 

```

![../images/478390_1_En_2_Chapter/478390_1_En_2_Figz_HTML.jpg](../images/478390_1_En_2_Chapter/478390_1_En_2_Figz_HTML.jpg)

在某些情况下，我们还必须收集特定组或单个类别的值列表。例如，假设一位客户去了一家在线商店，并访问了该商店网站上的不同页面。如果我们必须在一个列表中收集客户的所有活动，我们可以使用 PySpark 中的`collect`功能。我们可以通过两种不同的方式收集值:

*   收集列表

*   收集集合

### 收集

收集列表按原始出现顺序提供所有值(也可以颠倒顺序)，而收集组只提供唯一值，如下例所示。我们考虑对`Customer subtype`进行分组，并在新的列中收集`Numberof houses`的值，分别使用`list`和`set`。

```
[In]: df.groupby("Customer_subtype").agg(F.collect_set("Number_of_houses")).show()
[Out]:

```

![../images/478390_1_En_2_Chapter/478390_1_En_2_Figaa_HTML.jpg](../images/478390_1_En_2_Chapter/478390_1_En_2_Figaa_HTML.jpg)

```
[In]:
df.groupby("Customer_subtype").agg(F.collect_list("Number_of_houses")).show()
[Out]: 

```

![../images/478390_1_En_2_Chapter/478390_1_En_2_Figab_HTML.jpg](../images/478390_1_En_2_Chapter/478390_1_En_2_Figab_HTML.jpg)

创建具有常数值的新列的需求可能非常普遍。因此，我们可以在 PySpark 中使用`'lit'`函数来实现。在下面的示例中，我们创建了一个具有常数值的新列:

```
[In]: df=df.withColumn('constant',F.lit('finance'))
[In]: df.select('Customer_subtype','constant').show()
[Out]:

```

![../images/478390_1_En_2_Chapter/478390_1_En_2_Figac_HTML.jpg](../images/478390_1_En_2_Chapter/478390_1_En_2_Figac_HTML.jpg)

因为我们正在处理数据帧，所以在特定的列上应用特定的自定义函数并获得输出是一个常见的需求。因此，我们利用 UDF，以便在一个或多个列上应用 Python 函数。

## 用户定义函数(UDF)

在本例中，我们试图命名年龄类别，并为其创建一个标准的 Python 函数(`age_category`)。为了在 Spark 数据帧上应用它，我们使用这个 Python 函数创建了一个 UDF 对象。唯一的要求是提到函数的返回类型。在这种情况下，它只是一个字符串值。

```
[In]: from pyspark.sql.functions import udf
[In]: df.groupby("Avg_age").count().show()
[Out]:

```

![../images/478390_1_En_2_Chapter/478390_1_En_2_Figad_HTML.jpg](../images/478390_1_En_2_Chapter/478390_1_En_2_Figad_HTML.jpg)

```
[In]: def age_category(age):
    if age  == "20-30 years":
        return "Young"
    elif age== "30-40 years":
        return "Mid Aged"
    elif ((age== "40-50 years") or (age== "50-60 years")) :
        return "Old"
    else:
        return "Very Old"

[In]: age_udf=udf(age_category,StringType())
[In]: df=df.withColumn('age_category',age_udf(df['Avg_age']))
[In]: df.select('Avg_age','age_category').show()
[Out]: 

```

![../images/478390_1_En_2_Chapter/478390_1_En_2_Figae_HTML.jpg](../images/478390_1_En_2_Chapter/478390_1_En_2_Figae_HTML.jpg)

```
[In]: df.groupby("age_category").count().show()
[Out]:

```

![../images/478390_1_En_2_Chapter/478390_1_En_2_Figaf_HTML.jpg](../images/478390_1_En_2_Chapter/478390_1_En_2_Figaf_HTML.jpg)

熊猫 UDF 是另一个最近的进步，所以现在让我们回顾一下。

### 熊猫 UDF

与标准 Python UDFs 相比，Pandas UDFs 在处理和执行时间方面更快更有效。普通的 Python UDF 和熊猫 UDF 之间的主要区别在于 Python UDF 是逐行执行的，因此，它并没有提供分布式框架的优势。这可能需要更长的时间，相比之下，熊猫 UDF，一个块一个块地执行，给出更快的结果。有三种不同类型的熊猫 UDF:标量、分组地图和分组聚集。与传统的 UDF 相比，使用熊猫 UDF 的唯一区别在于声明。在下面的例子中，我们试图通过应用缩放来缩放`Avg_Salary`值。我们先取`Avg_Salary`的`min`和`max`值，从每个值中减去每个值的最低工资，然后除以`max`和`min`的差。

![$$ \frac{X-{X}_{\mathrm{min}}}{X_{max}-{X}_{min}} $$](../images/478390_1_En_2_Chapter/478390_1_En_2_Chapter_TeX_Equa.png)

```
[In]: df.select('Avg_Salary').summary().show()
[Out]:

```

![../images/478390_1_En_2_Chapter/478390_1_En_2_Figag_HTML.jpg](../images/478390_1_En_2_Chapter/478390_1_En_2_Figag_HTML.jpg)

```
[In]: min_sal=1361
[In]: max_sal=48919896

[In]: from pyspark.sql.functions import pandas_udf, PandasUDFType

[In]: def scaled_salary(salary):
        scaled_sal=(salary-min_sal)/(max_sal-min_sal)
        return scaled_sal

[In]: scaling_udf = pandas_udf(scaled_salary, DoubleType())
[In]:df.withColumn("scaled_salary",scaling_udf(df['Avg_Salary'])).show(10,False)
[Out]: 

```

![../images/478390_1_En_2_Chapter/478390_1_En_2_Figah_HTML.jpg](../images/478390_1_En_2_Chapter/478390_1_En_2_Figah_HTML.jpg)

这就是我们如何根据需要使用传统和 Pandas UDFs 在数据帧上应用不同的条件。

## 连接

合并不同的数据集是大数据世界中大多数数据处理管道中非常普遍的需求。PySpark 提供了一种非常方便的方法来根据需要合并和透视数据帧值。在下面的例子中，我们为所有的`Customer`类型创建了一个带有一些虚拟`Region Code`值的虚构数据帧。想法是将该数据帧与原始数据帧组合，以便使这些区域代码作为原始数据帧的一部分，作为一列。

```
[In]: region_data = spark.createDataFrame([('Family with grown ups','PN'),
                     ('Driven Growers','GJ'),
                       ('Conservative families','DD'),
                     ('Cruising Seniors','DL'),
                     ('Average Family ','MN'),
                     ('Living well','KA'),
                      ('Successful hedonists','JH'),
                       ('Retired and Religious','AX'),
                      ('Career Loners','HY'),('Farmers','JH')],schema=StructType().add("Customer_main_type","string").add("Region Code","string"))
[In]: region_data.show()
[Out]: 

```

![../images/478390_1_En_2_Chapter/478390_1_En_2_Figai_HTML.jpg](../images/478390_1_En_2_Chapter/478390_1_En_2_Figai_HTML.jpg)

```
[In]: new_df=df.join(region_data,on='Customer_main_type')

[In]: new_df.groupby("Region Code").count().show()
[Out]:

```

![../images/478390_1_En_2_Chapter/478390_1_En_2_Figaj_HTML.jpg](../images/478390_1_En_2_Chapter/478390_1_En_2_Figaj_HTML.jpg)

在将原始数据帧(`df`)与在`Customer_main_type`列上新创建的`region_data`数据帧结合之后，我们进行了区域计数。

## 绕轴旋转

我们可以使用 PySpark 中的`pivot`函数为特定的列创建一个数据帧的透视视图，如下例所示。这里，我们根据客户类型对数据进行分组。柱子代表不同的年龄组。数据透视表中的值是特定年龄组的每个客户类型类别的`Avg Salary`的总和。我们还通过用 0 填充所有空值来确保没有空值或空值。在接下来的例子中，我们创建了另一个数据透视表，使用了`label`列，并将`Avg Salary`的总和作为其中的值。

```
[In]:df.groupBy('Customer_main_type').pivot('Avg_age').sum('Avg_Salary').fillna(0).show()
[Out]:

```

![../images/478390_1_En_2_Chapter/478390_1_En_2_Figak_HTML.jpg](../images/478390_1_En_2_Chapter/478390_1_En_2_Figak_HTML.jpg)

```
[In]:df.groupBy('Customer_main_type').pivot('label').sum('Avg_Salary').fillna(0).show()
[Out]:

```

![../images/478390_1_En_2_Chapter/478390_1_En_2_Figal_HTML.jpg](../images/478390_1_En_2_Chapter/478390_1_En_2_Figal_HTML.jpg)

我们根据`Customer_main_type`列分割数据，并使用`pivot`函数获得每个`label`值(`0,1`)的`Avg_Salary`的累积和。

## 窗口函数或窗口集合

PySpark 中的这一功能允许您对记录组执行某些操作，即所谓的“在窗口内”它计算窗口中每一行的结果。使用`window`的一个经典例子是用户在不同会话期间的各种聚合。一个访问者可能在一个特定的网站上有多个会话，因此`window`可以用来统计用户在每个会话期间的总活动。PySpark 支持三种类型的`window`功能:

*   聚集

*   等级

*   分析学

在下面的例子中，我们导入了`window`函数，以及其他函数，比如`row_number`。下一步是定义窗口。有时它可以只是一个有序的列，或者有时它可以基于列中的特定类别。我们将会看到这些例子。在第一个例子中，我们定义了一个窗口，这个窗口基于排序后的`Avg Salary`列，我们对这些薪水进行排名。我们创建一个新列`'rank'`，并为每个`Avg Salary`值分配等级。

```
[In]: from pyspark.sql.window import Window
[In]: from pyspark.sql.functions import col,row_number

[In]: win = Window.orderBy(df['Avg_Salary'].desc())
[In]: df=df.withColumn('rank', row_number().over(win).alias('rank'))
[In]: df.show()
[Out]:

```

![../images/478390_1_En_2_Chapter/478390_1_En_2_Figam_HTML.jpg](../images/478390_1_En_2_Chapter/478390_1_En_2_Figam_HTML.jpg)

一个常见的需求是从一个类别中找出前三个值。在这种情况下，可以使用`window`来得到结果。在下面的例子中，我们通过`Customer subtype`列定义了窗口和分区。基本上，它所做的是对每个`Customer subtype`类别的`Avg Salary`进行排序，所以现在我们可以使用`filter`来获取每个组的前三个工资值。

```
[In]:win_1=Window.partitionBy("Customer_subtype").orderBy(df['Avg_Salary'].desc())
[In]: df=df.withColumn('rank', row_number().over(win_1).alias('rank'))

```

现在我们有了一个新的列 rank，它由每个类别的 rank 组成`Customer` _ `subtype`，我们可以很容易地过滤每个类别的前三个 rank。

```
[In]:
df.groupBy('rank').count().orderBy('rank').show()
[Out]:

```

![../images/478390_1_En_2_Chapter/478390_1_En_2_Figan_HTML.jpg](../images/478390_1_En_2_Chapter/478390_1_En_2_Figan_HTML.jpg)

```
[In]: df.filter(col('rank') < 4).show()
[Out]: 

```

![../images/478390_1_En_2_Chapter/478390_1_En_2_Figao_HTML.jpg](../images/478390_1_En_2_Chapter/478390_1_En_2_Figao_HTML.jpg)

## 结论

在本章中，我讨论了在 PySpark 中读取、清理和预处理数据的不同技术。您看到了连接数据帧并从中创建数据透视表的方法。本章的最后几节介绍了 PySpark 中的 UDF 和基于窗口的操作。接下来的章节将集中在 PySpark 中处理流数据和使用 MLlib 的机器学习。