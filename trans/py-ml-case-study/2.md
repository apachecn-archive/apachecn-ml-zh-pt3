# 2.回归

回归和时间序列分析使得定量目标变量的预测成为可能。本章旨在强调回归及其变体的核心概念。这里的重点是在解决现实世界的问题时，带读者经历模型选择的旅程。此外，本章还以统计测试为特色，以评估这些回归技术的发现。

Note

本书将 Python 2.7.11 作为编码示例的事实标准。此外，您需要为练习安装它。

在本章中，我们将使用混凝土抗压强度数据集来编写示例和练习。该数据转储可从 UCI 的网站链接下载:

[T2`http://archive.ics.uci.edu/ml/datasets/Concrete+Compressive+Strength`](http://archive.ics.uci.edu/ml/datasets/Concrete+Compressive+Strength)

## 案例研究:消除混凝土抗压强度的不一致性

安德鲁和史密斯就他经常收到的现有客户的投诉进行了一个小时的会谈。Smith 是一家建筑骨料公司的销售代表，在实现销售目标时遇到了困难。Smith 认为，他遇到困难的原因是现有客户抱怨质量不一致，再加上市场上的负面口碑，他很难与潜在客户达成交易。

工厂主管安德鲁是史密斯的朋友。起初，史密斯试图找到解决这个问题的方法，但当事情失去控制时，他决定和安德鲁开个会。

大多数公司的生产部门和销售部门之间通常会有摩擦。他们两人通常会为年度预算和实现年度目标的计划而争吵。史密斯知道这一点，但他确信安德鲁的情况会有所不同，因为他们已经成为朋友有一段时间了。然而，会议开始时，史密斯向安德鲁提出了这个问题，令他沮丧的是，安德鲁不准备承认生产中的任何不一致。正如史密斯回忆安德鲁所说，“这些抱怨对我来说没有任何意义。你问为什么！首先，上个季度我们的生产工厂进行了升级，事实上，我们使用的原材料在用于生产之前都经过了彻底的测试。”

这次会议对史密斯没有任何帮助，他把这件事搁置了一段时间。一天，当他在公司的一次企业活动中参加网络会议时，史密斯听到生产部门的一些高管在谈论工厂最近的升级。他回忆起听到一位高管说，“虽然我们最近在测量混凝土抗压强度的升级中没有任何测试改进，但由于我们对所用原材料的严格测试，我们的产品质量也是一流的。”

当 Smith 听到这个消息时，他做了一些研究，并惊讶地发现自从升级发生后，抱怨声越来越大。然而，他还发现，由于分配给采购部门的资金短缺，测试设备的采购被搁置。史密斯知道一个事实，那就是他不能等到第二年这个问题自己解决，他必须找到一种方法来纠正错误。

他听说最近的升级意味着机器可以记录数据。Smith 决定与 Claire(分析部经理)会面，看她能否帮助找出混凝土抗压强度不一致的原因。会议进行得很顺利，Claire 向 Smith 保证了公司内部的分析能力。正如 Claire 回忆的那样，“那时我们已经开始了垂直分析，但我确信我们有能力制定一个模型来回答这个异常现象。”

Smith 松了一口气，现在他必须制定一个策略来推进他的发现——也就是说，要么带着他的发现去找管理层，要么把他的发现分享给生产部门，让他们把这些发现整合到他们的内部流程中。然而，史密斯很好奇手头的数据是否足以推断出一些高质量的发现。此外，他有兴趣了解哪些因素对混凝土抗压强度的影响最大。

Smith 很想知道 Claire 将处理数据集的哪些要素。因此，Claire 想出了表 [2-1](#Tab1) 中的数据字典。

表 2-1。

Data Dictionary for the Concrete Compressive Strength Dataset

<colgroup><col> <col></colgroup> 
| 功能名称 | 描述 |
| --- | --- |
| 水泥(每立方米混合物中的千克数) | m3 混合料中使用的水泥量(单位:kg) |
| 高炉矿渣(每立方米混合物中的千克数) | m3 混合料中使用的高炉矿渣量(单位:kg) |
| 飞灰(每立方米混合物中的千克数) | m3 混合料中的粉煤灰用量(单位:kg) |
| 水(每立方米混合物中的千克数) | m3 混合料的用水量(单位:kg) |
| 超塑化剂(每立方米混合物中的千克) | m3 混合料中超塑化剂的用量(单位:kg) |
| 粗骨料(每立方米混合物中的千克数) | m3 混合料中粗骨料的用量(单位:kg) |
| 细骨料(每立方米混合物中的千克数) | m3 混合料中细骨料的用量(单位:kg) |
| 年龄(天数) | 混凝土的龄期(单位:天) |
| 混凝土抗压强度 | 混凝土抗压强度，单位为兆帕(MPa)。这是压力或应力的单位，也是确定混凝土抗压强度的常用单位。 |

在继续之前，Claire 考虑初始化下面的包。她更喜欢这样做，以便在本地机器上实现代码片段时避免瓶颈(清单 [2-1](#Par14) )。

```
%matplotlib inline

import time
import random
import datetime
import pandas as pd
import matplotlib.pyplot as plt
import statistics
import numpy as np
from scipy import stats
from sklearn.grid_search import GridSearchCV
from sklearn.linear_model import RANSACRegressor, LinearRegression, TheilSenRegressor
from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, median_absolute_error, r2_score

from sklearn.svm import SVR
from sklearn.linear_model import Ridge,Lasso,ElasticNet,BayesianRidge
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.cross_validation import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures
from sklearn.cross_validation import cross_val_score
import seaborn
from IPython.display import Image

Listing 2-1.Importing Packages Required for This Chapter

```

史密斯很好奇克莱尔会用什么技术来解决这个问题。Claire 认为这个问题是回归的理想应用，因为响应变量(混凝土抗压强度)是一个定量的量，她关心的是找到影响混凝土抗压强度的因素。这些影响因素，当按其成分换算并加在一起时，可以产生混凝土的抗压强度。因此，这将使混凝土抗压强度的计算，而不需要购买任何设备专门用于手头的问题。为了帮助 Smith 理解什么是回归，她收集了关于这个主题的信息。

## 回归的概念

回归描述了探索变量(即自变量)和响应变量(即因变量)之间的关系。探索性变量也称为预测值，其频率可能超过 1。回归正被用于预测和预报领域。当一个探索变量发生变化而其他自变量保持不变时，回归确定了响应变量的变化。这样做是为了理解每一个探索性变量之间的关系。请注意，无论用于预测离散响应变量的分类如何，回归都用于预测本质上连续的响应变量。回归的一个基本变体是线性回归。

### 插值和外推

外推是指使用回归来预测最初用于训练模型的探索值范围之外的响应变量值。该数据不用于训练模型，由图 [2-1](#Fig1) 中的虚线表示。

![A432778_1_En_2_Fig1_HTML.gif](A432778_1_En_2_Fig1_HTML.gif)

图 2-1。

Visual representation of interpolation and extrapolation

回归模型将根据手头的数据进行拟合/训练(即，由实线表示的插值)，然后训练的模型将用于在由虚线表示的部分上进行外推。因此，对已知数据进行插值，对未知数据进行外推。

### 线性回归

线性回归是一种回归形式，其中一个探索性变量用于预测响应变量的结果。数学上，它可以定义如下:

*   符号:y
*   公式:y = b <sub>0</sub> + b <sub>1</sub> x + e
*   插图:
    *   y:响应变量
    *   x:解释变量
    *   b <sub>0</sub> :斜率
    *   b <sub>1</sub> :截距
    *   e:回归残差

图 [2-2](#Fig2) 直观地展示了这个等式。

![A432778_1_En_2_Fig2_HTML.gif](A432778_1_En_2_Fig2_HTML.gif)

图 2-2。

Linear regression best fit line along with the regression coefficients explained

请注意，在图 [2-2](#Fig2) 中，截距是最佳拟合线穿过 y 轴的点。回归残差是响应变量的实际值和预测值之间的差值。

### x 上 y 的最小二乘回归线

最小二乘回归线使数据点到该线的垂直距离的平方和尽可能小。数学上，它可以定义如下:

*   符号:![$$ \widehat{y} $$](A432778_1_En_2_Chapter_IEq1.gif)
*   公式:![$$ \widehat{y}={b}_0+{b}_1x $$](A432778_1_En_2_Chapter_IEq2.gif)
*   进一步推导:
    *   ![$$ {b}_1=r\frac{s_y}{s_x} $$](A432778_1_En_2_Chapter_Equa.gif)

    *   ![$$ {b}_0=\overset{\acute{\mkern6mu}}{y}-{b}_1\overset{\acute{\mkern6mu}}{x} $$](A432778_1_En_2_Chapter_Equb.gif)

*   最小二乘回归线总是经过(![$$ \overset{\acute{\mkern6mu}}{x},\overset{\acute{\mkern6mu}}{y} $$](A432778_1_En_2_Chapter_IEq3.gif))

图 [2-3](#Fig3) 中的图示使用了线性回归(即一个探索性变量来描述最小二乘回归线所采用的方法)。y 表示响应变量，而 x 表示探索变量。显示了从数据点到直线的垂直距离，最佳拟合回归线被视为穿过这些数据点的中心。因此，这确保了每个数据点的残差平方最小化。

![A432778_1_En_2_Fig3_HTML.gif](A432778_1_En_2_Fig3_HTML.gif)

图 2-3。

Residuals

### 多次回归

多元回归是一种使用多个探索性变量来预测响应变量结果的回归。多元回归的数学定义如下:

*   符号:y
*   公式:y = b<sub>0</sub>+b<sub>1</sub>x<sub>1</sub>+b<sub>2</sub>x<sub>2</sub>+b<sub>3</sub>x<sub>3</sub>+…..b <sub>t</sub> x <sub>t</sub> + e
*   插图:
    *   y:响应变量
    *   x <sub>1</sub> ，x <sub>2</sub> ，x <sub>3，</sub> ……，x <sub>t</sub> :解释变量
    *   b <sub>0</sub> :斜率
    *   b <sub>1</sub> ，b <sub>2</sub> ，b <sub>3</sub> ，……。，b <sub>t</sub> :截距
    *   e:回归残差

上面的公式也可以称为回归方程。本质上，人们试图找到所有系数的最佳值，这样可以在给定所有 x 的情况下给出 y 的最佳可能值。在单个变量的情况下，它是一条线(即线性回归)，如图 [2-3](#Fig3) 所示，但在多个变量的情况下，它变成一个平面，如图 [2-4](#Fig4) 所示。

![A432778_1_En_2_Fig4_HTML.gif](A432778_1_En_2_Fig4_HTML.gif)

图 2-4。

Multiple regression with two exploratory and one response variables explained

将系数的最佳值(即斜率和截距)插回到上面的公式中，将得到一个等式。该等式中的未知值将是 x 值，在这种情况下是探索性变量值。对于任何给定的 x 值，该方程将返回一个预测值。

### 逐步回归

这种回归是多元回归的定制版本。逐步回归不是使用所有探索性变量来预测结果，而是从给定的库中自动选择能够产生最佳拟合线的独立变量。这可以通过执行以下任一操作来实现:

*   一次尝试一个探索性变量，并用它来拟合回归模型。如果发现有统计学意义，则加入探索性变量。
*   对所有探索性变量运行多元回归，并使用最具静态意义的变量来确定最佳拟合回归线。

图 [2-5](#Fig5) 最好地描述了逐步回归所采用的方法。

![A432778_1_En_2_Fig5_HTML.gif](A432778_1_En_2_Fig5_HTML.gif)

图 2-5。

Flowchart of stepwise regression Source:.slideshare.net/bzinchenko/quant-trader-algorithms

然而，根据一些统计学家，这种方法继承了以下问题:

*   选择一个探索性变量而放弃其他变量会产生对所选变量的偏见，并使人无法观察到大量的变化，而这些变化原本可以被首先放弃的探索性变量所捕捉到。
*   在从许多探索性变量中进行选择的情况下，完成整个练习将需要强大的计算能力，因此也需要时间。

### 多项式回归

有时，当绘制探索变量和响应变量之间的散点图时，我们会看到非线性趋势。这种趋势不会被线性回归所注意到，因此需要非线性处理，多项式回归就派上了用场。多项式回归使用多项式的次数来制作非线性回归线。多项式回归在数学上可以定义如下:

*   符号:y
*   公式:y = b<sub>0</sub>+b<sub>1</sub>x+b<sub>2</sub>x<sup>2</sup>+b<sub>3</sub>x<sup>3</sup>+b<sub>h</sub>x<sup>h</sup>+e
*   插图:
    *   y:响应变量
    *   x <sub>1</sub> ，x <sub>2</sub> ，x <sub>3，</sub> ……，x <sub>h</sub> :解释变量
    *   b <sub>0</sub> :斜率
    *   b <sub>1</sub> ，b <sub>2</sub> ，b <sub>3</sub> ，……。，b <sub>h</sub> :截距
    *   e:回归残差
    *   h:多项式的次数

查看图 [2-6](#Fig6) 您会注意到，增加多项式的次数会使曲线更加非线性，并在回归线内引入额外的曲线。

![A432778_1_En_2_Fig6_HTML.gif](A432778_1_En_2_Fig6_HTML.gif)

图 2-6。

Polynomial regression curves for different values of degrees (i.e., h)

## 回归假设

为了防止 Smith 认为回归是连续数据预测的魔棒，Claire 提出了一组假设，这些假设应该适用于适用于给定数据集的回归。

### 案例数量

病例与独立变量(iv)的比率理想情况下应该是 20:1。这意味着每个探索性变量应该有 20 个样本。然而，在极端情况下，一个探索性变量至少允许有 5 个样本(即，对于 5 个探索性变量，数据集中应该有 25 个样本)。

### 缺失数据

在回归分析中，缺失数据会导致模型不适合采样数据。您可能会成为以下任何一种情况的受害者:

*   可能会有这样的情况，一些观察数据中的所有字段都缺少值。删除它们将是一个明智的做法，因为如果丢失的值不重要，那么忽略它们不会干扰模型捕捉的整体行为。
*   如果在大多数观察中只有给定的列有缺失值，则不需要处理，因为回归将忽略该变量没有值的情况。

#### 极端值

离群值会在回归模型的结果中产生偏差，从而将最佳拟合线拉向自己。因此，异常值的处理在回归中至关重要，可以通过从第 [1](1.html) 章中强调的方法中选择适当的方法来完成。

### 多重共线性和奇异性

多重共线性和奇异性是破坏回归建模的两个概念，导致奇异和不准确的结果。如果探索性变量是高度相关的，那么回归就容易出现偏差。多重共线性指的是 0.9 或更高的相关性，而奇异性指的是完全相关(即 1)。

对此的补救办法是去除相关性超过 0.7 的探索性变量。但是这又给我们带来了下面的问题。考虑图 [2-7](#Fig7) 中的例子，其中体重和血压(BP)具有 0.95 的高度相关性。

![A432778_1_En_2_Fig7_HTML.gif](A432778_1_En_2_Fig7_HTML.gif)

图 2-7。

Illustration of multicollinearity

现在哪个(体重或 BP)是麻烦制造者，应该被清除？这就是宽容作为一种补救措施的意义所在？公差可以用数学方法定义如下:

*   公式:公差= 1-r <sup>2</sup>
*   插图:
    *   r <sup>2</sup> :该变量与回归方程中所有其他自变量的多重相关的平方

容差是指方程中其他探索性变量未捕捉到的探索性变量方差的比例。这个值越大越好。消除多重共线性和奇异性将确保响应变量是根据独立探测变量捕获的变量预测的，而不是根据它们之间存在的相关性。

在通读了对可用回归技术的描述之后，Smith 很想知道哪种方法最适合于手头的问题。然而，Claire 知道找出正确方法的过程需要在前进之前对数据有所了解。因此，她决定执行功能的探索。她所说的特征是指数据集中的变量。她主要感兴趣的是看看数据是什么样的，以及是否存在以下情况:

*   探索变量和响应变量之间是否相关
*   多重共线性和奇异性

## 特征探索

Claire 首先将数据加载到内存中(参见清单 [2-2](#Par84) )。

```
data = pd.read_csv('examples/concrete_data.csv')
Listing 2-2.Reading the Data in the Memory

```

史密斯很想知道有多少数据，数据是什么样子的。因此，Claire 编写了清单 [2-3](#Par86) 中的代码来打印数据集的一些初始观察结果，以了解它包含的内容。

```
print len(data)
data.head()
Listing 2-3.Printing the Size of the Dataset and Printing the First Few Rows of the Dataset

```

输出

表 2-2。

Print of Observations of the Dataset

![A432778_1_En_2_Figa_HTML.gif](A432778_1_En_2_Figa_HTML.gif)

```
1030

```

在查看表 [2-2](#Tab2) 中的前几个观察值时，Smith 注意到除了年龄之外的所有特征都是浮点数，年龄是唯一的整数变量。他还注意到，当建造混凝土时，高炉矿渣、粉煤灰和超塑化剂并不总是先决条件。此外，他注意到数据集中的具体数据可能是一年前的(即第 3 行的年龄为 365 年)

Claire 清楚地知道，对于要完成的分析(即相关、回归)，特性名称尽可能简单是很重要的。因此，她编写了清单 [2-4](#Par91) 中的代码片段，以使名称简洁易读。对于重命名，她遵循表 [2-3](#Tab3) 中定义的名称映射。

表 2-3。

Variable Names’ Mapping

<colgroup><col> <col></colgroup> 
| 旧功能名称 | 新功能名称 |
| --- | --- |
| 水泥(成分 1)(每立方米混合物中的千克数) | 水泥 _ 成分 |
| 高炉矿渣(成分 2)(每立方米混合物中的千克数) | 炉渣 |
| 飞灰(成分 3)(每立方米混合物中的千克数) | 粉煤灰 |
| 水(成分 4)(每立方米混合物中的千克数) | 水 _ 成分 |
| 超塑化剂(成分 5)(每立方米混合物中的千克数) | 超增塑剂 |
| 粗骨料(成分 6)(m3 混合物中的千克数) | 粗骨料 |
| 细骨料(成分 7)(每立方米混合物中的千克数) | 细骨料 |
| 年龄(天数) | 年龄 |
| 混凝土抗压强度(兆帕，兆帕) | 混凝土 _ 强度 |

```
data.columns = ['cement_component', 'furnace_slag', 'flay_ash', 'water_component', 'superplasticizer', \
            'coarse_aggregate', 'fine_aggregate', 'age', 'concrete_strength']
Listing 2-4.Renaming the Columns

```

看到数据的样子后，Claire 该看看探索变量与响应变量的关联程度，以及数据中是否存在多重共线性/奇异性。

### 相互关系

Claire 首先决定查看任何探索性变量是否与响应变量(即混凝土强度)相关。这个问题对她来说至关重要，因为她认为两个定量实体之间的高度相关性可以导致更好的最佳拟合线性回归线。因此，她有兴趣确定这些定量之间的关系的强度和方向。为此，她编写了清单 [2-5](#Par96) 中的代码。然而，在继续之前，她解释了为什么相关性和回归不完全相同:

> Correlation can be calculated between any two quantifications. However, regression always switches between response variables and exploration variables. Correlation is limited to two quantitative variables, while regression can have more than two quantitative variables, namely a response variable and more than one exploratory variable, also known as multiple regression.

Claire 还指出，直观查看相关性的一个简单方法是使用响应变量和探索变量之间的散点图，一次一个(图 [2-8](#Fig8) )。

![A432778_1_En_2_Fig8_HTML.gif](A432778_1_En_2_Fig8_HTML.gif)

图 2-8。

Scatter plot between response variable and exploratory variables

```
plt.figure(figsize=(15,10.5))
plot_count = 1

for feature in list(data.columns)[:-1]:
        plt.subplot(3,3,plot_count)
        plt.scatter(data[feature], data['concrete_strength'])
        plt.xlabel(feature.replace('_',' ').title())
        plt.ylabel('Concrete strength')
        plot_count+=1

plt.show()

Listing 2-5.Plotting Scatter Plots Between the Response and Exploratory Variables

```

克莱尔想出了清单 [2-5](#Par96) 中的代码描述，让史密斯对所用的方法放心。她回忆说了下面的话:

> By using subplots, we designed multiple plots in the same graph. First, we define the graphic size by `plt.figure(figsize=(15,10.5))`. This will fix the width of the figure to 15 and the height to 10.5\. The response variable (i.e. concrete strength) means that the response variables of all plots remain unchanged, while the exploration variables are considered one at a time (i.e. plot by plot). Therefore, we used: `for feature in list(data.columns)[:-1]`, which traversed all variables except the last one (that is, the response variable). Then we define the subplots index as follows: `plt.subplot(3,3,plot_count)`. The first three define the number of rows, while the other three define the number of columns. This means that the map will be able to accommodate up to 9 plots. `plot_count` Define the position index of a specific graph on the graph. The possible values in this case are 1 to 9.

史密斯决定试一试，通过查看图 [2-8](#Fig8) 来解释这种相关性。因此，他提出了以下三点见解:

*   除了混凝土强度和龄期之间的曲线之外，在大多数这些曲线中，异常值的存在是可以忽略的。
*   在一些散点图中，我们看到很高频率的值位于 0。这可以从混凝土`strength`变量和探测变量(主要是年龄、炉渣、飞灰和超塑化剂)之间的曲线图中看出。
*   似乎存在这样的例子，在这两个定量之间存在正的，有时是负的，有时是不相关的。

Claire 决定通过列出每个实例的配对来扩展这些发现(即，没有、正相关和负相关)。为此，她根据图 [2-9](#Fig9) 中的基准相关性评估了图 [2-6](#Fig6) 中的地块。如果有趋势，它将是负相关或正相关。缺乏明确的趋势将表明不存在相关性。

![A432778_1_En_2_Fig9_HTML.gif](A432778_1_En_2_Fig9_HTML.gif)

图 2-9。

Illustration of positive, negative, and no correlation

*   正相关存在于
    *   水泥成分和混凝土强度
    *   高效减水剂与混凝土强度
*   负相关位于
    *   粉煤灰与混凝土强度
    *   水成分和混凝土强度
    *   粗骨料和混凝土强度
    *   细骨料和混凝土强度
*   之间不存在关联
    *   炉渣与混凝土强度
    *   年龄和混凝土强度

视觉表征所识别的配对使得史密斯很容易理解什么是相关性。然而，他想验证这些发现在统计学上是否也有意义。因此，Claire 想出了清单 [2-6](#Par116) 中的代码片段并计算了 Pearson 相关性。

![A432778_1_En_2_Fig10_HTML.gif](A432778_1_En_2_Fig10_HTML.gif)

图 2-10。

Correlations between response variable and exploratory variables

```
pd.set_option('display.width', 100)
pd.set_option('precision', 3)
correlations = data.corr(method='pearson')
print(correlations)

Listing 2-6.Calculating Pair-wise Pearson

Correlations

```

Claire 回忆起用以下文字解释了这些发现:

> The output in figure [2-10](#Fig10) shows a grid, in which the Pearson correlation between all features in the data set is calculated, not just the correlation between exploration variables and response variables, as we saw in figure [2-8](#Fig8) before. The value of strong positive correlation is 1, the value of strong negative correlation is -1, and 0 means no correlation. By looking at the scatter diagram in Figure [2-8](#Fig8) , we assume that there is no correlation between age and concrete strength. However, the statistical results in Figure [2-10](#Fig10) are not the case. That is to say, there is a slight positive correlation between these two quantities. Visual inference is prone to human error, so we will choose the latter (that is, there is a slight positive correlation between these two quantities).

Smith 知道每个探索性变量是如何与响应变量相关联的，他很想调查数据集中是否存在奇异性或多重共线性。这正是 Claire 列表中的下一件事，因此她编写了清单 [2-7](#Par120) 中的代码。

![A432778_1_En_2_Fig11_HTML.gif](A432778_1_En_2_Fig11_HTML.gif)

图 2-11。

Pair plot of all features in the dataset

```
data_ = data[(data.T != 0).any()]
seaborn.        pairplot(data_, vars=data.columns, kind='reg')
plt.show()
Listing 2-7.Calculating Pair Plot Between All Features

```

克莱尔决定先向史密斯解释清单 [2-7](#Par120) 中的代码是什么意思。她指出了图 [2-8](#Fig8) 中一个显而易见的方面(即数据集中的一些要素在大多数情况下值为 0)。她认为，数据集中的大多数值为 0 会导致相关系数和回归线不能涵盖数据集的真正本质。她首先解释说`data_= data[(data.T != 0).any()]`意味着删除值为 0 的列中的记录。之后，她使用 seaborn pairplot 绘制了数据集中所有特征之间的相关性。

在解释图 [2-11](#Fig11) 中的输出时，Claire 要求 Smith 专注于第 1 列和第 2 行。克莱尔回忆起曾经解释过:“在许多情况下，特征对之间没有相关性。例外在于两种情况——即水泥成分和炉渣以及水泥成分和粉煤灰——我们可以看到强烈的负相关性。”

史密斯被他迄今为止获得的真知灼见所折服。他已经看到了从观测值中去掉零点后的曲线图。他很想知道从数据集中移除零点后，相关系数会是什么样子。此外，他知道这些特征之间存在 Spearman 相关性，因此他很想知道在给定的情况下相关系数是多少。最后，在查看图 [2-8](#Fig8) 时，在 Smith 看来，除了年龄之外，所有特征都是连续的，年龄在他看来就像是一个离散变量，因为年龄范围内的观察值属于七个离散值中的一个，Smith 感兴趣的是将数据分割成年龄段，并计算每个分割的相关性，以查看响应变量和任何探索变量之间的强相关性。你能帮助史密斯回答这些问题吗？在下面的练习中试试吧。

Exercises

1.  从数据要素中移除零，然后重新计算皮尔逊相关性。相关系数有提高吗？
2.  确定数据集中要素的 Spearman 相关性。这给相关性分数带来了边际差异吗？
3.  从散点图来看，年龄似乎更像是一个离散的特征。逐一获取每个年龄的数据，并计算其余特征的皮尔逊相关性。任何特定的年龄在反应变量和探索变量之间产生良好的相关性吗？

既然 Claire 和 Smith 理解了对混凝土强度有重大影响的探索性变量，并且了解了可能存在多重共线性的地方，那么是时候将回归付诸实践了。他们需要回归来得出一个方程，使他们能够测量混凝土强度，而不需要购买一些特殊的设备来测量它。然而，在继续回归模型之前，Claire 觉得她需要让 Smith 意识到过度拟合和欠拟合的重要概念。

## 过度拟合和欠拟合

过度拟合是指模型与数据集高度拟合的现象。因此，这种概括使模型无法对看不见的数据做出高度准确的预测。过拟合在图 [2-12](#Fig12) 中用非线性线表示。

![A432778_1_En_2_Fig12_HTML.gif](A432778_1_En_2_Fig12_HTML.gif)

图 2-12。

Illustration of overfitting

以下方法可用于抑制过度拟合:

*   让模型变得简单；也就是说，调整尽可能多的最小参数，因为它们越复杂，就越会使手头的数据过拟合。
*   执行交叉验证。例如，随机选择 x%的数据值用于训练，剩余的 y%用于测试。根据训练数据拟合模型，用它来预测测试数据的值，并计算测试误差。重复这个练习 n 次。每次训练/测试分割将随机进行。计算所有测试误差的平均值，找出手头模型的真实测试误差。

欠拟合是一种模型没有根据手头的数据进行高精度训练的现象。

对拟合不足的处理受到偏差和变化的影响。如图 [2-13](#Fig13) 所示，如果训练和测试误差都很高，则模型会有很高的偏差。

![A432778_1_En_2_Fig13_HTML.gif](A432778_1_En_2_Fig13_HTML.gif)

图 2-13。

Illustration of high bias

此外，如果测试和训练误差之间存在较大差距，模型将具有较高的方差，如图 [2-14](#Fig14) 所示。

![A432778_1_En_2_Fig14_HTML.gif](A432778_1_En_2_Fig14_HTML.gif)

图 2-14。

Illustration of high variance

如果模型具有高偏差类型的欠拟合，则补救措施可以是增加模型的复杂性，并且如果模型遭受高方差类型的欠拟合，则补救措施可以是引入更多数据或者使模型不那么复杂。

Claire 决定将数据随机分为训练/测试部分，以验证模型的准确性，并在过度拟合的情况下进行交叉验证(清单 [2-8](#Par138) )。

```
def split_train_test(data, feature, train_index=0.7):

    train, test = train_test_split(data, test_size = 1-train_index)

    if type(feature) == list:
        x_train = train[feature].as_matrix()
        y_train = train['concrete_strength'].as_matrix()

        x_test = test[feature].as_matrix()
        y_test = test['concrete_strength'].as_matrix()

    else:
        x_train = [[x] for x in list(train[feature])]
        y_train = [[x] for x in list(train['concrete_strength'])]

        x_test = [[x] for x in list(test[feature])]
        y_test = [[x] for x in list(test['concrete_strength'])]

    return x_train, y_train, x_test, y_test

Listing 2-8.Splitting the Data in Training and Testing sets

```

Claire 向 Smith 解释了清单 [2-8](#Par138) 中的代码如下:“在上面的代码中，我们执行的是 0.7-0.3 分割。这意味着数据中 70%的观察值构成了数据拟合的训练数据集，而 30%的观察值构成了模型评估的测试数据集。”

Claire 指出，欠拟合和过拟合是在首次应用时可以从模型的输出中检测到的。为了确定根除欠拟合和过拟合的最佳方法，您必须经历建模的多次迭代，直到您找到一个没有这两种情况的模型。因此，这是一个正在进行的练习，无论何时任何模型(回归、分类等)都应该进行。)应用于数据。

Smith 很想知道是否存在一些技术来评估应用于数据集的回归模型的准确性。Claire 的回答是肯定的，因此她为此编写了下一部分。

## 评估的回归度量

Sklearn.metrics 是确定回归模型性能的一个很好的工具。它通过实现几个效用函数、分数和损失来做到这一点。这些通常给出单一输出；然而，它们已经得到了增强，可以生成多种输出，最显著的是平均绝对误差、R <sup>2</sup> 得分、均方误差和解释方差得分。

Claire 列出了回归分析中最常用的评估方法。

### 解释方差得分

该分数定义了由最佳拟合回归模型解释的总体中方差的比例。该指标的最佳得分是 1。解释方差得分的数学定义如下:

*   公式:![$$ explaine{d}_{variance}\left(y,\widehat{y}\right)=1-\frac{Var\left\{y-\widehat{y}\right\}}{Var\left\{y\right\}} $$](A432778_1_En_2_Chapter_IEq4.gif)
*   其中:
    *   yˇ:预计目标产量
    *   y:相应的(正确的)目标输出
    *   Var:方差

### 绝对平均误差

平均绝对误差(MAE)是残差的平均值(即估计目标和实际目标结果之间的差异)。该度量的替代公式可以包括相对频率作为权重因子。MAE 在测量数据的同一尺度(即单位)上工作。因此，局限性在于无法使用不同的尺度/单位对系列进行比较。这可以用数学方法定义如下:

*   公式:![$$ {\displaystyle \begin{array}{c}\hfill {y}_i-{\widehat{y}}_i\vee \hfill \\ {}\hfill MAE\left(y,\widehat{y}\right)=\frac{1}{n_{samples}}{\sum}_{1=0}^{n_{samples}-1}\hfill \end{array}} $$](A432778_1_En_2_Chapter_IEq5.gif)
*   其中:
    *   yˇ<sub>I</sub>:预计目标产量
    *   y <sub>i</sub> :对应的(正确的)目标输出
    *   n <sub>个样本</sub>:样本数

### 均方误差

均方差(MSE)类似于 MAE，只是现在采用残差的平方，而不是采用估计的目标输出和相应的实际目标输出之间的绝对差。该值始终为正值，越接近 0 的值越好。取 MSE 的平方根将得到均方根偏差(RMSE ),其单位与目标输出的单位相同。MSE 的数学定义如下:

*   公式:![$$ MSE\left(y,\widehat{y}\right)=\frac{1}{n_{samples}}{\sum}_{1=0}^{n_{samples}-1}{\left({y}_i-{\widehat{y}}_i\right)}^2 $$](A432778_1_En_2_Chapter_IEq6.gif)
*   其中:
    *   yˇ<sub>I</sub>:第 i <sup>个</sup>样本的预计目标产量
    *   y <sub>i</sub> :对应的(正确的)目标输出
    *   n <sub>个样本</sub>:样本数

### R <sup>2</sup>

R <sup>2</sup> ，也称为决定系数，定义了回归模型有可能预测多少未来样本的度量。此外，它还表示响应变量中可从探索性变量中预测的方差的比例。值的范围从 1 到负无穷大，其中 1 是最佳值。数学表示如下:

*   公式:![$$ {R}^2\left(y,\widehat{y}\right)=1-\frac{{\displaystyle {\sum}_{1=0}^{n_{samples}-1}}{\left({y}_i-{\widehat{y}}_i\right)}^2}{{\displaystyle {\sum}_{1=0}^{n_{samples}-1}}{\left({y}_i-\acute{y}\right)}^2} $$](A432778_1_En_2_Chapter_IEq7.gif)
*   公式:![$$ {r}^2=\frac{varianceofpredictedvalues\widehat{y}}{varianceofobservedvaluesy} $$](A432778_1_En_2_Chapter_IEq8.gif)
*   示例:
    *   R = -0.7
    *   R <sup>2</sup> = 0.49(即一半的变化由线性关系解释)
*   其中:
    *   yˇ<sub>I</sub>:第 i <sup>个</sup>样本的预计目标产量
    *   y <sub>i</sub> :对应的(正确的)目标输出
    *   ![$$ \widehat{y}=\frac{1}{n_{samples}}\;{\displaystyle \sum_{1=0}^{n_{samples}-1}{y}_i} $$](A432778_1_En_2_Chapter_Equc.gif)

    *   n <sub>个样本</sub>:样本数

### 剩余

*   公式:残差=观测 y-预测 y 或(![$$ y-\widehat{y} $$](A432778_1_En_2_Chapter_IEq9.gif))
*   最小二乘残差的平均值总是 0

### 残差图

残差图是回归残差相对于探索变量 x(即独立变量)的散点图。参见图 [2-15](#Fig15) 。最小二乘回归线被拉向 x 方向上的一个极端点，该点附近没有其他点。虚线代表最小二乘回归线。

![A432778_1_En_2_Fig15_HTML.gif](A432778_1_En_2_Fig15_HTML.gif)

图 2-15。

Residual plot

### 残差平方和

残差平方和(RSS)是指回归模型的最佳拟合线未捕捉到的数据中的方差。RSS 越小，模型越好。

为了简单起见，我们将继续使用 R <sup>2</sup> 作为交叉验证的度量。此外，考虑到我们正在进行预测，这似乎是一个更好的选择，这一指标将帮助我们确定未来样本在准确度上的位置。

现在，Claire 已经了解了欠拟合和过拟合的概念以及回归的评估技术，她认为是时候测试他们在寻找最适合手头数据的回归模型方面的能力了。

## 回归的类型

Claire 继续指出，SKLearn 是 Python 中最常见的机器学习库。SKLearn 为回归建模提供了一系列算法。她从 SKLearn 的网站上找到了图 [2-13](#Fig13) ,作为确定回归算法的参考，这些算法通常是给定数据集大小的最佳算法。

可以从 [`http://scikit-learn.org/stable/tutorial/machine_learning_map/`](http://scikit-learn.org/stable/tutorial/machine_learning_map/) 下载图 [2-16](#Fig16) 所示的备忘单。

![A432778_1_En_2_Fig16_HTML.jpg](A432778_1_En_2_Fig16_HTML.jpg)

图 2-16。

Cheat sheet of Regression-SKLearn

Smith 决定尝试一下，尝试确定适合当前数据集大小的回归模型。他回忆说:“在我们的案例中，样本数量少于 100K，因此潜在的模型有 Lasso、ElasticNet、SVR(kernel='rbf ')、集合回归、岭回归和 SVR(kernel='linear ')。”

线性回归是最基本的回归模型，因此 Claire 决定从它开始建模。

### 线性回归

多亏了克莱尔，史密斯已经知道什么是线性回归(见图 [2-2](#Fig2) )。因此克莱尔决定把它放到应用程序中，看看它的表现如何，所以她编译了清单 [2-9](#Par184) 中的代码。

![A432778_1_En_2_Fig17_HTML.gif](A432778_1_En_2_Fig17_HTML.gif)

图 2-17。

Single linear regression plots

```
plt.figure(figsize=(15,7))
plot_count = 1

for feature in ['cement_component', 'flay_ash', 'water_component', 'superplasticizer', 'coarse_aggregate']:
    data_tr = data[['concrete_strength', feature]]
    data_tr=data_tr[(data_tr.T != 0).all()]

    x_train, y_train, x_test, y_test = split_train_test(data_tr, feature)

    # Create linear regression object
    regr = LinearRegression()

    # Train the model using the training sets
    regr.fit(x_train, y_train)
    y_pred = regr.predict(x_test)

    # Plot outputs
    plt.subplot(2,3,plot_count)

    plt.scatter(x_test, y_test,  color='black')
    plt.plot(x_test, y_pred, color='blue',
             linewidth=3)
    plt.xlabel(feature.replace('_',' ').title())
    plt.ylabel('Concrete strength')

    print feature, r2_score(y_test, y_pred)

    plot_count+=1

plt.show()

Listing 2-9.Calculating Single

Linear Regression

```

输出

```
cement_component     0.250526602238
flay_ash             0.0546142086558
water_component      0.112291736027
superplasticizer     0.0430808938239
coarse_aggregate     0.0206812124102

```

解释代码对史密斯理解结果是必要的。因此，Claire 对清单 [2-9](#Par184) 中的代码给出了如下解释:

> When looking at the above code, you will notice that we only used exploratory variables, among which we noticed positive correlation or negative correlation when we visually looked at Figure [2-8](#Fig8) . Secondly, we initialize a subplots graph with a width of 15 and a height of 7\. Thirdly, we use the training/test separation method for cross-validation.

史密斯决定尝试解释输出的含义。几乎所有探索性变量的 R <sup>2</sup> 值都更接近于 0，这表明线性回归最佳拟合线未能捕捉到每个变量的方差。但是，水泥成分是一个例外，其最佳拟合线捕捉到了相对较好的方差。考虑到 R <sup>2</sup> 较低，这两个模型都不能用于预测混凝土强度。

```
features = ['cement_component', 'flay_ash', 'water_component', 'superplasticizer', 'coarse_aggregate']

data_tr = data
data_tr=data_tr[(data_tr.T != 0).all()]

x_train, y_train, x_test, y_test = split_train_test(data_tr, features)

# Create linear regression object
regr = LinearRegression()

# Train the model using the training sets
regr.fit(x_train, y_train)
y_pred = regr.predict(x_test)

plt.scatter(range(len(y_test)), y_test,  color='black')
plt.plot(y_pred, color='blue', linewidth=3)

print 'Features: %s'%str(features)
print 'R2 score: %f'%r2_score(y_test, y_pred)
print 'Intercept: %f'%regr.intercept_
print 'Coefficients: %s'%str(regr.coef_)

Listing 2-10.Calculating Multiple

Linear Regression

```

输出

![A432778_1_En_2_Fig18_HTML.gif](A432778_1_En_2_Fig18_HTML.gif)

图 2-18。

Multiple linear regression plot

```
Features:      ['cement_component', 'flay_ash', 'water_component', 'superplasticizer', 'coarse_aggregate']
R2 score:      0.097982
Intercept:     77.802791
Coefficients:  [0.04531335  0.01168227 -0.13620573  0.24324622 -0.0329745

```

克莱尔很惊讶，因为史密斯完全正确。她很好奇，如果她同时加入所有探索性变量进行多元回归，会发生什么。因此，她想出了清单 [2-10](#Par191) 中的代码片段。史密斯能够在一定程度上理解代码；然而，他惊讶地看到图 [2-18](#Fig18) 中的非线性回归最佳拟合线。

他认为多元线性回归应该产生一条线性线，而不是一条非线性的最佳拟合线。对此克莱尔回应如下:

> The number of features in the analysis is proportional to the number of dimensions on the graph. In our example, we have five exploration variables and one response variable, with a total of six variables. It will be difficult to draw points in 6 dimensions, so use two dimensions to represent them in Figure [2-18](#Fig18) . If we see the best fitting line in 6-dimensional space, then it should be expressed linearly.

Claire 指出，由于该多元回归的 R <sup>2</sup> 极低，因此不能使用该模型来预测响应变量(即混凝土强度)。注意 R <sup>2</sup> 极低；因此，我们肯定不会用这个来回答眼前的问题。然而，输出也给了我们系数。第一个是α系数，随后的几个代表β系数。Smith 和 Claire 都知道数据中存在多重共线性。因此，在寻找适用于多重共线性数据的回归技术的过程中，他们遇到了岭回归，并决定尝试一下。然而，克莱尔认为最好向史密斯解释一下网格搜索。在继续下一步之前，让我们利用其他回归模型，找到最符合我们数据的模型。

### 网格搜索

网格搜索使用“拟合”和“评分”方法来确定给定模型的最佳参数。要调优的模型以及参数和它们的有限可能值在 GridSearchCV 中传递。输出表示模型将被最佳调整的参数值。

解释了网格搜索之后，Clare 编写了一个关于岭回归的知识库。

### 里脊回归

岭回归利用 L2 范数的正则化实现了线性最小二乘函数的损失函数。这种类型的回归有一个内在的支持，即接受多个探索性变量来预测响应变量。L2 是一个向量范数，它捕捉了向量的大小。L2 是最常见的教学和使用频率。假设我们有一个向量![$$ \overrightarrow{\beta} $$](A432778_1_En_2_Chapter_IEq10.gif)，它有两个分量β <sub>0</sub> 和β <sub>1</sub> 。L2-诺姆对此可以定义如下:

*   公式:![$$ {\left|\left|\overrightarrow{\beta}\right|\right|}_2=\sqrt{\beta_0^2+{\beta}_1^2} $$](A432778_1_En_2_Chapter_IEq11.gif)

这是从原点到这些矢量分量的笛卡尔距离(即β <sub>0</sub> 和![$$ {\beta}_1\Big) $$](A432778_1_En_2_Chapter_IEq12.gif))。岭回归适用于以下情况:

*   当我们有不止一个探索变量时
*   探索性变量之间存在多重共线性

多重共线性将导致有偏估计量，因为经过多次迭代计算后，β系数会变得异常高。因此，这将导致我们的问题，最佳系数的迭代是真正的？由于估计值现在会有偏差，在多次迭代中取系数平均值不会产生总体系数。然而，这些系数中的方差不会太高，因此将有助于更好地理解这些系数。为了避免 beta 值过大，我们需要对 beta 值的大小加以限制。因此，我们可以用数学方法将岭回归定义如下:

*   Formula: ![$$ \underset{\overrightarrow{\beta}}{\mathit{\min}}{\left|\left|\overrightarrow{y}-A\overrightarrow{B}\right|\right|}_2^2 $$](A432778_1_En_2_Chapter_IEq13.gif) subject to ![$$ \vee \overrightarrow{\beta}{\vee}_2^2\le {c}^2 $$](A432778_1_En_2_Chapter_IEq14.gif) OR

    ![$$ \underset{\overrightarrow{\beta}}{\mathit{\min}}{\left|\left|\overrightarrow{y}-A\overrightarrow{B}\right|\right|}_2^2+\lambda \vee \overrightarrow{\beta}{\vee}_2^2 $$](A432778_1_En_2_Chapter_Equd.gif)

在岭回归中，考虑到我们的选择是 L2 范数，我们的β值约束是圆形的，如图 [2-19](#Fig19) 所示。之前，我们对数据应用了一元和多元线性回归。然而，Ridge、Lasso 和 ElasticNet 回归更适合多元回归，因此将是我们的分析选择。

![A432778_1_En_2_Fig19_HTML.gif](A432778_1_En_2_Fig19_HTML.gif)

图 2-19。

Representation of Ridge regression

既然 Smith 已经理解了岭回归，他就有兴趣看看它从可用的探索性变量中预测混凝土强度的效果如何。正如我们前面读到的，岭回归支持接受多个探索性变量作为输入；因此，Claire 决定运行多重岭回归，如清单 [2-11](#Par209) 所示。图 [2-20](#Fig20) 显示了结果。

```
alphas = np.arange(0.1,5,0.1)

model = Ridge()
cv = GridSearchCV(estimator=model, param_grid=dict(alpha=alphas))

y_pred = cv.fit(x_train, y_train).predict(x_test)

plt.scatter(range(len(y_test)), y_test,  color='black')
plt.plot(y_pred, color='blue', linewidth=3)

print 'Features: %s'%str(features)
print 'R2 score: %f'%r2_score(y_test, y_pred)
print 'Intercept: %f'%regr.intercept_
print 'Coefficients: %s'%str(regr.coef_)

Listing 2-11.Calculating Multiple

Ridge Regression

```

输出

![A432778_1_En_2_Fig20_HTML.gif](A432778_1_En_2_Fig20_HTML.gif)

图 2-20。

Multiple Ridge regression plot

```
Features:      ['cement_component', 'flay_ash', 'water_component', 'superplasticizer', 'coarse_aggregate']
R2 score:      0.097982
Intercept:     77.802791
Coefficients:  [0.04531335  0.01168227 -0.13620573  0.24324622 -0.03297459] 

```

史密斯不难理解清单 [2-11](#Par209) 中的代码，因为它与清单 [2-10](#Par191) 中的代码相似。他唯一不明白的是网格搜索是如何应用的。因此 Claire 开始解释与 GridSearchCV 相关的代码。她首先指出 alphas = np.arange(0.1，5，0.1)用于生成一个范围为 0.1 到 5 的值数组，偏移量为 0.1。这和岭回归模型一起被输入 GridSearchCV。因此 GridSearchCV 一个接一个地尝试 alpha 值，以确定最能微调模型的值。

理解了清单 [2-11](#Par209) 中的代码后，Smith 有一个问题。应用岭回归时，R <sup>2</sup> 表现更好吗？对此克莱尔说:

一点也不，因为它完全一样，系数也一样。其原因可能是在存在多个特征的情况下，多重共线性线性回归往往偏向 L2 范数，而 L2 范数是岭回归使用的正则化项，因此有相似之处。此外，还要注意我们在方法中使用网格搜索来优化 alpha。那么阿尔法到底是什么？α是一个正则化参数，用于加权岭回归中的 L2 范数项。alpha 值为 0 将岭回归模型转换为普通最小二乘回归模型。因此，α值越高，平滑度约束越高，系数的幅度越低。阿尔法值的效果如图 [2-21](#Fig21) 所示。

![A432778_1_En_2_Fig21_HTML.gif](A432778_1_En_2_Fig21_HTML.gif)

图 2-21。

阿尔法值对回归拟合线平滑度的影响

岭回归即使在参数之后也没有比多元线性回归带来任何改进。Claire 认为，将正则项从 L2 范数改为 L1 范数可能会给 R <sup>2</sup> 分数带来一些改善。因此，她决定尝试套索回归，并在接下来的部分给出了描述。

### 套索回归

与岭回归相反，套索回归使用 L1 范数。拉索回归的 L1 范数可定义如下:

*   公式:![$$ {\left|\left|\overrightarrow{\beta}\right|\right|}_1=\left|{\beta}_0\right|+{\beta}_1\vee $$](A432778_1_En_2_Chapter_IEq15.gif)

数学上，我们将套索回归定义如下:

*   公式:![$$ \underset{\overrightarrow{\beta}}{\mathit{\min}}{\left|\left|\overrightarrow{y}-A\overrightarrow{B}\right|\right|}_2^2 $$](A432778_1_En_2_Chapter_IEq16.gif)服从![$$ \vee \overrightarrow{\beta}{\vee}_1\le c $$](A432778_1_En_2_Chapter_IEq17.gif)或
*   ![$$ \underset{\overrightarrow{\beta}}{\mathit{\min}}{\left|\left|\overrightarrow{y}-A\overrightarrow{B}\right|\right|}_2^2+\lambda \vee \overrightarrow{\beta}{\vee}_1 $$](A432778_1_En_2_Chapter_Eque.gif)

因为我们的岭回归选择是 L2 范数，我们的β值约束是圆形的。然而，在考虑 L1 范数的 Lasso 回归中，我们的β约束是菱形的(见图 [2-22](#Fig22) )。

![A432778_1_En_2_Fig22_HTML.gif](A432778_1_En_2_Fig22_HTML.gif)

图 2-22。

Representation of Lasso regression

考虑到这个菱形的边缘位于第一象限的 x 轴上，我们可以看到坐标是(c，0)。上面的水平曲线在(0，c)处接触菱形，这意味着我们有β <sub>1</sub> 的值，而没有β <sub>2</sub> 的值。因此，在等式中代入以下内容将导致与β <sub>1</sub> 相关的探索变量变为 0。因此，与形状为圆形并包括数据集中所有探索性变量的岭回归相比，整个过程中的套索回归将仅以几个探索性变量结束。

史密斯预计 R <sup>2</sup> 会有所改进，因为他相信只精选几个探索性变量而忽略其他变量会使模型更强大。因此，克莱尔测试了这个模型，如清单 [2-12](#Par225) 所示。

```
model = Lasso()
cv = GridSearchCV(estimator=model, param_grid=dict(alpha=alphas))

y_pred = cv.fit(x_train, y_train).predict(x_test)

plt.scatter(range(len(y_test)), y_test,  color='black')
plt.plot(y_pred, color='blue', linewidth=3)

print 'Features: %s'%str(features)
print 'R2 score: %f'%r2_score(y_test, y_pred)
print 'Intercept: %f'%regr.intercept_
print 'Coefficients: %s'%str(regr.coef_)

Listing 2-12.Calculating Multiple

Lasso Regression

```

输出

```
Features:      ['cement_component', 'flay_ash', 'water_component', 'superplasticizer', 'coarse_aggregate']
R2 score:      0.103610
Intercept:     77.802791
Coefficients:  [0.04531335 0.01168227 -0.13620573 0.24324622 -0.03297459]

```

Smith 注意到 Lasso 回归比多重岭回归和线性回归表现得更好(图 [2-23](#Fig23) )。

![A432778_1_En_2_Fig23_HTML.gif](A432778_1_En_2_Fig23_HTML.gif)

图 2-23。

Multiple Lasso regression plot

然而，按照 Claire 的说法，0.1 的 R <sup>2</sup> 非常低，不是响应变量可以外推得到的。此外，她指出，网格搜索再次用于优化阿尔法项，正如在岭回归中所做的那样。

到目前为止没有取得重大成功，Claire 决定引入最好的回归方法。ElasticNet 结合了这两种方法(即脊和套索)。

### 弹性网

ElasticNet 克服了 Lasso 回归中的局限性(即惩罚函数)。Lasso 倾向于只选择一些探索性变量，对于探索性变量的多重共线性组，它将只从组中选择一个。为了避免这种情况，明智的做法是在罚函数中添加一个二次部分(即，岭回归中存在的![$$ {\left|\left|\beta \right|\right|}^2\Big) $$](A432778_1_En_2_Chapter_IEq18.gif))。因此，ElasticNet 所做的是包括套索和脊回归惩罚的凸和。

Claire 编写了清单 [2-13](#Par233) 中的代码，以在数据上尝试多个 ElasticNet。

```
model = ElasticNet()
cv = GridSearchCV(estimator=model, param_grid=dict(alpha=alphas))

y_pred = cv.fit(x_train, y_train).predict(x_test)

plt.scatter(range(len(y_test)), y_test,  color='black')
plt.plot(y_pred, color='blue', linewidth=3)

print 'Features: %s'%str(features)
print 'R2 score: %f'%r2_score(y_test, y_pred)
print 'Intercept: %f'%regr.intercept_
print 'Coefficients: %s'%str(regr.coef_)

Listing 2-13.Calculating Multiple ElasticNet Regression

```

输出

![A432778_1_En_2_Fig24_HTML.gif](A432778_1_En_2_Fig24_HTML.gif)

图 2-24。

Multiple ElasticNet regression plot

```
Features:      ['cement_component', 'flay_ash', 'water_component', 'superplasticizer', 'coarse_aggregate']
R2 score:      0.099785
Intercept:     77.802791
Coefficients:  [0.04531335  0.01168227 -0.13620573  0.24324622 -0.03297459] 

```

克莱尔对这个模型的结果很失望(图 [2-24](#Fig24) )。她回忆道:

> Compared with Lasso regression, the cross-validation here seems to be lower. As we read earlier, Lasso seems to consider a large block of exploratory variables; Therefore, ElasticNet's efforts to add them together offset Lasso's advantage in isolation. This is the behavior we see in the data at hand, but it should be noted that it does not represent the universality of all problem sets.

Smith 很好奇是否有技术可以迭代地提高回归模型的准确性。克莱尔知道答案(即梯度推进回归)。

### 梯度推进回归

分类或回归树模型的集合填充到梯度增强模型中(见图 [2-25](#Fig25) )。Boosting 是一种非线性灵活回归技术，它通过为错误预测分配更多权重来帮助提高树的准确性。引入更多权重的原因是模型可以更加强调这些错误预测的样本，并自我调整以提高准确性。梯度提升方法解决了提升树的固有问题(即，速度低和人类可解释性)。该算法通过指定线程数量来支持并行性。

![A432778_1_En_2_Fig25_HTML.gif](A432778_1_En_2_Fig25_HTML.gif)

图 2-25。

Illustration of gradient boosting regression

Claire 决定对相关数据集运行单一和多重梯度推进回归。她从编写清单 [2-14](#Par241) 中的单梯度推进回归代码开始。

```
plt.figure(figsize=(15,7))
plot_count = 1

for feature in ['cement_component', 'flay_ash', 'water_component', 'superplasticizer', 'coarse_aggregate']:
    data_tr = data[['concrete_strength', feature]]
    data_tr=data_tr[(data_tr.T != 0).all()]

    x_train, y_train, x_test, y_test = split_train_test(data_tr, feature)

    # Create linear regression object
    regr = GradientBoostingRegressor()

    # Train the model using the training sets
    regr.fit(x_train, y_train)
    y_pred = regr.predict(x_test)

    # Plot outputs
    plt.subplot(2,3,plot_count)

    plt.scatter(x_test, y_test,  color='black')
    plt.plot(x_test, y_pred, color='blue',
             linewidth=3)
    plt.xlabel(feature.replace('_',' ').title())
    plt.ylabel('Concrete strength')

    print feature, r2_score(y_test, y_pred)

    plot_count+=1

plt.show()

Listing 2-14.Calculating Single

Gradient Boosting Regression

```

输出

![A432778_1_En_2_Fig26_HTML.gif](A432778_1_En_2_Fig26_HTML.gif)

图 2-26。

Single gradient boosting regression plot

```
cement_component    0.339838267592
flay_ash            0.0804872388797
water_component     0.311858270879
superplasticizer    0.123130891086
coarse_aggregate    0.230383758064

```

史密斯很高兴看到这些结果，因为经过大量努力，他们能够显著提高回归模型的 R <sup>2</sup> 系数(图 [2-26](#Fig26) )。

在查看水泥成分、水成分和粗骨料的 R <sup>2</sup> 系数值时，Smith 发现 R <sup>2</sup> 相对于他们从线性回归中获得的值更好。他知道通过结合网格搜索来调整参数值可以改善结果。克莱尔仍然对结果不满意；因此，她继续运行清单 [2-15](#Par246) 中的多重梯度增强。

```
model = GradientBoostingRegressor()

y_pred = model.fit(x_train, y_train).predict(x_test)

plt.scatter(range(len(y_test)), y_test,  color='black')
plt.plot(y_pred, color='blue',
         linewidth=3)

print 'Features: %s'%str(features)
print 'R2 score: %f'%r2_score(y_test, y_pred)
print 'Intercept: %f'%regr.intercept_
print 'Coefficients: %s'%str(regr.coef_)

Listing 2-15.Calculating Multiple

Gradient Boosting Regression

```

输出

![A432778_1_En_2_Fig27_HTML.gif](A432778_1_En_2_Fig27_HTML.gif)

图 2-27。

Multiple gradient boosting regression plot

```
Features:      ['cement_component', 'flay_ash', 'water_component', 'superplasticizer', 'coarse_aggregate']
R2 score:      0.005876
Intercept:     77.802791
Coefficients:  [0.04531335  0.01168227 -0.13620573  0.24324622 -0.03297459]

```

史密斯看到 R <sup>2</sup> 每况愈下。单梯度推进回归似乎比多梯度推进回归表现得更好(图 [2-27](#Fig27) )。

Claire 曾经读到过，为了更好地对数据进行分类或外推，一种方法是在高维空间中绘制数据。支持向量机倾向于在有核的情况下这样做。

### 支持向量机

![A432778_1_En_2_Fig28_HTML.gif](A432778_1_En_2_Fig28_HTML.gif)

图 2-28。

Illustration of support vector machine regression

支持向量机构建超平面用于分类和回归(见图 [2-28](#Fig28) )。目标是在两个类之间有最大的分离。这是通过最大化超平面和每侧数据点之间的距离来确保的。在此过程中，集合在该空间中通常不是线性可分的，因此建议我们将其映射到一个高维空间(即，通过引入核)。

Claire 决定在数据集上运行单个和多个支持向量机回归。她还计划将线性核应用于支持向量回归机，以获得更好的结果。

```
plt.figure(figsize=(15,7))
plot_count = 1

for feature in ['cement_component', 'flay_ash', 'water_component', 'superplasticizer', 'coarse_aggregate']:
    data_tr = data[['concrete_strength', feature]]
    data_tr=data_tr[(data_tr.T != 0).all()]

    x_train, y_train, x_test, y_test = split_train_test(data_tr, feature)

    # Create linear regression object
    regr = SVR(kernel='linear')

    # Train the model using the training sets
    regr.fit(x_train, y_train)
    y_pred = regr.predict(x_test)

    # Plot outputs
    plt.subplot(2,3,plot_count)

    plt.scatter(x_test, y_test,  color='black')
    plt.plot(x_test, y_pred, color='blue', linewidth=3)
    plt.xlabel(feature.replace('_',' ').title())
    plt.ylabel('Concrete strength')

    print feature, r2_score(y_test, y_pred)

    plot_count+=1

plt.show()

Listing 2-16.Calculating Single

Support Vector Machine Regression Using Linear Kernel

```

输出

![A432778_1_En_2_Fig29_HTML.gif](A432778_1_En_2_Fig29_HTML.gif)

图 2-29。

Single support vector machine regression plot

```
cement_component    0.186215229943
flay_ash            0.0566844466086
water_component     0.0824723749594
superplasticizer    0.0412024702221
coarse_aggregate    0.0293294512993

```

Smith 注意到，与图 [2-23](#Fig23) 中的梯度推进回归方程的最佳拟合线相反，图 [2-26](#Fig26) 中的最佳拟合线看起来更加清晰、线性，并且相对较少过度拟合。然而，R <sup>2</sup> 受到了打击，因为它比他们在单梯度推进回归器中观察到的要低(见图 [2-29](#Fig29) )。

克莱尔想出了清单 [2-17](#Par258) 中的代码，看看多个支持向量机是否会给 R <sup>2</sup> 系数带来任何改善。图 [2-30](#Fig30) 显示了结果。

```
model = SVR(kernel='linear')

y_pred = model.fit(x_train, y_train).predict(x_test)

plt.scatter(range(len(y_test)), y_test,  color='black')
plt.plot(y_pred, color='blue', linewidth=3)

print 'Features: %s'%str(features)
print 'R2 score: %f'%r2_score(y_test, y_pred)

Listing 2-17.Calculating Multiple

Support Vector Machine Regression Using Linear Kernel

```

输出

![A432778_1_En_2_Fig30_HTML.gif](A432778_1_En_2_Fig30_HTML.gif)

图 2-30。

Multiple Support Vector Machine regression plot

```
Features:  ['cement_component', 'flay_ash', 'water_component', 'superplasticizer', 'coarse_aggregate']
R2 score:  0.010077

```

按照 Smith 的标准，支持向量回归机在他们之前研究的模型中表现最差。Claire 认为可以通过调整参数来提高性能。此外，其他类型的内核可以投入实践，以提高预测的效率。

史密斯和克莱尔今天结束了。帮助他们调整模型，并通过完成以下练习来尝试不同的内核。

Exercises

1.  运行带有“rbf”核的支持向量机回归。此外，尝试运行“多项式”和“sigmoid”内核，以优化预测精度。
2.  梯度推进回归器具有学习率、`min_samples_split`、`min_samples_leaf`参数，可在这些参数上进行调整。对这些进行网格搜索，以提高回归器的效率。
3.  支持向量回归机有 c 和 gamma 参数，可以根据这些参数进行调整。对这些进行网格搜索，以提高回归器的效率。
4.  梯度推进回归器返回每个拟合模型的特征重要性。探索模型更重视的特性，并通过单独或一起考虑这些模型来重新建模。
5.  在之前的练习中，我们根据年龄和计算的相关性对数据进行了分割。选择具有最大相关性的年龄分割，然后重复这些示例和练习，看看我们是否可以为该组数据拟合更高精度的模型。

## 回归的应用

回归的应用在几个研究领域都很活跃。

### 预测销售额

回归可用于预测一种商品或服务的销售，如需求分析(图 [2-31](#Fig31) )。

![A432778_1_En_2_Fig31_HTML.gif](A432778_1_En_2_Fig31_HTML.gif)

图 2-31。

Prediction of retail sales and personal income

销售的历史数据可以用来推断未来的结果。营销成本、商品化、价格和库存单位(SKU)数量等特征可以作为模型的探索变量。

### 预测债券价值

不同时期的通货膨胀率可以用来预测债券的价值。在估算包含债券的投资组合的预期回报时，这可能会改变游戏规则。

### 通货膨胀率

在经济学中，我们有通货膨胀率和货币供给理论的概念。如果一个研究者认为通货膨胀率是经济中货币供应量的函数，那么他能做什么？他可以把通货膨胀率作为反应变量，把货币供应量作为探索变量(见图 [2-32](#Fig32) )。

![A432778_1_En_2_Fig32_HTML.gif](A432778_1_En_2_Fig32_HTML.gif)

图 2-32。

Prediction of inflation rates

### 保险公司

这些公司使用回归来预测可能索回医疗保险或人寿保险的人数，或者可能在其投保的车辆中发生事故的人数。

### 呼叫中心

呼叫中心经理可能想知道投诉数量的增加与呼叫等待时间的关系。

### 农业

回归可用于预测给定地区预期的水果产量。可能的探索变量可以是降雨量、日照时数、影响作物的疾病数量、受火灾或疾病影响的作物数量、土壤质量指数、土地肥力指数等等。

### 预测工资

大学可以使用预测模型来预测学生的工资。可能的探索性变量可以是分数、参加的比赛数量、班级中的座位安排、完成的实习数量、发表的研究论文数量、完成的项目数量、参加的体育赛事数量等等。

### 房地产行业

房地产的价格可以从房产的平方英尺大小、房间数量、停车位的可用性、开放空间的平方英尺、距离主干道的英里数等方面进行预测(见图 [2-33](#Fig33) )。

![A432778_1_En_2_Fig33_HTML.gif](A432778_1_En_2_Fig33_HTML.gif)

图 2-33。

Predicting property as a function of size in feet

总结本章，Claire 和 Smith 首先达成共识，使用回归来预测混凝土强度。他们计划通过回归方程来实现这一点，以避免购买测试设备进行测量的成本。他们从分析开始，首先理解混凝土强度和探测变量之间存在的相关性。表现出正相关或负相关的那些用于回归建模。此外，相关性有助于发现三个探索性变量之间存在的多重共线性。

查看回归模型评估指标，并挑选 R <sup>2</sup> 来评估回归模型。他们研究了什么是网格搜索，以及如何应用网格搜索来确定最佳参数值，以便对模型进行最佳调整。他们将数据分成测试和训练子集，以实现交叉验证。在应用的许多回归技术中，单一梯度推进回归器表现出最好的 R <sup>2</sup> 值。因此，在分析结束时，Smith 和 Claire 有了以下三个模型，他们可以单独使用这些模型来预测混凝土强度。

首先，Claire 定义了梯度推进回归模型的用例，该模型将水泥成分作为预测混凝土强度的输入。她从清单 [2-14](#Par241) 的输出中回忆起，这个回归模型的置信水平大约是 34%。

```
feature = 'cement_component'
cc_new_data = [213.5]

data_tr = data[['concrete_strength', feature]]
data_tr=data_tr[(data_tr.T != 0).all()]

x_train, y_train, x_test, y_test = split_train_test(data_tr, feature)

regr = GradientBoostingRegressor()

# Train the model using the training sets

regr.fit(x_train, y_train)
cs_pred = regr.predict(cc_new_data)
print 'Predicted value of concrete strength: %f'%cs_pred

Listing 2-18.Predicting Concrete Strength from Cement Component

```

输出

```
Predicted value of concrete strength: 34.008896

```

举例来说，水泥成分的输入值在变量‘cc _ new _ data’中传递。克莱尔指出，为了预测混凝土强度，可以在同一个变量中传递一个新值。

然后克莱尔重复了这个练习，不同的是水的成分被作为输入。她从清单 [2-14](#Par241) 的输出中回忆起，这个回归模型的置信水平大约是 31%。

```
feature = 'water_component'
wc_new_data = [200]

data_tr = data[['concrete_strength', feature]]
data_tr=data_tr[(data_tr.T != 0).all()]

x_train, y_train, x_test, y_test = split_train_test(data_tr, feature)

regr = GradientBoostingRegressor()

# Train the model using the training sets
regr.fit(x_train, y_train)
cs_pred = regr.predict(wc_new_data)
print 'Predicted value of concrete strength: %f'%cs_pred

Listing 2-19.Predicting Concrete Strength from Water Component

```

输出

```
Predicted value of concrete strength: 35.533469

```

作为示例，水成分的输入值在变量‘WC _ new _ data’中传递..

最后，Claire 重复了这个练习，这次的不同之处在于粗骨料被作为输入。她从清单 [2-14](#Par241) 的输出中回忆起，这个回归模型的置信水平大约是 23%。

```
feature = 'coarse_aggregate'
ca_new_data = [1000]

data_tr = data[['concrete_strength', feature]]
data_tr=data_tr[(data_tr.T != 0).all()]

x_train, y_train, x_test, y_test = split_train_test(data_tr, feature)

regr = GradientBoostingRegressor()

# Train the model using the training sets
regr.fit(x_train, y_train)
cs_pred = regr.predict(ca_new_data)
print 'Predicted value of concrete strength: %f'%cs_pred

Listing 2-20.Predicting Concrete Strength from Coarse Aggregate

```

输出

```
Predicted value of concrete strength: 32.680344

```

举例来说，水成分的输入值在变量‘ca _ new _ data’中传递..Claire 采用的方法令 Smith 信服，他认为如果在 R&D(研发)上投入更多时间，他可以向管理层推荐这种替代方案，使他们不再需要购买更多设备。