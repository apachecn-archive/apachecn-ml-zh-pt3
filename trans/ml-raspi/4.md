# 4.深度学习的准备

本章将为从以下章节开始的深度学习(DL)讨论提供足够的背景。在试图理解任何实际的 DL 算法之前，理解一些基本的 DL 术语和概念是很重要的。我试图最小化数学，但是有一些不可避免的方程，因为 DL 本质上是数学。

## DL 基础

任何数字图书馆的讨论都必须从回答这个问题开始，“什么是数字图书馆？”像许多相对较新的技术领域一样，如果你请十几个专家来定义某个东西，你可能会得到十几个不同但奇怪地相似的回答。和 DL 没什么区别。我研究了许多不同的 DL 定义，并创建了以下一个似乎包含了许多定义中的共同主题:

> 深度学习是机器学习的一个子领域，涉及称为人工神经网络的算法，这些算法受到人脑结构和功能的启发。从数据集学习可以是 [*有监督*](https://en.wikipedia.org/wiki/Supervised_learning)*[*半监督*](https://en.wikipedia.org/wiki/Semi-supervised_learning) *或者无监督。**

 *我将在下一章介绍和讨论人工神经网络(ANN ),但首先我需要讨论这些基础知识。

### 从数据模式进行机器学习

机器学习(ML)是 DL 的一个重要子集，通常被描述为对算法和数据模型的研究，这些算法和数据模型可以执行计算机实现的任务，而无需显式编程来完成这些任务。相反，ML 依赖于检测数据模式并生成关于数据的推论。有四个主要的任务通常归因于 ML。这些任务是

*   侦查

*   分类

*   承认

*   预报

这些任务中的大部分(如果不是全部)可以应用于各种数据集，包括静态图像、数值数据和实时数据流。最后提到的数据集包括视频、音频，甚至射频(RF)流。我也绝对肯定还有其他我没有提到的 ML 的应用。

我将从集中讨论分类任务开始我的基础讨论，我已经在前面的章节中讨论过了。回想一下，在第 [1](1.html) 章 k-NN 数据模型讨论中，我陈述了以下内容:

> *我将 k-NN 描述为非参数的，这意味着该模型不做任何关于底层数据分布的假设。换句话说，模型结构是由数据决定的。鉴于这一事实，当很少或没有关于数据如何分布的先验知识时，k-NN 可能应该是分类研究的首选之一。*

您可能对我在描述 k-NN 数据模型时使用的术语*非参数*感到有点困惑。希望描述什么是参数化分类算法有助于澄清这个术语。术语参数化定义如下:

> *参数化是一个由表达一个**[系统的状态、](https://en.m.wikipedia.org/wiki/Process_%2528science%2529) *过程或模型作为一个* [函数](https://en.m.wikipedia.org/wiki/Function_%2528mathematics%2529) *的一些独立的量称为*。*

 *在 ML 应用中，用于描述系统状态的关键参数是

*   数据

*   评分功能

*   损失函数

*   权重和偏差

是的，我意识到最后一个列表项有两个组成部分，但是它们紧密地交织在一起，通常被认为是构成一个单一的参数。我将分别讨论每个参数。

数据——这是流程中一个显而易见的元素，所有的 ML 都必须以此为基础。数据有两面，第一面是值，第二面是类标签。从图像中的原始像素值到房屋数据集中的房价，这些值的差异很大。数据通常表示为 ML 域中的矩阵。这种矩阵通常被称为设计矩阵，命名为 **X** ，其中

![$$ {x}_i={i}^{th}\ element\ in\ the\ design\ matrix $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equa.png)

![$$ {y}_i={i}^{th}\ class\ label $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equb.png)

评分函数–将输入数据映射到预测类别标签的函数。这可以用一般的方程形式来表示

![$$ f\left( input\ data\right)= predicted\ class\ label $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equc.png)

实际上，这个等式只产生一个值。与最大值相关联的类标签将是该分类器的预测。

损失函数–一种量化预测类标签与数据集中记录的实际类标签吻合程度的函数。在 ML 术语中，实际的类标签也称为基础事实标签。低损耗值是所期望的，因为这意味着预测与地面真实标签非常一致。本章后面将详细讨论如何计算最小损失值的损失函数。

权重和偏差——权重矩阵`W`和偏差向量`b`被迭代计算，以最小化关于评分函数的损失函数。

#### 线性分类器

在这一节中，我将讨论什么是线性分类器以及它是如何工作的。我包括这个讨论的原因是给你一个框架，通过这个框架你可以更好地理解神经网络是如何工作的。线性分类器和神经网络的基本概念基本相同。我还将结合前面讨论中介绍的四个关键参数中的三个。损失函数将在下一节单独讨论。

用于该线性分类器的数据集被命名为哺乳动物，由三个类别的 5000 幅图像组成，即猫、狗和松鼠。每张图像的分辨率相当低，为 32 x 32 像素，总共 1024 像素。此外，每个像素是全色 RGB，需要三个字节来表示 RGB 颜色通道值，这意味着分类器要处理 3072 个值。代表图像的数据点被“展平”成一个一维向量(1D)，它被命名为具有 N 个元素的 **X** ，其中 N 等于 3072。加权矩阵 **W** 的形状必须为 3 x 3072，因为数据集中有三个类。最后，偏置向量 **b** 的大小正好为 3×1。使用刚才描述的符号的最终得分公式为

![$$ f\left(\boldsymbol{X},\boldsymbol{W},\boldsymbol{b}\right)=\boldsymbol{W}\cdotp \boldsymbol{X}\kern0.5em +\boldsymbol{b} $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equd.png)

其中***W******X***为加权矩阵与输入数据向量的点积。

图 [4-1](#Fig1) 是评分函数的图示。

![../images/482214_1_En_4_Chapter/482214_1_En_4_Fig1_HTML.jpg](../images/482214_1_En_4_Chapter/482214_1_En_4_Fig1_HTML.jpg)

图 4-1

评分功能的图形表示

图中所有的数值都是编出来的，但是我确实把 cat 类做成了评分函数向量中最高的值，这样会使它成为预测的类标签。

有一种常见的简化“技巧”,通常用于将评分函数中的参数数量从三个减少到两个。这个技巧是将偏置向量包含到加权矩阵中。图 [4-2](#Fig2) 显示了这是如何完成的。

![../images/482214_1_En_4_Chapter/482214_1_En_4_Fig2_HTML.jpg](../images/482214_1_En_4_Chapter/482214_1_En_4_Fig2_HTML.jpg)

图 4-2

将偏置向量添加到加权矩阵

输入数据向量 ***X*** 已经被扩展了一个总是包含值 1 的元素。本例中的加权矩阵 ***W*** 也被扩展了一列，现在其形状为 3×3073。这个附加列就是偏置向量 ***b*** 。新的评分函数现在简化为单个点积乘法，如以下等式所示:

![$$ f\left(\boldsymbol{X},\boldsymbol{W}\right)=\boldsymbol{W}\cdotp \boldsymbol{X} $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Eque.png)

使用这个偏置技巧意味着你只需要一个加权矩阵，而不是一个加权矩阵和一个偏置向量。这个技巧只是帮助简化整个 ML 工作的预处理数据的一部分。

下面将尝试向您展示如何用 Python 实现线性分类器。这个脚本很明显被“操纵”来选择标签列表中的第一个类。这是必要的，因为在这个脚本中没有使用预先训练好的网络。我的目标只是展示如何编写一个简单的线性分类器。我不认为这是一次演示，我认为这更像是伪代码。同样，它只是供您参考，因为它确实使用了我已经讨论过的四个主要参数，并且它只缺少对预训练网络的调用。这个脚本被命名为 linear_Classifier.py。我没有把它放在这本书的配套网站上，因为我相信这只是你教育的一个教学工具。如果您愿意，可以随意从列表中复制它。

```py
# Import the required libraries
import numpy as np
import cv2

# Initialize class labels and set pseudo-random seed value
labels = ['dog', 'cat', 'squirrel']
np.random.seed(1)

# Randomly initialize the weighting matrix and bias vector
W = np.random.randn(3, 3072)
b = np.random.randn(3)

# Set the font used to draw the label
font = cv2.FONT_HERSHEY_SIMPLEX

# Load the image and resize it. The image is taken from the dataset.
orig = cv2.imread('dog.png')
image = cv2.resize(orig, (32,32)).flatten()

# Compute output scores
scores = W.dot(image) + b

# Loop over the scores and labels
for (label, score) in zip(labels, scores):
   print('[INFO] {}: {:.2f}'.format(label, score))

# Get the class label for the highest scoring class
classLabel = labels[np.argmax(scores)]

# Draw the predicted label on the original image
cv2.putText(orig, classLabel, (10,30),  font, 0.9, (255,0,0), 2)

# Display image
cv2.imshow('Image', orig)
cv2.waitKey(0)

```

通过输入以下命令运行该脚本:

```py
python linear_Classifier.py

```

图 [4-3](#Fig3) 显示了输入命令后的终端窗口结果。

![../images/482214_1_En_4_Chapter/482214_1_En_4_Fig3_HTML.jpg](../images/482214_1_En_4_Chapter/482214_1_En_4_Fig3_HTML.jpg)

图 4-3

运行 linear_Classifier 脚本后的终端结果

您应该能够看到 dog 类具有最大值，因此是预测的类标签。

图 [4-4](#Fig4) 显示了叠加了类别标签的原始图像。

![../images/482214_1_En_4_Chapter/482214_1_En_4_Fig4_HTML.jpg](../images/482214_1_En_4_Chapter/482214_1_En_4_Fig4_HTML.jpg)

图 4-4

linear_Classifier 脚本的已处理图像

从这个例子中得到的关键是要认识到，为了实际执行真正的分类任务，必须优化加权矩阵。我将在下一章用一个现实世界的问题来讨论这是如何实现的。我不太关心时间点上的偏差向量，因为它通常用于“调整”最终的网络解决方案。在担心偏差向量之前，创建一个工作加权矩阵要重要得多。

### 损失函数

损失函数是 ML 的核心。它允许您将您的算法从理论概念变为实际实现，并将神经网络从抽象矩阵乘法转换为 DL。

损失函数的概念非常简单。这是一种评估算法对输入数据集建模效果的方法。如果预测或分类有错误，那么损失函数将输出一个很高的数字。如果他们是合理的，那么这个数字将会很低。它还告知开发人员，随着网络和模型的训练，算法的改进程度如何。该函数将很容易显示训练努力是收敛还是发散。当你想避免发散时，收敛是好的。

#### 不同类型的损失函数

损失函数的一种简单方法可能简单到如下所示:

![$$ \epsilon = abs\left(\hat{y_i}-{y}_i\right) $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equf.png)

在哪里

*   ε = 错误

*   ![$$ \hat{y_i} $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_IEq1.png) =预测类标签

*   *y*<sub>T3】IT5】=实际类标签(地面真实)</sub>

在这个损失函数定义中，预测过高或过低都没有区别。重要的是它们有多不正确，也就是说，它们是方向不可知的。这种方法不一定适用于所有的损失函数。损失函数将基于 ML 问题域而显著变化。在一个给定的项目中，猜测过高或过低可能会更糟，所选的损失函数必须反映这种情况。

以下是流行的损失函数列表:

均方差(MSE)–这是基本损失函数的一个非常受欢迎的选项。它易于理解和实现，并且运行良好。MSE 的计算方法是:取预测值和实际值之间的差值，对其求平方，然后根据整个数据集的大小对总和求平均值。MSE 的 Python 代码如下所示:

```py
def MSE(y_Hat, y):
    sq_error = (y_Hat - y) ** 2
    sum_sq_error = np.sum(sq_error)
    mse = sum_sq_sq_error / y.size
    return mse

```

似然函数–似然函数也很简单，常用于分类问题。该函数获取每个输入数据类的预测概率并将它们相乘。尽管输出不是人类可以理解的，但它对于比较模型的表现非常有用。

考虑以下示例，其中模型输出一系列与不同数据集类相关联的概率。概率如表 [4-1](#Tab1) 所示。

表 4-1

示例分类模型的概率

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"> <col class="tcol4 align-left"></colgroup> 
| 

班级

 | 

地面实况(y <sub>i</sub>

 | 

概率

 | 

1 - p

 |
| --- | --- | --- | --- |
| A | Zero | Zero point four | Zero point six |
| B | one | Zero point six | 不适用的 |
| C | one | Zero point nine | 不适用的 |
| D | Zero | Zero point one | Zero point nine |

在模型输出错误或 0 地面真实值的情况下，1 - p 概率用于可能性计算，如下所示:

*   0.6’0.6’0.9’0.9 = 0.292

对数损失(交叉熵损失)-对数损失是一种损失函数，也经常用于分类问题。它是使用对数的似然函数的修改。

![$$ \epsilon =-\Big(y\ast \mathit{\log}(p)+\left(1-y\right)\ast \mathit{\log}\left(1-p\right) $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equg.png)

这是与似然函数相同的等式，但是使用对数。但是你要认识到，当类值为 1 时，函数的后半部分消失，当类为 0 时，前半部分消失。以这种方式，在乘法运算中仅使用基础真实类的预测概率的对数。

日志损失函数有一个有趣的特性，它对非常自信的*和非常错误的*处以重罚。预测错误类别的高概率会使函数“爆炸”图 [4-5](#Fig5) 说明了当真标签= 1 时会发生什么。你可以看到，当 label = 0 的预测概率接近 1 时，它飞速上升。**

 **![../images/482214_1_En_4_Chapter/482214_1_En_4_Fig5_HTML.jpg](../images/482214_1_En_4_Chapter/482214_1_En_4_Fig5_HTML.jpg)

图 4-5

对数损失函数图

损失函数讨论的最后一点是，它们不仅仅提供了一个模型如何运行的静态表示。大多数 ML 算法在优化过程中使用损失函数，或者为数据集确定最佳参数(权重)。

作为一个例子，考虑我在前一章中讨论的线性回归数据模型。在传统的“最小二乘”回归中，最佳拟合线是通过 MSE 确定的。对于模型尝试的每组权重，MSE 将在所有输入示例中进行计算。然后，该模型优化 MSE 函数，或者换句话说，通过使用优化算法(如梯度下降)使其尽可能最低。

正如不同的问题有不同类型的损失函数一样，也有不同的优化器来匹配特定的问题域，我将在下一节中对此进行讨论。

### 优化算法

ML 从业者经常说优化器算法是 ML 的核心。没有好的，ML 不可能存在。最常见的算法被称为梯度下降，它有两种风格，线性和随机。我将首先讨论线性版本，因为它最容易理解和解释。实际上，随机版本是现实世界中最常用的版本。我将在介绍完线性版本后解释这个版本。

为了给梯度下降的讨论做好准备，我想让你想象以下场景。假设你在山里徒步旅行，忘记了时间，天开始黑了。你很快意识到你必须在天变得太黑太冷之前到达地势较低的地方。假设你也忘了带手电筒或提灯，那么你必须依靠剩下的越来越少的阳光到达安全和较低的地面。很自然地，你开始向山下走去，但是由于光线不好，你看不到很远的前方。这种情况迫使你采取小步骤，以避免撞上巨石或落入洞中或地面凹陷处。从本质上说，你是在缓慢地下山，每走一步总是试图走得更低，每次你不小心开始上坡时就做些小的修正。

这个场景类似于梯度下降的工作原理。梯度下降的波峰和波谷是如何定义算法的直接结果。出于讨论的目的，考虑一个非常简单的网络，只有两个权重，没有偏差值。这种简单情况下的损失函数将完全取决于两个权重值。图 [4-6](#Fig6) 显示了损失函数与两个独立权重值的假想 3D 图。

![../images/482214_1_En_4_Chapter/482214_1_En_4_Fig6_HTML.jpg](../images/482214_1_En_4_Chapter/482214_1_En_4_Fig6_HTML.jpg)

图 4-6

两个加权变量的损失函数

该图中的峰值是要避免的最大损耗值，而谷值是最佳网络性能所需的最小值。我还要指出，图中有些山谷比其他山谷更深。总体上最深的谷被称为全局最小值，并且总是设置所有权重值的最期望的谷。“不深”的谷被称为局部最小值，虽然不是最优的，但通常可能“足够好”在稍后的讨论中会有更多的介绍。

计算图上任意一点的斜率是决定如何在波峰和波谷间横向移动或“行走”的关键。请记住，在介绍性场景中，您总是希望走下坡路，从而避开高峰，高峰是高成本区域。因此，不断计算瞬时斜率并向下进行是有意义的。在分析意义上，斜率是通过确定原始方程的导数来计算的。对于这种情况，原方程是损失函数，有两个自变量，分别代表两个权重。在这种情况下必须使用偏导数，因为涉及到两个独立变量。寻找全局最小值还涉及到所采取的步骤的大小。过大的步幅和最小步幅很容易被错过，过小的步幅会导致训练时间过长。你很快就会发现步长和学习速度是同义词。

此时应该进一步指出一点。之前显示的 3D 图仅涉及两个权重和一个损失函数。实际上，实际神经网络模型中涉及的权重远不止三个。在下一章，我将讨论一个权重为 100 的网络。对于如何概念化 100 个独立变量如何与一个单一的输出函数相互作用，这超出了我的理解范围，我怀疑其他许多人也是如此。当然，没有办法想象这种相互作用，因为我只用了两个变量。我建议您简单地接受这样一个假设，即以处理 2 个变量的相同方式优化 100 个变量在数学上是有意义的。

考虑垂直平面与图 [4-6](#Fig6) 所示曲线相交的情况，并产生显示损失函数与独立变量范围的 2D 曲线。这个范围对于这个讨论来说并不重要。图 [4-7](#Fig7) 显示了具有全局和局部最小值的典型 2D 图。

![../images/482214_1_En_4_Chapter/482214_1_En_4_Fig7_HTML.jpg](../images/482214_1_En_4_Chapter/482214_1_En_4_Fig7_HTML.jpg)

图 4-7

显示全局和局部最小值的 2D 图

#### 深入研究梯度下降算法

对梯度下降算法的深入研究从回顾我在第 [2](2.html) 章中讨论的线性回归(LR)数据模型开始。我建议您回顾一下该讨论，以更新关于该模型的关键点。广义 LR 方程为

![$$ y=\kern0.75em m\ast x+b $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equh.png)

在哪里

*   *m* =直线的斜率

*   b = y 轴截距

请注意，我对第 [2](2.html) 章中所示的等式进行了轻微修改，删除了误差估计项，并将斜率常数改为 *m* ，截距常数改为 *b* 。这样做是为了帮助符合本讨论中使用的数字。

我将从一些数据的 x-y 散点图开始，使用如图 [4-8](#Fig8) 所示的扁平线 LR 方程。

![../images/482214_1_En_4_Chapter/482214_1_En_4_Fig8_HTML.jpg](../images/482214_1_En_4_Chapter/482214_1_En_4_Fig8_HTML.jpg)

图 4-8

带扁平线 LR 预测值的数据散点图

最佳 LR 预测线在这样的位置穿过数据点，以最小化如果单独使用预测线来为任何给定的 *x* <sub>*i*</sub> 数据点计算 *y* <sub>*i*</sub> 时会导致的总误差。确定最佳 *m* 和 *b* 值的一般程序是迭代所有这些值的数据集，并确定使用所有 *x* <sub>*i*</sub> 值产生的最小误差。下面列出了实现这一通用过程的一些示例 Python 代码:

```py
# Use y = mx + b equation
# m is slope, b is y-intercept
def computeErrorForLineGivenPoints(b, m, points):
    totalError = 0
    for i in range(0, len(points)):
        totalError += (points[i].y - (m * points[i].x + b)) ** 2
    return totalError / float(len(points))

```

在调用这个脚本之前，需要有一个名为`points`的数据数组设置，包含所有的原始 x-y 点。此外，主调用脚本必须为要测试的 *m* 和 *b* 变量设置范围。

此 LR 示例的形式误差函数如下所示:

![$$ {e}_{m,b}=\frac{1}{N}\sum \limits_{i=1}^N{\left({y}_i-\left(m\ast {x}_i+b\right)\right)}^2 $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equi.png)

现在的重点是开发一个等式，根据 *m* 和 *b* 优化该误差函数，这将产生最小误差。在讨论这个问题之前，说明这个 LR 例子中变量相互作用的性质是有帮助的。图 [4-9](#Fig9) 显示了不同的 m*和 b*值如何影响误差函数的两种观点。

![../images/482214_1_En_4_Chapter/482214_1_En_4_Fig9_HTML.jpg](../images/482214_1_En_4_Chapter/482214_1_En_4_Fig9_HTML.jpg)

图 4-9

误差图与 m 和 b 变量的关系

你应该能够很容易地想象从上坡滚下一个弹球，让它停在某个最小点。该最小值将具有与最小值 *e* <sub>*m，b*</sub> 相关联的 *m* 和 *b* 值。使用梯度下降搜索方法相当于将弹球滚下斜坡。

实施梯度下降法的第一步是对误差函数进行两次偏微分，因为有两个独立变量。这一步与我前面讨论的两个权重示例中的步骤完全相同。偏微分方程实际上比原始误差方程更简单:

![$$ \frac{\partial }{\partial m}=\frac{2}{N}\sum \limits_{i=N}^N-{x}_i\ast \left({y}_i-\left(m\ast {x}_i+b\right)\right) $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equj.png)

![$$ \frac{\partial }{\partial b}=\frac{2}{N}\sum \limits_{i=N}^N-\left({y}_i-\left(m\ast {x}_i+b\right)\right) $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equk.png)

搜索通常从原点开始，即*m*=–1 和 *b* = 0。值为-1 时，弹球开始向山下滚动。梯度下降算法是迭代的，这意味着迈出一小步，然后重新评估误差函数，如果有可能进一步改进，则再迈出一步。以下 Python 代码为 LR 示例实现了梯度下降算法:

```py
def stepGradient(b_current, m_current, points, learningRate):
    b_gradient = 0
    m_gradient = 0
    N = float(len(points))
    for i in range(0, len(points)):
        b_gradient += -(2/N) * (points[i].y - ((m_current*points[i].x) + b_current))
        m_gradient += -(2/N) * points[i].x * (points[i].y - ((m_current*points[i].x) + b_current))
        new_b = b_current - (learningRate * b_gradient)
        new_m = m_current - (learningRate * m_gradient)
    return [new_b, new_m]

```

前面脚本中的 learningRate 参数控制步长。此参数必须小心调整，因为太大的值很容易错过最小值，而太小的值会不必要地增加定位最小值之前的迭代次数。

接下来的一系列图将说明梯度下降算法如何收敛到这个 LR 例子的最优解。在每个图中，左边的图显示梯度下降开始的位置，右边的图显示当前 *m* 和 *b* 变量的数据和预测线。

图 [4-10](#Fig10) 显示了梯度搜索的开始。

![../images/482214_1_En_4_Chapter/482214_1_En_4_Fig10_HTML.jpg](../images/482214_1_En_4_Chapter/482214_1_En_4_Fig10_HTML.jpg)

图 4-10

梯度搜索开始

从这个数字中可以清楚地看出，最初的估计相差甚远。

下一次迭代如图 [4-11](#Fig11) 所示。

![../images/482214_1_En_4_Chapter/482214_1_En_4_Fig11_HTML.jpg](../images/482214_1_En_4_Chapter/482214_1_En_4_Fig11_HTML.jpg)

图 4-11

梯度搜索的第二次迭代

请注意，图中左侧的图现在有一条线，指示搜索算法从初始起点开始所采用的路径。图中右侧显示的预测线是对初始图的一大改进。然而，它显然需要改进，因为它无法截取任何数据点。

图 [4-12](#Fig12) 显示了第三次迭代，尽管右边的图显示了绘图区中的迭代 2。这是因为第一次尝试被标记为 0。

![../images/482214_1_En_4_Chapter/482214_1_En_4_Fig12_HTML.jpg](../images/482214_1_En_4_Chapter/482214_1_En_4_Fig12_HTML.jpg)

图 4-12

梯度搜索的第三次迭代

搜索算法采用的路径显然与第二次迭代的路径相同。然而，预测线现在只能勉强截取一些数据点，但显然需要进一步改进。

跳到迭代 100(由于基于 0 的计数，实际上是 101)，您可以从图 [4-13](#Fig13) 中看到，预测线在视觉上“看起来”很适合。

![../images/482214_1_En_4_Chapter/482214_1_En_4_Fig13_HTML.jpg](../images/482214_1_En_4_Chapter/482214_1_En_4_Fig13_HTML.jpg)

图 4-13

梯度搜索 101 次迭代

左边的图显示了梯度搜索路径在搜索全局最小值时向右移动了一小段。

图 [4-14](#Fig14) 是误差与迭代次数的关系图。

![../images/482214_1_En_4_Chapter/482214_1_En_4_Fig14_HTML.jpg](../images/482214_1_En_4_Chapter/482214_1_En_4_Fig14_HTML.jpg)

图 4-14

误差与迭代次数的关系图

从这个图中可以明显看出，将迭代次数扩展到 100 次以上并没有什么改进。完全有可能没有达到全球最低标准；然而，任何边际损失的改善都是微不足道的。这种情况就是我前面讨论的“足够好”的意思。

对于感兴趣的读者，最终最佳拟合 LR 方程最终确定为

![$$ y=1.3\ast x+0.61 $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equl.png)

## 人工神经网络

人工神经网络(ANN)在人工智能领域有着相对较长的历史。关于人工神经网络的开创性论文被认为是沃伦麦卡洛克和沃尔特皮茨在 1943 年发表的题为“神经活动中固有的思想的逻辑演算”的论文，其中他们基于数学和他们称为阈值逻辑的算法假设了神经网络的计算模型。这个模型为将来神经网络研究分成两种方法铺平了道路。一种方法专注于大脑中的生物过程，而另一种方法专注于神经网络在人工智能中的应用。

人工神经网络的一个核心概念是神经元模型，其目的是在某种程度上模拟人脑神经元。我认为在讨论人工神经元之前，先讨论人类大脑神经元是很重要的。通过这种方式，你应该能够理解为什么人工神经元会以今天的方式被创造出来。

图 [4-15](#Fig15) 描绘了人类大脑神经元的生物图。

![../images/482214_1_En_4_Chapter/482214_1_En_4_Fig15_HTML.jpg](../images/482214_1_En_4_Chapter/482214_1_En_4_Fig15_HTML.jpg)

图 4-15

人脑神经元图

图中所示的人类神经元的一些部分描述如下:

*   树突-树突允许细胞体接收来自大量(> 1000)相邻神经元的信号。每个树突能够执行电信号与该树突“权重值”的“相乘”这种倍增是通过增加或减少突触神经递质与响应突触神经递质而引入树突的信号化学物质的比率来实现的。负倍增效应可以通过响应于突触神经递质的接收而沿着树突体传递信号抑制剂(即，带相反电荷的离子)来实现。

*   soma–soma 作为一个求和函数。当正负信号(分别为兴奋和抑制)从树突到达细胞体时，正负离子通过在细胞体内部的溶液中混合在一起而有效地相加。

*   轴突——轴突从发生在胞体内部的总和行为中获得信号。轴突的开口实质上是对胞体内部溶液的电位进行采样。一旦胞体达到一定的电位，轴突就会沿着它的长度传输一个全进信号脉冲。通过这种方式，轴突与其他神经元直接沟通。

生物神经元以离散脉冲的形式放电。每当胞体内部的电位达到预设的阈值时，就会有一个脉冲沿着轴突传递下去。这种脉动可以转化为连续的值。速率(每秒激活次数等。)直接转化为相邻神经元获得引入其中的信号离子的速率。生物神经元放电越快，附近的神经元积累电势就越快(或失去电势，取决于连接到放电神经元的树突的“权重”)。正是这种转换允许人工智能研究人员使用人工神经元模拟生物神经网络，这些人工神经元可以输出不同的值，通常在-1 到 1 的范围内。

早期人工智能研究人员开发了一个相对简单的大脑神经元模型，部分基于之前提出的生物学事实。图 [4-16](#Fig16) 显示了具有 *N* 个输入和一个输出的人工神经元图。

![../images/482214_1_En_4_Chapter/482214_1_En_4_Fig16_HTML.jpg](../images/482214_1_En_4_Chapter/482214_1_En_4_Fig16_HTML.jpg)

图 4-16

人工神经元图

具有权重的输入分支相当于树突。圆圈是一个求和节点，相当于躯体中发生的情况。标有激活功能的方框相当于轴突，当加权电信号的总和超过某个阈值时，轴突就会激活。我相信这个模型非常简单，但似乎真实地捕捉到了真实大脑神经元中发生的事情。

人工神经元的数学表示简明地表示为

![$$ g\left({x}_1,{x}_2,{x}_3,\cdots {x}_n\right)=g(X)=\sum \limits_{i=1}^n{x}_i $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equm.png)

![$$ y=f\left(g(X)\right)=1\kern0.5em if\ g(X)\ge \theta $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equn.png)

![$$ =0\kern0.5em if\ g(X)&lt;\theta $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equo.png)

1 和 0 输出仅代表加权输入总和超过某个阈值 *θ* 时的状态。实际输出值取决于由激活函数转换的实际求和值。

人工神经元模型经常使用的激活函数是 sigmoid，我在第 [2-16](2.html#Fig16) 章给你介绍过。请参考前面的讨论，复习和回顾图 [2](2.html) ，了解该功能如何转换求和信号。通过研究该图，您应该很容易认识到，对于–8 至+8 范围内的大多数求和信号，最终输出信号将在 0 至 1.0 范围内。

### 人工神经网络是如何训练和运作的

是时候探索人工神经元或人工神经网络了。

图 [4-17](#Fig17) 显示了一个通用的三层人工神经网络。

![../images/482214_1_En_4_Chapter/482214_1_En_4_Fig17_HTML.jpg](../images/482214_1_En_4_Chapter/482214_1_En_4_Fig17_HTML.jpg)

图 4-17

三层神经网络

构成人工神经网络的三层是

*   输入–原始数据输入应用于该层。这些数据输入没有加权。人工神经网络中只有一个输入层。

*   隐藏-这是指既不是输入层也不是输出层的任何层。可以有一个到多个隐藏层。权重通常与隐藏层节点之间的互连以及通向输出层的最后一个隐藏层之间的连接相关联。

*   输出-处理后的信号显示为该层的输出。输出节点的数量通常等于分类神经网络的类的数量。

人工神经网络通常分为两类:

*   前馈

*   反馈

图 [4-18](#Fig18) 是显示信号如何在每个人工神经网络中流动的图表。

![../images/482214_1_En_4_Chapter/482214_1_En_4_Fig18_HTML.jpg](../images/482214_1_En_4_Chapter/482214_1_En_4_Fig18_HTML.jpg)

图 4-18

前馈和反馈神经网络模型

每一类人工神经网络都有其优劣势。最终使用的人工神经网络模型取决于数据集的性质和人工神经网络的预期目的。然而，反馈模型是训练人工神经网络时经常使用的模型。这是因为训练的主要目的是确定用于隐藏层节点的权重值。确定这些值是人工神经网络“学习”的关键方法人工神经网络学习首先将一个大型数据集输入网络，一次输入一个记录或元素。该输入数据最终产生输出数据，然后与地面真实数据进行比较。然后，在反馈配置中使用任何结果误差来调整权重值，以便减少和最小化误差。为了完整地训练网络，这种单独的记录训练循环通常要重复数千次。术语“历元”用于描述人工神经网络一次使用整个数据集的过程。在训练会话中使用多个时期并不罕见，其中训练数据集被稍微重新安排用于下一个时期，以便与仅进行单遍相比获得更好的学习结果。整个训练过程称为*反向传播*。

图 [4-19](#Fig19) 显示了一个三层人工神经网络，其权重标注在节点互连上。

![../images/482214_1_En_4_Chapter/482214_1_En_4_Fig19_HTML.jpg](../images/482214_1_En_4_Chapter/482214_1_En_4_Fig19_HTML.jpg)

图 4-19

带权值的三层人工神经网络

权重的注释显示为*w*<sub>T3】I，jT5】其中 *i* 是源节点而 *j* 是目的节点。并不是图中显示的所有节点都有互连，因为我不想让图太“忙”。实际上，当学习开始时，所有节点都将“连接”到其他节点。最终，一些重量最终会在不使用它们的地方减少价值，这意味着节点连接实际上将不复存在。</sub>

#### 实际人工神经网络示例

我相信展示一个完全“成功”的人工神经网络例子将是一个有用的学习练习。图 [4-20](#Fig20) 显示了一个高度简化的两层人工神经网络，我将在这个例子中使用。

![../images/482214_1_En_4_Chapter/482214_1_En_4_Fig20_HTML.jpg](../images/482214_1_En_4_Chapter/482214_1_En_4_Fig20_HTML.jpg)

图 4-20

双层人工神经网络

在这个例子中不需要隐藏层，因为我只关注反向传播过程的演示。用于开始本例的初始数据输入和权重如表 [4-2](#Tab2) 所示。

表 4-2

初始输入和权重值

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"></colgroup> 
| 

标志

 | 

价值

 |
| --- | --- |
| in1 | Zero point eight |
| in2 | Zero point four |
| w <sub>1，1</sub> | Zero point eight |
| w <sub>1，2</sub> | Zero point one |
| w <sub>2，1</sub> | Zero point nine |
| w <sub>2，2</sub> | Zero point four |

这些值是随机的，并不反映任何现实世界的问题域。我将在后面的部分描述如何使用随机数生成器来预设一整套权重。需要注意的另一点是，我将使用 sigmoid 函数来转换求和值。sigmoid 方程是

![$$ y=\frac{1}{\left(1+{e}^{-x}\right)} $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equp.png)

在哪里

*   e = 2.71828...(欧拉数)

代入前面的等式 *x* = 1 将得到 *y* = 0.731。

计算出 1 的*需要求解以下方程:*

![$$ sum\ of\ weighted\ inputs=x={w}_{1,1}\ast in1+{w}_{2,1}\ast in2 $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equq.png)

![$$ out1=\frac{1}{\left(1+{e}^{-x}\right)} $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equr.png)

代入表 [4-2](#Tab2) 中的值，得出

![$$ x=0.8\ast 0.8+0.9\ast 0.4=1.0 $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equs.png)

![$$ out1=\frac{1}{\left(1+{e}^{-1}\right)}=0.731 $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equt.png)

以类似的方式，计算出 2 的*需要解这些方程:*

![$$ sum\ of\ weighted\ inputs=x={w}_{1,2}\ast in1+{w}_{2,2}\ast in2 $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equu.png)

![$$ out2=\frac{1}{\left(1+{e}^{-x}\right)} $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equv.png)

代入表 [4-2](#Tab2) 中的值，得出

![$$ x=0.1\ast 0.8+0.4\ast 0.4=0.24 $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equw.png)

![$$ out2=\frac{1}{\left(1+{e}^{-0.24}\right)}=0.560 $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equx.png)

这两个人工神经网络的输出已经通过大量的手工计算确定。现在很明显，试图手动计算更大更复杂的人工神经网络的输出是不现实的。矩阵和矩阵运算将从现在开始使用，因为我已经演示了使用手工计算是多么的乏味。

这个简单示例的输入数据可以表示为一个向量:

![$$ \left\{\begin{array}{c} in1\\ {} in2\end{array}\right\} $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equy.png)

同样，加权矩阵可以表示为 2×2 矩阵:

![$$ \left\{\begin{array}{c}{w}_{1,1}\ {w}_{1,2}\\ {}{w}_{2,1}\ {w}_{2,2}\end{array}\right\} $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equz.png)

图 [4-21](#Fig21) 显示了在交互式 Python 会话中使用矩阵运算执行的相同手动计算。

![../images/482214_1_En_4_Chapter/482214_1_En_4_Fig21_HTML.jpg](../images/482214_1_En_4_Chapter/482214_1_En_4_Fig21_HTML.jpg)

图 4-21

交互式 Python 会话

您可以很容易地看到 Python 会话结果与手动计算相匹配。

#### 复杂人工神经网络示例

图 [4-22](#Fig22) 显示了我使用 Python 脚本处理的一个更复杂的 ANN 例子。

![../images/482214_1_En_4_Chapter/482214_1_En_4_Fig22_HTML.jpg](../images/482214_1_En_4_Chapter/482214_1_En_4_Fig22_HTML.jpg)

图 4-22

复杂人工神经网络示例

输入数据向量不代表任何有意义的问题域。它只是一组随机数，因为本演示的目的是展示用于复杂人工神经网络的计算过程。

矢量格式的输入数据集为

![$$ input=\left\{\begin{array}{c}0.8\\ {}0.2\\ {}0.7\end{array}\right\} $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equaa.png)

输入层和隐藏层之间的加权矩阵( *wtg* <sub>*ih*</sub> )为

![$$ {wtg}_{ih}=\left\{\begin{array}{c}{w}_{1,1}\ {w}_{1,2}\ {w}_{1,3}\\ {}{w}_{2,1}\ {w}_{2,2}\ {w}_{2,3}\\ {}{w}_{3,1}\ {w}_{3,2}\ {w}_{3,3}\end{array}\right\} $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_IEq2.png) = ![$$ \left\{\begin{array}{c}0.8\ 0.6\ 0.3\\ {}0.2\ 0.9\ 0.3\\ {}0.2\ 0.5\ 0.8\end{array}\right\} $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_IEq3.png)

隐含层和输出层之间的加权矩阵( *wtg* <sub>*ho*</sub> )为

![$$ {wtg}_{ho}=\left\{\begin{array}{c}{w}_{1,1}\ {w}_{1,2}\ {w}_{1,3}\\ {}{w}_{2,1}\ {w}_{2,2}\ {w}_{2,3}\\ {}{w}_{3,1}\ {w}_{3,2}\ {w}_{3,3}\end{array}\right\} $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_IEq4.png) = ![$$ \left\{\begin{array}{c}0.4\ 0.8\ 0.4\\ {}0.5\ 0.7\ 0.2\\ {}0.9\ 0.1\ 0.6\end{array}\right\} $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_IEq5.png)

这些矩阵被赋予 0 到 1.0 范围内的随机数元素。

用于处理这个人工神经网络的 Python 脚本被命名为 annDemo1.py，可以从该书的配套网站上获得。剧本被很好地评论了，虽然我确实在列表之后增加了一点评论。

```py
# Import required libraries
import numpy as np

# Create the input data vector
input = np.array([0.8, 0.2, 0.7])[:,None]

# Create the wtgih matrix
wtgih = np.matrix([[0.8, 0.6, 0.3], \
                   [0.2, 0.9, 0.3], \
                   [0.2, 0.5, 0.8]])

# Create the wtgho matrix
wtgho = np.matrix([[0.4, 0.8, 0.4], \
                   [0.5, 0.7, 0.2], \
                   [0.9, 0.1, 0.6]])

# Compute the dot product of the input vector and wtgih matrix
X1 = np.dot(input.T, wtgih)
# Display the matrix
print('X1 matrix\n', X1)
print()

# Apply the activation function to the X1 matrix
out1 = 1 / (1 + np.exp(-X1))
# Display the matrix
print('out1 matrix\n', out1)
print()

# Compute the dot product of the X1 and wtgho matrices
X2 = np.dot(out1, wtgho)
# Display the matrix
print('X2 matrix\n', X2)
print()

# Apply the activation function to the X2 matrix
out2 = 1 / (1 + np.exp(-X2))
# Display the matrix
print('out2 matrix\n', out2)

```

这个脚本利用 numpy 点积函数来完成矩阵乘法。还要注意使用 numpy `exp`函数来应用激活函数是多么容易。

通过输入以下命令运行该脚本:

```py
python annDemo1.py

```

图 [4-23](#Fig23) 显示了运行脚本后的结果。

![../images/482214_1_En_4_Chapter/482214_1_En_4_Fig23_HTML.jpg](../images/482214_1_En_4_Chapter/482214_1_En_4_Fig23_HTML.jpg)

图 4-23

运行 annDemo1 脚本的结果

最终输出向量以及所有中间矩阵和向量都会显示出来。顺便说一下，当我混淆矩阵和向量这两个术语时，请不要混淆。我通常把单行数据称为向量，尽管从技术上讲，你可以把它称为 1D 矩阵。那个标签在我看来有点太迂腐了。

这个 ANN 的最终输出数据没有意义，因为输入数据没有意义。然而，最终的输出应该在一定程度上反映输入值。表 [4-3](#Tab3) 比较输入和输出值以及它们之间的误差。

表 4-3

输入和输出值之间的比较

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"></colgroup> 
| 

投入

 | 

输出

 | 

错误

 |
| --- | --- | --- |
| Zero point eight | 0.78187033 | 0.01812967 |
| Zero point two | 0.75745360 | –0.55745360 |
| Zero point seven | 0.69970531 | 0.00029469 |

除了表中的中间值之外，结果都很接近。这表明必须修改初始权重以减小误差。但是这是怎么做到的呢？答案见下一节。

#### 修改权重值

考虑如图 [4-24](#Fig24) 所示连接三个节点的情况。

![../images/482214_1_En_4_Chapter/482214_1_En_4_Fig24_HTML.jpg](../images/482214_1_En_4_Chapter/482214_1_En_4_Fig24_HTML.jpg)

图 4-24

单一错误分配设置

求和节点中存在一些误差，必须通过调整输入节点 1 和 2 与输出节点 1 之间的权重来校正。一种简单的方法可能是在节点之间平均分配误差。然而，这不能准确地表示每个输入节点的误差贡献，因为节点 1 的权重是节点 2 的两倍。正确的解决方案是将误差与连接节点的权重成正比。在这种情况下，节点 1 应该承担三分之二的误差，节点 2 承担三分之一的误差。

以这种方式使用加权矩阵是一个额外的特征，当第一次遇到 ANN 时，它不会立即显现出来。通常，信号是在前馈配置中传播的，正如我前面提到的。这种修改方法使用具有误差值的权重，该误差值沿反向传播。这就是为什么误差确定被称为反向传播。

现在考虑两个输出节点出现多个错误的情况，如图 [4-25](#Fig25) 所示。

![../images/482214_1_En_4_Chapter/482214_1_En_4_Fig25_HTML.jpg](../images/482214_1_En_4_Chapter/482214_1_En_4_Fig25_HTML.jpg)

图 4-25

多重错误分配设置

多个节点的权重修改过程与单个节点相同。这是因为输出节点彼此独立。输出节点之间没有直接连接。分配给每个互连的误差量是仅基于连接到输出节点的每条线上的权重值的分数。在图 [4-25](#Fig25) 的情况下，应用于误差 e <sub>1</sub> 的 w <sub>1，1</sub> 和 w <sub>2，1</sub> 的分数为

![$$ \frac{w_{1,1}}{\left({w}_{1,1}+{w}_{2,1}\right)}\kern0.5em \mathrm{and}\ \frac{w_{2,1}}{\left({w}_{1,1}+{w}_{2,1}\right)} $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equab.png)

类似地，e <sub>2</sub> 的误差为

![$$ \frac{w_{1,2}}{\left({w}_{1,2}+{w}_{2,2}\right)}\kern0.5em \mathrm{and}\ \frac{w_{2,2}}{\left({w}_{1,2}+{w}_{2,2}\right)} $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equac.png)

到目前为止，基于输出误差修改权重的过程很简单。因为训练数据容易获得，所以很容易确定误差。当训练两层 ANN 时，没有其他要求。但是，当隐藏层中肯定有错误，但没有可用的训练数据时，如何处理三层 ANN 呢？

图 [4-26](#Fig26) 显示了一个三层六节点人工神经网络，每层有两个节点。

![../images/482214_1_En_4_Chapter/482214_1_En_4_Fig26_HTML.jpg](../images/482214_1_En_4_Chapter/482214_1_En_4_Fig26_HTML.jpg)

图 4-26

三层六节点人工神经网络

这种人工神经网络已经被简化，以帮助您专注于所需的少量反向传播计算。输出错误是随机产生的，因为下面的计算需要它们。使用图中所示的权重计算出以下单个误差贡献:

对于 w <sub>1，1</sub> 线:

![$$ {e}_{1 node1}\ast \frac{w_{1,1}}{\left({w}_{1,1}+{w}_{2,1}\right)}=0.96\ast \frac{2}{\left(2+3\right)}=0.96\ast 0.4=0.38 $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equad.png)

对于 w <sub>2，1</sub> 线:

![$$ {e}_{1 node2}\ast \frac{w_{2,1}}{\left({w}_{1,1}+{w}_{2,1}\right)}=0.96\ast \frac{3}{\left(2+3\right)}=0.96\ast 0.6=0.58 $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equae.png)

对于 w <sub>1，2</sub> 线:

![$$ {e}_{2 node1}\ast \frac{w_{1,2}}{\left({w}_{1,2}+{w}_{2,2}\right)}=0.8\ast \frac{2}{\left(2+1\right)}=0.8\ast 0.66=0.53 $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equaf.png)

对于 w <sub>2，2</sub> 线:

![$$ {e}_{2 node2}\ast \frac{w_{2,2}}{\left({w}_{1,2}+{w}_{2,2}\right)}=0.8\ast \frac{1}{\left(2+1\right)}=0.8\ast 0.33=0.27 $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equag.png)

每个隐藏节点的总归一化误差值是该节点的单个误差贡献的总和，计算如下:

![$$ {e}_1={e}_{1 node1}\kern0.5em +{e}_{2 node1}=0.38+0.53=0.91 $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equah.png)

![$$ {e}_2={e}_{1 node2}\kern0.5em +{e}_{2 node2}=0.58+0.27=0.85 $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equai.png)

这些值显示在图 [4-26](#Fig26) 中每个隐藏节点的旁边。

该误差计算过程可以继续进行，以包含所有剩余的隐藏层节点的所有误差，不仅针对如该示例所示的单个隐藏层，而且针对 ANN 中的多个隐藏层。但是，不需要计算输入层的误差值，因为所有输入层节点的误差都必须为 0。它们只是不加修改地传递输入数据向量值。

你应该能够感觉到计算隐藏层误差值是一个单调乏味的过程，并且可以像前馈计算那样自动完成。如果误差计算是从我刚才演示的手动过程直接转换过来的，那么下面的矩阵符号将适用:

![$$ {e}_{hidden}=\left\{\begin{array}{c}\frac{w_{1,1}}{\left({w}_{1,1}+{w}_{2,1}\right)}\kern0.5em \frac{w_{1,2}}{\left({w}_{1,1}+{w}_{2,1}\right)}\\ {}\frac{w_{2,1}}{\left({w}_{2,1}+{w}_{2,2}\right)}\kern0.5em \frac{w_{2,2}}{\left({w}_{2,1}+{w}_{2,2}\right)}\end{array}\right\}\ast \left\{\begin{array}{c} in1\\ {} in2\end{array}\right\} $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equaj.png)

不幸的是，没有简单的方法来输入矩阵中显示的分数。然而，只需考虑分数仅归一化误差贡献，这意味着贡献值的范围仅为 0 到 1.0。通过丢弃分母，仍然可以保持相对误差贡献。移除分母会产生

![$$ {e}_{hidden}=\left\{\begin{array}{c}{w}_{1,1}\kern0.5em {w}_{1,2}\\ {}{w}_{2,1}\kern0.5em {w}_{2,2}\end{array}\right\}\ast \left\{\begin{array}{c} in1\\ {} in2\end{array}\right\} $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equak.png)

这个矩阵公式与前面演示的公式相同，可以很容易地用 numpy 函数来处理。

至此，我只讨论了如何确定单个误差贡献。现在是讨论一旦确定了误差贡献如何修改权重的时候了。

首先，我将向您展示一个相当复杂的方程，它计算三层九节点人工神经网络的给定输出节点的输出

![$$ {O}_k=\frac{1}{-\sum \limits_{j=1}^3\left({w}_{j,k}\ast \frac{1}{\sum \limits_{j=1}^3\left({w}_{j,k}\ast {x}_i\right)}\right)} $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equal.png)

在哪里

![$$ {O}_k= output\ at\ kth\ node $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equam.png)

![$$ {w}_{j,k}= interconnected\ weights $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equan.png)

![$$ {x}_i= input\ data $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equao.png)

这是一个令人生畏的等式，尽管它只适用于一个简单的三层九节点人工神经网络。想象一下适用于六输入、五层人工神经网络的等式。拥有更大的人工神经网络是非常常见的，因此试图解析求解人工神经网络方程是完全不切实际的，也超出了人类的理解能力。在排除了分析方法之后，您可能会尝试“暴力”方法。

考虑使用一台速度极快的计算机，对每种重量尝试一系列不同的数值。让我们假设权重范围是–1 到+1，这对于实际的人工神经网络来说是完全可能的。进一步假设增量大小是 0.001，这也是一个合理的假设。这意味着对于一个三层、九节点的 ANN，将有 18 个权重需要测试，每个连接 2000 个测试，总共 36，000 个增量测试。假设做一个测试需要 1 秒钟，那么总共需要 36，000 秒或大约 10 小时的计算时间。十个小时很长，但你可以去睡觉，电脑会在早上完成。但是现在考虑一个实际的 900 节点 ANN，我计划在下一章演示。这将需要近十亿次测试，大约需要 32 年才能完成。我不知道你怎么想，但是等待一代人的时间来完成一个计算似乎有点太多了。蛮力方法的实际替代方法是使用我在本章前面介绍的梯度下降算法。

图 [4-27](#Fig27) 将作为我用来解释如何将梯度下降应用于人工神经网络的网络。

![../images/482214_1_En_4_Chapter/482214_1_En_4_Fig27_HTML.jpg](../images/482214_1_En_4_Chapter/482214_1_En_4_Fig27_HTML.jpg)

图 4-27

三层六节点人工神经网络

除了图中所示的符号外，还需要一个额外的符号 *e* <sub>*k*</sub> 来表示输出节点误差。

输出节点误差由下式表示

![$$ {e}_k={t}_k-{o}_k $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equap.png)

在哪里

![$$ {t}_k= ground\ truth\ value $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equaq.png)

![$$ {o}_k= output\ resulting\ from\ {x}_i\ input $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equar.png)

总误差是每个节点的误差值之和。由此得出的等式是

![$$ {e}_k=\sum \limits_{i=1}^N{\left({t}_k-{o}_k\right)}^2 $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equas.png)

在哪里

![$$ N= total\ number\ of\ nodes\ in\ ANN $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equat.png)

误差项也被平方，以确保负误差不会抵消正误差，正如我在梯度下降讨论中提到的那样。

这个误差函数等价于损失函数。这意味着它必须相对于 w <sub>j，k</sub> 求导，以创建用于优化权重的等式。导数形式是

![$$ \frac{\partial e}{\partial {w}_{j,k}}=\frac{\partial }{\partial {w}_{j,k}}\sum \limits_{i=1}^N{\left({t}_k-{o}_k\right)}^2 $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equau.png)

只要注意到任何给定节点的误差仅由其输入连接引起，这个等式就可以大大简化。这意味着第 *k* <sub>*第*</sub> 节点仅依赖于其输入连接上的 *w* <sub>*j，k*</sub> 权重。认识到这一事实允许您从误差函数中移除求和，因为没有其他节点对第 *k* <sub>*个*</sub> 节点的输出有贡献。这种简化导致更简单的误差函数:

![$$ \frac{\partial e}{\partial {w}_{j,k}}=\frac{\partial }{\partial {w}_{j,k}}{\left({t}_k-{o}_k\right)}^2 $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equav.png)

进行微分并应用激活函数后的最终方程为

![$$ \frac{\partial e}{\partial {w}_{j,k}}=-\left({t}_k-{o}_k\right)\ast sigmoid\left(\sum \limits_j{w}_{j,k}\ast {o}_j\right)\ast \left(1- sigmoid\left(\sum \limits_j{w}_{j,k}\ast {o}_j\right)\right)\ast {o}_j $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equaw.png)

前面的等式虽然看起来很复杂，但如果有物理解释的话，其实很容易理解。第一部分，(*t*<sub>*k*</sub>—*o*<sub>*k*</sub>)，就是错误。sigmoid 函数内的求和是第 *k* <sub>*个*</sub> 最终层节点的输入。最后一项， *o* <sub>*j*</sub> ，是隐藏层中第*j*<sub>*th*</sub>节点的输出。

隐藏层梯度下降算法的公式类似于前面所示的公式。这是

![$$ \frac{\partial e}{\partial {w}_{i,j}}=-\left({e}_j\ \right)\ast sigmoid\left(\sum \limits_j{w}_{i,j}\ast {o}_i\right)\ast \left(1- sigmoid\left(\sum \limits_j{w}_{i,j}\ast {o}_i\right)\right)\ast {o}_i $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equax.png)

要显示的唯一剩余等式是显示如何在给定旧权重和梯度下降算法的结果的情况下计算新权重的等式。这个等式是

![$$ new\ {w}_{j,k}= old\ {w}_{j,k}-\alpha \ast \frac{\partial e}{\partial {w}_{j,k}} $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equay.png)

在哪里

![$$ \alpha = learning\ rate $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equaz.png)

您应该能够看到学习率参数对人工神经网络在梯度下降过程中的表现有很大的影响。

用矩阵符号表示所有前面的方程在计算上是有效的。以下函数计算将隐藏层节点连接到输出节点的一个链接的梯度下降值:

![$$ g\left({w}_{j,k}\right)=\alpha \ast {e}_k\ast sigmoid\left({o}_k\right)\ast \left(1- sigmoid\left({o}_k\right)\right)\ast {o}_j^T $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equba.png)

在哪里

![$$ {o}_j^T= transpose\ of\ the\ hidden\ layer\ matrix $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equbb.png)

以下是三层六节点人工神经网络示例的矩阵:

![$$ \left\{\begin{array}{c}g\left({w}_{1,1}\right)\ g\left({w}_{2,1}\right)\ g\left(\ {w}_{3,1}\right)\\ {}g\left({w}_{1,2}\right)\ g\left({w}_{2,2}\right)\ g\left({w}_{3,2}\right)\end{array}\right\}\ast \left\{\begin{array}{c}{e}_1\ast {sigmoid}_1\ast \Big(1-{sigmoid}_1\\ {}{e}_2\ast {sigmoid}_2\ast \Big(1-{sigmoid}_2\end{array}\right\}\ast \left\{{o}_1\kern0.5em {o}_2\right\} $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equbc.png)

在哪里

*   *o*<sub>*n*</sub>=从隐藏层输出

至此，我已经涵盖了所有必要的理论和数学背景，让您能够理解一个关于人工神经网络如何学习的完整示例。

### 人工神经网络权重修正实例

我将使用与早期模型略有不同的网络来详细说明如何计算修改后的权重。图 [4-28](#Fig28) 是图 [4-26](#Fig26) 的修改版本，其中我插入了两个随机值来表示隐藏层节点的输出。

![../images/482214_1_En_4_Chapter/482214_1_En_4_Fig28_HTML.jpg](../images/482214_1_En_4_Chapter/482214_1_En_4_Fig28_HTML.jpg)

图 4-28

改进的三层六节点人工神经网络

计算从更新 *w* <sub>*1，1*</sub> 开始，这是将隐藏层中的节点 1 连接到输出层中的节点 1 的链接。以下是该链路的梯度下降方程:

![$$ \frac{\partial e}{\partial {w}_{j,k}}=-\left({t}_k-{o}_k\right)\ast sigmoid\left(\sum \limits_j{w}_{j,k}\ast {o}_j\right)\ast \left(1- sigmoid\left(\sum \limits_j{w}_{j,k}\ast {o}_j\right)\right)\ast {o}_j $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equbd.png)

将图中的值代入该方程得到

![$$ \left({t}_k-{o}_k\right)={e}_1=0.96 $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Eqube.png)

![$$ \left(\sum \limits_j{w}_{j,k}\ast {o}_j\right)=\left(2.0\ast 0.6\right)+\left(3.0\ast 0.4\right)=2.4 $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equbf.png)

![$$ sigmoid=\frac{1}{\left(1+{e}^{-2.4}\right)}=0.9168 $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equbg.png)

![$$ 1- sigmoid=0.0832 $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equbh.png)

![$$ {o}_1=0.6 $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equbi.png)

将前面的因子相乘得到

![$$ -0.96\ast 0.9168\ast 0.0832\ast 0.6=-0.04394 $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equbj.png)

如果假设学习率为 0.15，则新的权重将为

![$$ new\ {w}_{j,k}= old\ {w}_{j,k}-\alpha \ast \frac{\partial e}{\partial {w}_{j,k}} $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equbk.png)

![$$ new\ {w}_{j,k}=2.0-0.15\ast \left(-0.04394\right)=2.0+0.0066=2.0066 $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equbl.png)

新的重量与旧的重量相差不大；但是，您必须注意，在达到全局最小值之前，即使没有数千次迭代，也会有数百次迭代。经过多次迭代，小的变化最终会导致大的变化。

所有其他网络权重的调整过程与我刚才演示的过程相同。

#### 人工神经网络学习的一些问题

关于乙状结肠激活功能，有两个事项你应该知道。为了支持这一讨论，我将图 [2-16](2.html#Fig16) 复制为图 [4-29](#Fig29) 。

![../images/482214_1_En_4_Chapter/482214_1_En_4_Fig29_HTML.jpg](../images/482214_1_En_4_Chapter/482214_1_En_4_Fig29_HTML.jpg)

图 4-29

Sigmoid 函数

查看该图，您应该能够看到，对于大于 2.5 的 x 输入，y 输出变化很小。这是因为 sigmoid 函数在该值附近渐近地接近 1.0。大 x 输入的小变化意味着发生非常小的梯度变化。人工神经网络学习受到抑制，因为梯度下降算法依赖于“合理的”斜率。因此，人工神经网络训练数据集应该将 x 值限制在大约-3 到 3 的*伪线性*范围内。出现负极限是因为 sigmoid 函数关于 y 轴对称，当 x 等于或小于 2.5 时出现饱和。在*伪线性*范围之外的 x 值将导致 ANN 的饱和效应，并且不会发生有效的权重更新。

sigmoid 函数的另一个问题是它不能输出大于 1.0 或小于 0 的值。必须选择初始权重，以确保函数可以在其允许的范围内输出。实际上，由于前面描述的渐近性质，输出范围必须从大约 0.01 到 0.99。

正如我刚才描述的，初始重量的选择很重要。选择一组好的初始人工神经网络权重将避免输入饱和和输出限制问题。第一个显而易见的选择是将权重限制在我之前指定的*伪线性*范围内。然而，权重通常被限制为 1，这样更保守一些。

人工智能研究人员多年来一直遵循一条有用的经验法则来帮助选择体重:

> *权重应使用正态分布进行初始分配，其平均值等于人工神经网络中节点数量平方根的倒数。*

如果你用的是 36 个节点的小 ANN，那么均值就是![$$ \frac{1}{\sqrt{36}} $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_IEq6.png)或者 0.16667。图 [4-30](#Fig30) 显示了具有该平均值和 2 个标准差的正态概率分布。

![../images/482214_1_En_4_Chapter/482214_1_En_4_Fig30_HTML.jpg](../images/482214_1_En_4_Chapter/482214_1_En_4_Fig30_HTML.jpg)

图 4-30

36 节点人工神经网络初始权值的正态偏差

在大约-0.5 到 0.833 的范围内随机选择权重将很好地为 36 节点人工神经网络的学习提供一个极好的起点。

关于初始重量的选择，有两个要点。首先是避免将所有权重设置为相同的值。人工神经网络学习依赖于不均匀的权重分布。第二点(希望是显而易见的)是不要将所有权重设置为 0，因为这将禁用人工神经网络。

这最后一节完成了我所有关于人工神经网络的预备性讨论。是时候采用一个真正的基于 Python 的 ANN 了。

### ANN Python 演示–第 1 部分

在本演示的第 1 部分，我将向您展示如何使用 Python 创建一个未经训练的 ANN。在第 2 部分，我将向你展示如何训练人工神经网络。

这个讨论从描述一个实际的人工神经网络的组成模块开始。每个模块都必须有软件来实现它的目的。

第一个模块是 Init 模块，用于“构建”ANN 结构。在这次演示中，我将构建一个三层九节点的人工神经网络。这意味着我必须拥有代表每一层的对象，以及输入、输出和权重。表 [4-4](#Tab4) 显示了初始化模块对象和引用。

表 4-4

初始化模块对象和引用

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"></colgroup> 
| 

名字

 | 

描述

 |
| --- | --- |
| 节点 | 输入层中的节点数 |
| 鳞毛蕨 | 隐藏层中的节点数 |
| 奥诺德 | 输出层中的节点数 |
| wtgih | 输入层和隐藏层之间的权重矩阵 |
| wtgho | 隐藏层和输出层之间的权重矩阵 |
| w <sub>ij</sub> | 个体权重矩阵元素 |
| 投入 | 输入向量 |
| 输出 | 输出向量 |
| 隐藏的 | 隐藏层输出的数组 |
| 实验室反应堆 | 学习率 |

以下 Init 模块代码设置节点的数量和类型以及学习速率:

```py
def __init__(self, inode, hnode, onode, lr):
    # Set local variables
    self.inode = inode
    self.hnode = hnode
    self.onode = onode
    self.lr = lr

```

必须用适当的值调用这个 Init 模块代码，以构建一个三层、九节点的 ANN。这些值是

*   信息节点= 3

*   hnode = 3

*   onode = 3

*   lr = 0.25

下一个要讨论的模块是设置权重矩阵的模块。我决定使用均值为 0.1667、标准差为 0.3333 的正态分布，就像我之前讨论的那样。Numpy 包含一个随机数生成器，很好地满足了这个需求。以下代码创建了一个名为 wtgih 的 3 x 3 矩阵，其中填充了具有所需统计特征的随机数:

```py
self.wtgih = np.random.normal(0.1667, 0.3333, self.hnodes, self. inodes)

```

我在 Python 交互式会话中测试了前面的代码，如图 [4-31](#Fig31) 所示。

![../images/482214_1_En_4_Chapter/482214_1_En_4_Fig31_HTML.jpg](../images/482214_1_En_4_Chapter/482214_1_En_4_Fig31_HTML.jpg)

图 4-31

测试代码的 Python 交互式会话

产生的 wtgih 矩阵具有良好的初始值。Init 模块现在可以扩展为包括矩阵生成代码，在这里我使用前面描述的经验法则来设置矩阵的统计参数。

```py
def __init__(self, inode, hnode, onode, lr):
    # Set local variables
    self.inode = inode
    self.hnode = hnode
    self.onode = onode
    self.lr = lr

    # Mean is the reciprocal of the sqrt of node sum
    mean = 1 / (pow((inode + hnode + onode), 0.5))

    # Std dev is approx 1/6 of total weight range
    # Total range = 2
    sd = 2 / 6

    # Generate both weight matrices
    # Input to hidden layer
    self.wtgih = np.random.normal(mean, sd, [hnode, inode])

    # Hidden to output layer
    self.wtgho = np.random.normal(mean, sd, [onode, hnode])

```

现在，我将介绍第二个模块，用于测试 Init 模块。这个新模块被命名为 testNet，这反映了它的用途。这个模块接受一个输入向量并返回一个输出向量。这个新模块执行以下步骤:

1.  将输入数据向量转换为 numpy 数组。

2.  将输入数组乘以 wtgih 权重矩阵。

3.  应用 sigmoid 激活功能。

4.  将隐藏层输出乘以 wtgho 矩阵。

5.  应用 sigmoid 激活功能。

这个新模块的清单如下:

```py
import numpy as np
def testNet(self, input):
    # Convert input data vector into an array
    input = np.array(input, ndmin=2).T

    # Multiply input array by wtgih matrix
    hInput = np.dot(self.wtgih, input)

    # Apply activation function
    hOutput = 1 / (1 + np.exp(-hInput))

    # Multiply hidden layer output by wtgho matrix
    oInput = np.dot(self.wtgho, hOutput)

    # Apply activation function
    oOutput = 1 / (1 + np.exp(-oInput))

    return oOutput

```

Init 和 testNet 模块都放在一个名为 ANN 的 Python 类中，我将在第 2 部分演示之后向您展示这个类。然而，我首先需要演示一个完全未经训练的人工神经网络是如何工作的。

### 注意

此时，您将无法复制下面的交互式会话，因为 ANN 类文件不在您的主目录中。您可以在创建或加载类文件后尝试进行这种交互式会话。

图 [4-32](#Fig32) 显示了实例化一个名为 ann 的 ANN 对象，然后调用 testNet 方法的交互会话。请注意，初始化方法是在实例化 ann 对象时自动调用的。

![../images/482214_1_En_4_Chapter/482214_1_En_4_Fig32_HTML.jpg](../images/482214_1_En_4_Chapter/482214_1_En_4_Fig32_HTML.jpg)

图 4-32

testNet 调用的交互式 Python 会话

输出中存在一些严重的错误，我在表 [4-5](#Tab5) 中详细说明了这些错误。

表 4-5

初始测试误差

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"> <col class="tcol4 align-left"></colgroup> 
| 

投入

 | 

输出

 | 

错误

 | 

百分比误差

 |
| --- | --- | --- | --- |
| Zero point eight | 0.628566 | –0.171434 | Twenty-one point four |
| Zero point five | 0.782561 | 0.282561 | Fifty-six point five |
| Zero point six | 0.672449 | 0.072449 | Twelve point one |

这些误差在网络训练后应该会大大减少，这是下一次演示的主题。

### ANN Python 演示–第 2 部分

在演示的第 2 部分中，我将向您展示如何训练您在第 1 部分中创建的网络。训练将使用名为 trainNet 的第三个模块，并被添加到 ANN 类文件中。该模块的功能类似于 testNet 模块，根据输入数据集计算输出数据集。然而，训练网模块输入数据集是预定的训练集，而不是随机生成的数据集。随着我对模块开发的讨论，预定在这个上下文中的含义将变得更加清楚。

trainNet 模块计算误差数据集，即人工神经网络输出和输入训练数据集之间的差异。这种行为被称为监督学习，因为网络“知道”正确的输出应该是什么，并且可以修改其权重，以尝试实现输入训练数据集中包含的基本真实值。

trainNet 模块的下一个清单以一些初始化代码开始，这些代码在 Init 模块代码中发生的初始化之外:

```py
def trainNet(self, inputT, train):
    # This module depends on values, arrays, and matrices
    # created when the init module is run
    # Create the arrays from the list arguments
    self.inputT = np.array(inputT, ndmin=2).T
    self.train = np.arrat(train, ndmin=2).T

```

计算的误差是训练集值和实际输出之间的差异。如前所示，第 *k* <sub>*第*</sub> 输出节点的误差方程为

![$$ {e}_k={t}_k-{o}_k $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equbm.png)

输出误差的矩阵符号为

![$$ self. eOutput= self. train- self. oOutput $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equbn.png)

本例 ANN 的矩阵符号中的隐藏层误差数组为

![$$ hError={\left\{\begin{array}{c}{w}_{1,1}\kern0.5em {w}_{1,2}\kern0.5em {w}_{1,3}\\ {}{w}_{2,1}\kern0.5em {w}_{2,2}\kern0.5em {w}_{2,3}\\ {}{w}_{3,1}\kern0.5em {w}_{3,2}\kern0.5em {w}_{3,3}\end{array}\right\}}^T\ast \left\{\begin{array}{c}{e}_1\\ {}{e}_2\\ {}{e}_3\end{array}\right\} $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equbo.png)

以下是生成此数组的 Python 代码:

```py
self.hError = np.dot(self.wtgho.T, self.eOutput)

```

下面是用于调整第 j 层和第 k 层之间的链接的权重更新等式:

![$$ g\left({w}_{j,k}\right)=\alpha \ast {e}_k\ast sigmoid\left({o}_k\right)\ast \left(1- sigmoid\left({o}_k\right)\right)\ast {o}_j^T $$](../images/482214_1_En_4_Chapter/482214_1_En_4_Chapter_TeX_Equbp.png)

新的*g*(*w*<sub>T5】j， *k*</sub> )数组必须添加到原始数组中，因为这些数组是对原始数组的调整。前面的等式可以很容易地用 Python 代码来表达

```py
self.wtgho += self.lr*np.dot((self.eOutput*self.oOutputT*(1 - self.oOutputT)), self.hOutputT.T)

```

输入层和隐藏层之间的权重更新代码使用完全相同的格式:

```py
self.wtgih += self.lr*np.dot((self.hError*self.hOutputT*(1 - self.hOutputT)), self.inputT.T)

```

前面两条 Python 语句是梯度下降算法的核心。它们基本上是沿着复杂的误差(损失)函数轮廓向下搜索全局最小值。您应该注意到，没有任何限制语句会停止这种搜索。这是调用函数的责任，我将很快演示。

完整的 ANN 类列表如下，其中包括 trainNet 模块以及 Init 和 testNet 模块。这个文件被命名为 ANN.py，可以从该书的配套网站上获得。除了清单中包含的内容，我没有添加任何额外的代码注释。我觉得我前面所有的讨论都有希望解释这段代码是如何工作的。

```py
# Import required libraries
import numpy as np
class ANN:

    def __init__(self, inode, hnode, onode, lr):
        # Set local variables
        self.inode = inode
        self.hnode = hnode
        self.onode = onode
        self.lr = lr

        # Mean is the reciprocal of the sqrt of total nodes
        mean = 1/(pow((inode + hnode + onode), 0.5))

        # Std dev is approx 1/6 of total range
        # Range = 2
        sd = 2/6

        # Generate both weight matrices
        # Input to hidden layer matrix
        self.wtgih = np.random.normal(mean, sd, [hnode, inode])

        # Hidden to output layer matrix
        self.wtgho = np.random.normal(mean, sd, [onode, hnode])

    def testNet(self, input):
        # Convert input data vector into numpy array
        input = np.array(input, ndmin=2).T

        # Multiply input by wtgih
        hInput = np.dot(self.wtgih, input)

        # Apply activation function
        hOutput = 1/(1 + np.exp(-hInput))

        # Multiply hidden layer output by wtgho
        oInput = np.dot(self.wtgho, hOutput)

        # Apply activation function
        oOutput = 1/(1 + np.exp(-oInput))

        return oOutput

    def trainNet(self, inputT, train):
        # This module depends upon values, arrays and matrices
        # created when the init module is run

        # Create the arrays from the arguments
        self.inputT = np.array(inputT, ndmin=2).T
        self.train = np.array(train, ndmin=2).T

        # Multiply inputT array by wtgih
        self.hInputT = np.dot(self.wtgih, self.inputT)

        # Apply activation function
        self.hOutputT = 1/(1 + np.exp(-self.hInputT))

        # Multiply hidden layer output by wtgho
        self.oInputT = np.dot(self.wtgho, self.hOutputT)

        # Apply activation function
        self.oOutputT = 1/(1 + np.exp(-self.oInputT))

        # Calculate output errors
        self.eOutput = self.train - self.oOutputT

        # Calculate hidden layer error array
        self.hError = np.dot(self.wtgho.T, self.eOutput)

        # Update weight matrix wtgho
        self.wtgho += self.lr*np.dot((self.eOutput*self.oOutputT*(1 - self.oOutputT)), self.hOutputT.T)

        # Update weight matrix wtgih
        self.wtgih += self.lr*np.dot((self.hError*self.hOutputT*(1 - self.hOutputT)), self.inputT.T)

```

以下脚本使用 ANN 类来训练我在演示的第 1 部分中使用的相同规模的网络。该脚本名为 testANN3.py，可从该书的配套网站获得:

```py
# Import required libraries
from ANN import ANN

# Create input data vector
inputT = [0.8, 0.5, 0.6]
# Display it
print('Input data vector')
print(inputT)
print()

# Train for 1 iteration
train = inputT
ann = ANN(3,3,3,0.3)
output = ann.testNet(inputT)
# Display output
print('After one iteration')
print(output)
print()

# Train for 499 iterations
for i in range(499):
    ann.trainNet(inputT, train)

output = ann.testNet(inputT)
# Display output
print('After 500 iterations')
print(output)
print()

```

通过输入以下命令运行该脚本:

```py
python testANN3.py

```

图 [4-33](#Fig33) 显示了脚本运行后的结果。

![../images/482214_1_En_4_Chapter/482214_1_En_4_Fig33_HTML.png](../images/482214_1_En_4_Chapter/482214_1_En_4_Fig33_HTML.png)

图 4-33

运行 testANN3 脚本后的结果

您可以清楚地看到，除了第三个元素之外，初始输出与初始数据向量相差甚远。然而，在总共 500 次迭代之后，输出基本上与输入匹配，这表明网络针对这个特定的输入数据集向量进行了充分的训练。如果你想知道，我选择 500 次迭代作为限制没有特别的原因，除了输出结果不变，因为我在那个区域尝试了不同的数字。我使用了“试错”方法，因为 RasPi 只需要几秒钟就可以完成数百次迭代。这个结果只是表明，关于要使用的迭代次数，没有“神奇”的数字可寻，因为每个网络都是唯一的。有时候你只要用一个足够好的数字就行了，我之前已经说过了。

我还对两个权重矩阵从初始化版本到完全训练版本的净变化感兴趣。因此，我向 ANN 类添加了一个额外的方法，该方法在被调用时返回两个矩阵。此方法名为 getMatrices，如下所示:

```py
def getMatrices(self):
    matrixList = list([self.wtgih, self.wtgho])
    return matrixList

```

然后对 testANN3 脚本进行了轻微的修改，以对 getMatrices 方法进行两次调用。第一次调用是在第一次迭代之后，第二次调用是在第 500 次迭代之后。修改后的 testANN3.py 脚本被重命名为 testANN4.py，可从该书的配套网站获得。

```py
from ANN import ANN
inputT = [0.8, 0.5, 0.6]
print('Input data vector')
print(inputT)
print()

train = inputT
ann = ANN(3,3,3,0.3)
output = ann.testNet(inputT)
print('After one iteration')
print(output)
print()

matrixList = ann.getMatrices()
print('wtgih matrix')
print(matrixList[0])
print()
print('wtgho matrix')
print(matrixList[1])
print()

for i in range(499):
    ann.trainNet(inputT, train)

output = ann.testNet(inputT)
print('After 500 iterations')
print(output)
print()

matrixList = ann.getMatrices()
print('wtgih matrix')
print(matrixList[0])
print()
print('wtgho matrix')
print(matrixList[1])
print()

```

通过输入以下命令运行该脚本:

```py
python testANN4.py

```

图 [4-34](#Fig34) 显示了脚本运行后的结果。

![../images/482214_1_En_4_Chapter/482214_1_En_4_Fig34_HTML.jpg](../images/482214_1_En_4_Chapter/482214_1_En_4_Fig34_HTML.jpg)

图 4-34

运行 testANN4 脚本后的结果

两个矩阵的初始版本和最终版本之间存在显著差异，这清楚地显示了梯度下降算法如何改变权重。

接下来，我很好奇，如果简单地重新运行脚本，这两个矩阵会发生什么。图 [4-35](#Fig35) 显示了重新运行的结果。

![../images/482214_1_En_4_Chapter/482214_1_En_4_Fig35_HTML.jpg](../images/482214_1_En_4_Chapter/482214_1_En_4_Fig35_HTML.jpg)

图 4-35

重新运行 testANN4 脚本后的结果

当比较图 [4-34](#Fig34) 和图 [4-35](#Fig35) 时，很容易看出每种情况下的起始矩阵是不同的。这是因为 Init 模块使用随机数过程来创建每个矩阵。一个更有趣的特性是，每次脚本运行时，每个矩阵的最终版本都不同。我的结论是，对于一个特定的解决方案，一定没有单一的优化矩阵集，并且最终的矩阵集依赖于初始化的矩阵集。从数学的角度来看，这意味着必须有一个无限的矩阵集，可以使用这个特定的人工神经网络来创建这个特定的输入数据集。我推测，这是人工智能研究人员将人工神经网络称为“黑盒”的一个可能原因，因为这些非分析解决方案。

我的深度学习准备章节到此结束。如果我在某些话题上有点“过火”,特别是在数学方面，我谦卑地道歉，但我觉得有必要让你至少了解一次 DL 的重要基础。现在，您应该已经做好充分准备，能够理解和欣赏接下来几章中有趣而实用的人工神经网络演示。****